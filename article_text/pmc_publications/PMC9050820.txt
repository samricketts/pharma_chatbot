LICENSE: This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.


101201192
33099
Pharm Stat
Pharm Stat
Pharmaceutical statistics
1539-1604
1539-1612

34962077
9050820
10.1002/pst.2186
NIHMS1763746
Article
Empirical likelihood confidence interval for sensitivity to the early disease stage
Rahman Husneara 1
Zhao Yichuan 1
Alzheimer’s Disease Neuroimaging Initiative2
1 Department of Mathematics and Statistics, Georgia State University, Georgia, USA
Correspondence Yichuan Zhao, Department of Mathematics and Statistics, Georgia State University, Atlanta, GA, 30303, USA yichuan@gsu.edu
30 12 2021
5 2022
27 12 2021
01 5 2023
21 3 566583
This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.
Disease status can naturally be classified into three or more ordinal stages rather than just being binary stages. Many works have been done for the estimation and inference procedure regarding three ordinal disease stages, which are non-disease, early disease and full disease stages. The early disease stage can be very important for therapeutic intervention and prevention potentiality. As a diagnostic measure, sensitivity to the early disease stage is often used. In this paper, we propose confidence intervals for the sensitivity to early disease stage based on given target specificity for non-disease stage and target sensitivity to full disease stage using both empirical likelihood and adjusted empirical likelihood procedures. We compare the performance of the proposed empirical likelihood confidence intervals with other procedures in our simulation study. The proposed procedures are further applied to Alzheimer’s Disease Neuroimaging Initiative (ADNI) data set.

Empirical likelihood
ordinal disease stages
early disease stage
sensitivity
confidence interval

pmc1 | INTRODUCTION

The performance of diagnostic tests in identifying different diseases is frequently studied in statistics. Usually the considered diseases are of binary class. Many statistical procedures have been developed related to binary diseases such as sensitivity, specificity, positive predictive values, negative predictive values, ROC curve, Lorenz curve, Gini index, kappa statistic, etc. Among them, receiver operating characteristic (ROC) curves are studied extensively. Summary measures for ROC curves are area under the curve (AUC), partial AUC and Youden index, etc.1, 2, 3 Ordinal disease stages with more than two classes are studied for some diseases. Increased awareness on the adversity of different diseases makes the studies of these disease stages, especially the early disease stage, of more interest.

Early disease stage plays a significant role in prevention and therapeutic intervention for the diseases with multiple stages. Being able to correctly identify diseases at early stages increases the chance of an effective treatment. In some situations taking extra caution, therapeutic measures and dietary or behavioral controls help to prevent or to delay the onset of the diseases. In this way, the early detection can prevent extensive complications of diseases and reduce expenditures on expensive medications.

There are several diseases where the studies of early disease stage plays a vital role. For example, chronic Kidney Disease (CKD) has an increased prevalence worldwide. There are five stages to CKD, whereas stage 1 and stage 2 are considered as early stages. Taking some cautious measures like treatment of high blood pressure, glycaemic control, cessation of smoking, diatery control and exercise, etc. during the early stages of CKD can slow down the progression of the disease and reduce the associated risks of cardiovascular events, kidney failure, and death.4, 5 Another disease with high prevalence is Type I diabetes. It is a chronic disease, which causes insulin dependence. Over time, the disease can cause lethal complications. The early stage of Type I diabetes is being emphasized by researchers for the early diagnosis and prevention purposes. Effective interventions at pre-symptomatic stages of Type I diabetes may delay the progression to symptomatic Type I diabetes.6 Alzheimer’s disease is one of the most common cause of dementia. Detection at an early disease stage appears to be very important to take precautions and to lessen the adverse effect of the disease.7, 8, 9 Among other diseases, for which early disease stages are studied are multiple sclerosis, heart disease, different cancers like breast cancer, ovarian cancer or lung cancer, etc. Multiple sclerosis develops gradually and starting of treatment in early stage is highly recommended.10 Early detection increases the survival rate in cancer patient and can save lives for heart diseased patients as well.

An extension of receiver operating characteristic (ROC) curve for ordinal three stage diseases is ROC surface, which is used for observing the performance of biomarkers in identifying three stages of the disease. Volume under the surface (VUS) is studied as a summary measure. There have been several developments on VUS or ROC surface. For example, nonparametric Mann-Whitney U-statistic based variance estimation for the VUS,11 generalization of three-class classification and ROC hyper-surface construction for ordered multi-class classification problem,12 kernel smoothing estimation for VUS with an application to liver cancer data,13 nonparametric calculation of VUS and parametric three-way ROC surface analysis using MATHEMATICA,14 etc. Luo and Xiong15 developed a useful R package to calculate and analyze three-group classification problem. An application to neuropsychological markers for Alzheimer’s disease is also illustrated by them. SAS can also be used used to estimate VUS with different parametric and nonparametric approaches.16 Xiong et al.17 proposed a general linear mixed model for the clustered ordinal diagnostic group. The model incorporates the dependency or correlation on the diagnostic marker. It also allows the use of covariates and missing data.

In the analysis of diagnostic test accuracy, trade-offs exist between the sensitivity and the specificity. By changing cut-off values, we can find the varying combination of sensitivity and specificity. One of the convenient procedures to choose the cut-off or the combination of sensitivity and specificity is by considering a target specificity. Thus, the interval estimation for the sensitivity for given specificity level is of interest. Zhou and Qin18 and Qin et al.19 incorporated empirical likelihood and bootstrap procedures to improve the accuracy of the confidence interval estimation for the sensitivity, whereas Tian20 proposed interval estimation for the sensitivity for a combination of markers at fixed level of specificity. Dong et al.,21 Dong and Tian22 proposed different parametric and nonparametric approaches along with an EL approach to the estimation and the construction of confidence intervals for the sensitivity to the early disease stage at the given specificity level. More nonparametric developments like jackknife empirical likelihood confidence interval in the related area such as ROC curve, AUC, ordinal dominance curve, VUS and Youden index are observed.23, 24, 25, 26, 27, 28

Since the development of empirical likelihood by Owen,29 it has been playing an important role in statistical inference procedures. Empirical likelihood is a non-parametric approach that allows us to use likelihood methods, where no assumption is required for the family of underlying distributions. The empirical distribution of the data plays the central role in such inference procedures. Empirical likelihood has the flexibility of performing different types of statistical analysis and inference procedures, whereas different other non-parametric methods fail to have the flexibility. More details can be found in Owen30, 31. In this paper, we propose two empirical likelihood approaches for the sensitivity to early disease stage for given target specificity and target sensitivity to full disease stage.

Along with the difficulty of satisfying the underlying distribution assumption, parametric approaches to confidence interval estimation have some other disadvantages. It requires formulation and estimation of variance of the parameter of interest, which becomes complicated for different situations. Existing methods for the construction of confidence interval for the sensitivity to early disease stage either utilize parametric approach, empirical likelihood (EL) approach with scaled chi-square distribution of the test statistic or bootstrap procedures. While the EL approach with scaled chi-square approximation still requires the calculation of variance of the parameter. The proposed empirical likelihood approach, which utilizes profiling out the nuisance parameters, does not have such complications. As a data driven and computationally intensive procedure, the profile empirical likelihood may face difficulty in attaining the convergence in some situations. An improvement to the procedure, the adjusted empirical likelihood approach was proposed by Chen et al.32 The procedure confirms to attain the convergence. We propose an adjusted empirical likelihood confidence interval for the sensitivity to the early disease stage. We compared the performance of our proposed approaches with an existing EL approach which use bootstrap variance estimation (ELB) and with percentile bootstrap approach (PB) using simulation studies and real data applications.

The organization of the rest of the paper is as follows. We discuss the formulation of a sensitivity to early disease stage, non-parametric approaches to estimate the sensitivity and existing approaches to construct confidence intervals in Section 2. The new method for the proposed confidence interval using profile empirical likelihood (PEL) is described in Section 3. In Section 4, the proposed adjusted empirical likelihood approach is illustrated. Our extensive simulation studies are carried out in Section 5. Application of the procedures to ADNI data set is given in Section 6. Concluding remarks and discussions are stated in Section 7. The proofs of theorems are provided in the Appendix.

2 | SENSITIVITY TO THE EARLY DISEASE STAGE

Let X, Y, Z be observations of diagnostic tests from non-diseased, early diseased and diseased individuals, respectively, with corresponding distribution functions F(X), G(Y) and H(Z). For the given thresholds c1 and c2, where c1 &lt; c2, we assume that a subject is considered as diseased if the corresponding test value is greater than c2, is considered as non-diseased if corresponding test value is smaller than c1 and as early diseased otherwise (i.e., test value between c1 and c2). Let P1, P2 and P3 be correct classification rates for non-disease, early-disease and full disease stages, respectively. Then, we define the specificity and the sensitivities as (1) P1=P(X&lt;c1)=F(c1)=1−SF(c1),P2=P(c1&lt;Y&lt;c2)=G(c2)−G(c1)=SG(c1)−SG(c2),P3=P(Z&gt;c2)=1−H(c2)=SH(c2).

Here SF ,SG and SH are corresponding survival functions of non-diseased, early diseased and fully diseased observations, respectively. Thus we can define the thresholds as c1=F−1(P1)=SF−1(1−P1) and c2=H−1(1−P3)=SH−1(P3). P2, the sensitivity to the early disease stage, can be expressed as a function of P1, the specificity and of P3, the sensitivity for the full disease stage. (2) P2=P2(P1,P3)=G(c2)−G(c1)=G(H−1(1−P3))−G(F−1(P1)).

For the given P1 and P3, P2 can be viewed as the ROC surface on three-dimensional space (P1, P3, P2).

2.1 | Nonparametric estimation

We assume that there are n1 observations from the non-diseased population as {Xi; i = 1,…, n1} with corresponding empirical distribution function Fn1(⋅) and empirical survival function SF,n1(⋅), respectively. From the early diseased population, there are n2 observations as {Yj; j = 1,…, n2} with Gn2(⋅) as the empirical distribution function and SG,n2(⋅) as the empirical survival function, respectively. From the fully diseased population, there are n3 observations as {Zk; j = 1,…, n3} with Hn3(⋅) as the empirical distribution function and SH,n3(⋅) as the empirical survival function respectively. We define SF,n1−1(r)=Fn1−1(1−r):=X(⌊(1−r)n1⌋), SG,n2−1(r)=Gn2−1(1−r):=Y(⌊(1−r)n2⌋), SH,n3−1(r)=Hn3−1(1−r):=Z(⌊(1−r)n3⌋), and where X(i), Y(j), Z(k) are corresponding order statistic and ⌊a⌋ denotes the largest integer smaller than a for any real number a. The above definitions will provide an estimate of c1 as Fn1−1(P1)=SF,n1−1(1−P1) and an estimate of c2 as Hn3−1(1−P3)=SH,n3−1(P3). The estimator of P2 is defined as follows, (3) P^2(P1,P3)=Gn2(Hn3−1(1−P3))−Gn2(Fn1−1(P1)).

From a probability perspective, an alternative trimmed Mann-Whitney U-statistic is given as follows, (4) P^2(P1,P3)=1n2∑j=1n2Vj(P1,P2),

where Vj(P1,P2):=I{Fn1−1(P1)≤Yj≤Hn3−1(1−P3)}.

33 proposed an adjustment to the estimation by (5) P^2(P1,P3)=∑j=1n2Vj(P1,P2)+z1−α/22/2n2+z1−α/22.

2.2 | Existing approaches

An existing approach to construct 100(1–α)% confidence interval is the percentile bootstrap (PB) approach. The PB confidence interval is as follows, (6) (P^2b(α/2),P^2b(1−α/2)),

where P^2b(α) is the (100α)th percentile of bootstrap re-sample estimates.

Dong and Tian22 proposed another nonparametric approach that utilizes bootstrap variance estimation to the empirical likelihood approach, where the empirical likelihood ratio test statistic approximately follows scaled chi-square distribution. Dong and Tian22 established the following result rP1,P2,P3l(P2)→χ12,

where l(P2) = −2 log r(P2), r(P2) is the empirical likelihood ratio and rP1,P2,P3=σU^i2n2σP^22.

Then, the 100(1 – α)% ELB confidence interval can be constructed as follows (7) I(α)={P2(P1,P3):rP1,P2,P3*l(P2)≤χ12(α)},

where χ12(α) is the upper α–quantile of χ12 distribution and an estimate of scale constant is, rP1,P2,P3*=P^2(1−P^2)/(n2σ^P^22).22 For the ELB method, σ^P^22 is estimated from b = 500 bootstrap re-samples.

3 | EMPIRICAL LIKELIHOOD METHOD FOR THE SENSITIVITY

Let X1,…,Xn1, Y1,…,Yn2 and Z1,…,Zn3 be three independent samples from non-diseased, early diseased and fully diseased individuals’ test results with distribution functions F, G and H, respectively. For given values of P1, P3, the hypotheses are H0:{F−1(P1)=c1,H−1(1−P3)=c2,G(H−1(1−P3))−G(F−1(P1))=P2(P1,P3),

which are equivalent to H0:{F(c1)=P1,H(c2)=1−P3,P(c1&lt;Y&lt;c2)=P2(P1,P3).

To generalize the hypotheses, we denote {h1(X,c1)=h1(X)=I(X&lt;c1),h2(Z,c2)=h2(Z)=I(Z&gt;c2),h3(Y,c1,c2)=h3(Y)=I(c1&lt;Y&lt;c2).

Suppose that p1,…,pn1 are the probabilities at X1,…,Xn1 with all pi &gt; 0 and ∑i=1n1pi=1. Similarly, let q1,…,qn2 be the probabilities at Y1,…,Yn2 for all qj &gt; 0, ∑j=1n2qj=1. We also assume r1,…,rn3 be the probabilities at Z1,…,Zn3 for all rk &gt; 0, ∑k=1n3rk=1. The above hypotheses can be expressed as {∑i=1n1(h1(Xi)−P1)pi=0,∑k=1n3(h2(Zk)−P3)rk=0,∑j=1n2(h3(Yj)−P2(P1,P3))qj=0.

We denote hT (X, Y, Z) = (h1(X), h2(Z), h3(Y)), ΘT = (P1, P3, P2(P1, P3)) and OT = (0, 0, 0). We express the above hypotheses using the vector notation as follows, (8) ∑i=1n1∑j=1n2∑k=1n3(h(Xi,Yj,Zk)−Θ)piqjrk=O,

where (c1, c2, P2(P1, P3)) is the unknown parameter and c1, c2 are contained in the function h(X, Y, Z). Then the empirical likelihood is given by (9) L(c1,c2,P2(P1,P3))=suppi,qj,rk{∏i=1n1∏j=1n2∏k=1n3(piqjrk):pi&gt;0,qj&gt;0,rk&gt;0,∑i=1n1pi=1,∑j=1n2qj=1,∑k=1n3rk=1,∑i=1n1∑j=1n2∑k=1n3(h(Xi,Yj,Zk)−Θ)piqjrk=O}.

The likelihood ratio can be constructed by taking into account of the fact that unconstrained maximum of the likelihood is attained at pi = 1/n1; i = 1,…, n1, qj = 1/n2; j = 1,…, n2, rk = 1/n3; k = 1,…, n3. The EL ratio can be expressed as follows, (10) R(c1,c2,P2(P1,P3))=suppi,qj,rk{∏i=1n1∏j=1n2∏k=1n3(n1n2n3piqjrk):pi&gt;0,qj&gt;0,rk&gt;0,∑i=1n1pi=1,∑j=1n2qj=1,∑k=1n3rk=1,∑i=1n1∑j=1n2∑k=1n3piqjrk(h(Xi,Yj,Zk)−Θ)=O}.

The logarithm of the EL ratio is re-expressed as follows (11) logR(c1,c2,P2(P1,P3))=suppi,qj,rk{(∑i=1n1log(n1pi)+∑j=1n2log(n2qj)+∑i=1n3log(n3rk)):pi&gt;0,qj&gt;0,rk&gt;0,∑i=1n1pi=1,∑j=1n2qj=1,∑k=1n3rk=1,∑i=1n1∑j=1n2∑k=1n3piqjrk(h(Xi,Yj,Zk)−Θ)=O}.

The Lagrangian expression of the constrained optimization problem is L(pi,qj,rkγ,η,λ)=∑i=1n1log(n1pi)+∑j=1n2log(n2qj)+∑k=1n3log(n3rk)+γ(∑i=1n1pi−1)+η(∑j=1n2qj−1)+ζ(∑k=1n3rk−1)−λT∑i=1n1∑j=1n2∑k=1n3(h(Xi,Yj,Zk)−Θ)piqjrk.

In the above constrained optimization problem, γ, η, ζ and λ are Lagrange multipliers and λT = (λ1, λ2, λ3). From the standard Lagrange multipliers method, we have optimal values as pi=1n1+λ1(h1(Xi)−P1);∑i=1n1pi=1,

qj=1n2+λ3(h3(Yj)−P2);∑j=1n2qj=1,

rk=1n3+λ2(h2(Zk)−P3);∑k=1n3rk=1.

The values of λ can be obtained using numerical search procedures. c1 and c2 are implicit in the function h(·). The empirical likelihood ratio function in equation (11) contains c1 and c2, which are not of our interest. We can profile out the two nuisance parameters simultaneously, and obtain the optimal value. Using the profile EL over c1 and c2, we have −2logR(P2(P1,P3))=−2maxc1,c2[logR(c1,c2,P2(P1,P3))]=−2logR(c^1,c^2,P2(P1,P3)).

We establish Wilk’s theorem for the empirical likelihood as follows,

Theorem 1:

We assume that n1/n2 → ρ1, 0 &lt; ρ1 &lt; ∞ and n3/n2 → ρ2, 0 &lt; ρ2 &lt; ∞. The density functions of F, G and H are positive and continuous at c1 and c2. At the true value P20(P1, P3) of P2(P1, P3), −2logR(P20(P1,P3))→Dχ12 as min(n1, n2, n3) → ∞, where χ12 is chi-square distribution with one degree of freedom.

An EL confidence interval with 100(1 – α)% nominal level is constructed as follows I(α)={P2(P1,P3):−2logR(P2(P1,P3))≤χ12(α)},

where χ12(α) is the upper α-quantile of χ12.

4 | ADJUSTED EMPIRICAL LIKELIHOOD METHOD

We have denoted, hT(X, Y, Z) = (h1(X), h2(Z), h3(Y)), we also have denoted that ΘT = (θ1, θ2, θ3) = (P1, P3, P2(P1, P3)) and OT = (0, 0, 0). Our hypotheses using vector notation are expressed as follows, (12) ∑i=1n1∑j=1n2∑k=1n3(h(Xi,Yj,Zk)−Θ)piqjrk=O,

For the adjusted EL, let us denote

H1i=H1(Xi)=h1(Xi)−θ1;i=1,…,n1 and H1(n1+1)=−a1∑i=1n1(H1i/n1)=−a1H¯1n1,

H2k=H2(Zk)=h2(Zk)−θ2;k=1,…,n3 and H2(n3+1)=−a3∑k=1n3(H2k/n3)=−a3H¯2n3,

H3j=H3(Yj)=h3(Yj)−θ3;j=1,…, n2 and H3(n2+1)=−a2∑j=1n2(H3j/n2)=−a2H¯3n2.

Here as = max(1, log(ns)/2); s = 1, 2, 3. The adjusted empirical log likelihood ratio function is as follows, (13) logRa(c1,c2,P2(P1,P3))=suppi,qj,rk{(∑i=1n1+1log((n1+1)pi)+∑j=1n2+1log((n2+1)qj)+∑k=1n3+1log((n3+1)rk)):pi&gt;0,qj&gt;0,rk&gt;0,∑i=1n1+1pi=1,∑j=1n2+1qj=1,∑k=1n3+1rk=1,∑i=1n1+1∑j=1n2+1∑k=1n3+1piqjrk(H1i,H2k,H3j)T=O}.

We profile out nuisance parameters c1 and c2 and obtain adjusted log-likelihood ratio as follows: −2logRa(P2(P1,P3))=−2maxc1,c2[logRa(c1,c2,P2(P1,P3))]=−2logRa(c^1,c^2,P2(P1,P3)).

We follow the similar derivations like −2 log R(P2(P1,P3)).

The Wilk’s theorem for the adjusted empirical likelihood is established as follows,

Theorem 2:

We assume that n1/n2 → ρ1, 0 &lt; ρ1 &lt; ∞ and n3/n2 → ρ2, 0 &lt; ρ2 &lt; ∞. The density functions of F, G and H are positive and continuous at c1 and c2. At the true value P20(P1, P3) of P2(P1, P3), −2logRa(P20(P1,P3))→Dχ12 as min(n1, n2, n3) → ∞.

An adjusted EL (AEL) confidence interval with 100(1 – α)% nominal level is constructed as follows Ia(α)={P2(P1,P3):−2logRa(P2(P1,P3))≤χ12(α)}.

5 | SIMULATION STUDY

To examine the performance of the proposed confidence intervals, we generated data from normal, beta and combination of gamma, log-normal and Weibull distributions with specificity as 0.8 and sensitivity to the full disease stage as 0.8. We calculated confidence intervals using profile empirical likelihood procedure (PEL), the empirical likelihood procedure using scaled chi-squared distribution with variance estimation from the bootstrap procedure (ELB), the adjusted empirical likelihood procedure (AEL) and the percentile bootstrap procedure (PB). Each simulation was repeated 1000 times and average values of the results were reported. We used 500 bootstrap times for each of the bootstrap procedures.

The usage of a smoothing function can improve the coverage accuracy of empirical likelihood confidence intervals. We used the following function to smooth the indicator function: Iϵ(x,x*)=I(x≤x*)={1,ifx≤x*−ϵ0.5−3(x−x*)4ϵ+(x−x*)34ϵ3,ifx*−ϵ&lt;x≤x*+ϵ0,ifx&gt;x*+ϵ

where ϵ &gt; 0. This is the second order kernel smoothing function. More details can be found in Chen and Hall.34

For the first set of simulations, we considered using normal distribution (simulation 1). When we considered true value of P2 = 0.5, non-diseased observations were generated from N(0, 1), early diseased observations were generated from N(2.5, 1.12) and the diseased observations were generated from N(3.69, 1.22). For the second set of simulations, we considered the true value of P2 = 0.8. Non-diseased observations were generated from N(0, 1), early diseased observations were generated from N(3.5, 1.12) and the diseased observations were generated from N(5.5, 1.22). The results are displayed in Table 1. The PEL procedure provides coverage probabilities closer to 0.95 level with few instances of under coverage probabilities and no significant over coverage probability. The PEL procedure results in the smallest average length of the interval in most of the situations. The ELB procedure performs better than other procedures in few situations. The procedure provides no considerable under coverage probability, but frequently leads to over coverage probability and wider length compared to the PEL procedure. The AEL procedure produces good coverage probability but wider average length and the PB procedure provides over coverage probability and wider average length.

We performed two sets of simulations considering data from beta distribution (simulation 2). For true value of P2 = 0.5, we generated data from β(1, 6), β(6, 6) and β(9.6, 6) for non-diseased, early diseased and fully-diseased individuals, respectively. For the true value of P2 = 0.80, we generated data from β(1, 6), β(9, 6) and β(20.4, 6) for non-diseased, early diseased and fully-diseased individuals, respectively. The results are presented in Table 2. In terms of coverage probability, the proposed PEL method provides closer to 0.95 level compared to other procedures in most of the situations. When comparing the average length, we observe that the PEL procedure results in wider average length compared to the ELB procedure for P2 = 0.5, whereas for P2 = 0.8, the PEL procedure has better performance. Both the AEL and the PB procedures result in over coverage probability and wider length.

We also considered three different distributions for the three-disease groups (simulation 3). For this set of simulations, the non-diseased individuals were generated from gamma distribution (G(6, 12)), the early diseased individuals were generated from log-normal distribution (LN(1.5, 0.5)). For P2 = 0.5, the diseased observations were generated from Weibull distribution with parameters (a, b) = (4, 6.6) and for P2 = 0.8, the diseased observations were generated from Weibull distribution with parameters (a, b) = (4, 10). The results are displayed in Table 3. We observe that the PEL procedure performs very closely to or better than the ELB procedure in terms of coverage probability in most of the situations for P2 = 0.5, whereas the PEL procedure leads to better coverage probability in all the situations for P2 = 0.8. In terms of average length, we observe some instances where the ELB procedure performs better and some instances where the PEL procedure performs better for P2 = 0.5, whereas PEL performs better in the majority of the situations for P2 = 0.8. For smaller P2, both ELB and PEL procedures are competitive whereas for higher P2, the PEL procedure performs better. The AEL procedure performs better than the PB procedure in terms of average length. Both the AEL and the PB procedures lead to over coverage probability in most of the considered situations.

6 | APPLICATION TO REAL DATA

Data used in the preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (http://adni.loni.usc.edu). The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer’s disease (AD).

Alzheimer’s disease is one of the leading causes of dementia. It is a degenerative disease of brain. Three stages of the disease were considered as non-diseased or healthy control, early stage and advanced stage of Alzheimer’s disease. CDR and MMSE scores along with clinical measures were used to classify the subjects into different disease groups. Subjects with MMSE scores 24–30, CDR of 0, non-depressed, non MCI, and non demented were classified as normal. MCI subjects were identified with MMSE scores between 24–30, memory complaint, objective memory loss measured by education adjusted scores on Wechsler Memory Scale Logical Memory II, CDR of 0.5, the absence of significant levels of impairment in other cognitive domains, essentially preserved activities of daily living, and an absence of dementia. Patients were classified with mild AD if they have MMSE scores between 20–26, CDR of 0.5 or 1.0, and meets NINCDS/ADRDA criteria for probable AD. The details can be found in the study protocol (http://adni.loni.usc.edu/wp-content/uploads/2010/09/ADNI_GeneralProceduresManual.pdf). We considered three prospective biomarkers or diagnostic procedures such as ADAS13 score, FDG and the ratio Tau/AB42 (at 24 month visit). We considered the specificity to non-disease stage and sensitivity to full disease stages as 0.8 and 0.8, respectively. We calculated sensitivities to the early disease stage and constructed confidence intervals for the sensitivities. The results are displayed in Table 4. We observe that ADAS13 score has better sensitivity, which is higher than that of other two biomarkers. The confidence intervals for ADAS13 are significantly higher than zero. Hence, ADAS13 score is a better diagnostic procedure compared to other two biomarkers in identifying the early stage of Alzheimer’s disease. There is a shorter confidence interval using proposed PEL approach compared to other approaches for ADAS13 score. For other two biomarkers, we observe very close lengths of the confidence intervals using different approaches. Thus, the proposed procedure performs satisfactorily in real data application.

To observe the performance of ADAS13 score in different situations, we considered other two different values for the target specificity to the non-disease stage (P1) and target sensitivity to the full disease stage stage (P3). For the target P1 as 0.83 and P3 as 0.83, estimated sensitivity to the early disease stage (P2) for ADAS13 is 0.416 with the PEL CI as (0.350, 0.466), for FDG, estimated P2 is 0.007 with 95% PEL CI as (0.006, 0.081), for the ratio Tau/AB42 estimated P2 is 0.054 with 95% PEL CI (0.002, 0.151). For the target P1 = 0.7 and P3 = 0.7, the estimated P2 for ADAS13 is 0.628 with 95% PEL CI as (0.582, 0.658), for FDG the estimated sensitivity is 0.290 with 95% PEL CI as (0.216, 0.375), for the ratio Tau/AB42, the estimated sensitivity is 0.352 with 95% PEL CI as (0.202, 0.476). Hence, ADAS13 performs better compared to other two considered diagnostic procedures to detect the early stage of Alzheimer’s disease in terms of sensitivity for different target specificities to the non-disease stage and sensitivities to the early disease stage.

7 | DISCUSSION

We propose two empirical likelihood confidence intervals for sensitivity to the early disease stage. The performance of the proposed PEL and AEL confidence intervals is investigated using simulation studies in different settings. We compare the performance with two other existing nonparametric approaches like empirical likelihood confidence interval with scaled chi-square distribution using bootstrap variance estimation (ELB), percentile bootstrap procedure (PB), etc. The proposed empirical likelihood confidence intervals perform well in terms of coverage probability and average length of the intervals in different situations. The percentile bootstrap procedure results in over coverage probability with wider average lengths of the intervals. We compared percentile bootstrap procedure with other empirical likelihood procedures and the findings support previous conclusions for similar situations. Dong et al.21 show that the PB procedure performs better in some situations when compared with other parametric or non-parametric procedures. When the empirical likelihood is considered, Dong et al.35 have found that the ELB procedure outperforms other existing procedures including the PB method.

The proposed PEL procedure performs better in most of the situations. The proposed AEL procedure provides over coverage probability for beta distribution (simulation 2). For a normal distribution (simulation 1) and for a mixture of gamma, log-normal and Weibull distributions (simulation 3), the AEL procedure performs well. The ELB procedure outperforms other procedures in some instances but provides over coverage probabilities in many situations. We observe better performance for both PEL and ELB procedures compared to other two procedures, whereas, the ELB procedure shows a tendency to provide over coverage probability. The PEL procedure shows very few instances of over coverage probability, some instances of under coverage probability and closer to 0.95 in the majority of the situations. More simulation studies may be carried out to acquire additional evidences.

There are scopes of improving the computational algorithm and reducing the computational cost. More simulation studies considering different scenarios and other distributions can be of future interest. The empirical likelihood inference for the difference between two sensitivities is also our interest. In the future, we will explore the Bayesian approach to improve coverage probabilities.36

ACKNOWLEDGEMENTS

We would like to thank the editor Dr. Lilly Yue, an associate editor and two reviewers for their helpful comments and insightful suggestions, which improve the paper significantly. Dr. Yichuan Zhao acknowledges the support from the NSA Grant (H89230-20-1-0020) and the Simmons Foundation (grant number: 638679).

Data collection and sharing for this project was funded by the Alzheimer’s Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12-2-0012). ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from the following: AbbVie, Alzheimer’s Association; Alzheimer’s Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research &amp; Development, LLC.; Johnson &amp; Johnson Pharmaceutical Research &amp; Development LLC.; Lumosity; Lundbeck; Merck &amp; Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda Pharmaceutical Company; and Transition Therapeutics. The Canadian Institutes of Health Research is providing funds to support ADNI clinical sites in Canada. Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org). The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimer’s Therapeutic Research Institute at the University of Southern California. ADNI data are disseminated by the Laboratory for NeuroImaging at the University of Southern California.

APPENDIX

A PROOF OF THEOREM 1

Proof of the theorem follows from Zhao37 and Owen.31 At Chapter 11.4, of the book,31 it is shown that for multi-sample case, −2 logR converges to a quadratic form after utilizing Taylor’s approximation. The quadratic form follows a χ2 distribution with one degree of freedom when there is only one dimensional parameter. Using this generalization and Theorem 3.2 of Owen,31 when there are k dimensional parameter of interest (θ), −2 logR ≈ Q converges to χ2 distribution with k degrees of freedom under θ = θ0.

We first assume a general case and later we discuss particular situation for this paper. For the general case, we want to estimate θ = (θ1, ⋯, θk) with hypothesized values being θ0 = (θ10, ⋯, θk0) and the sample estimate from (n1 + n2 + n3) observations being θ^1,⋯,θ^k. We partition first l elements and the remaining k – l elements of these vectors such that Z = (Z1, Z2), where Z1=(θ^1−θ1,⋯,θ^l−θl) and Z2=(θ^l+1−θl+1,⋯,θ^k−θk). We made similar partitioning for variance-covariance matrix of θ. The variance covariance matrix is described as follows: V=1n1+n2+n3(σ11σ12⋯σ1kσ21σ22⋯σ2k⋯⋯⋯⋯σk1σp2⋯σkk)=1n1+n2+n3(V11V12V12TV22),

where σi,j/(n1 + n2 + n3) = σj,i/(n1 + n2 + n3) is the covariance of θi, θj for i, j = 1, 2, 3. Similarly, (n1 + n2 + n3)−1V11 and (n1 + n2 + n3)−1V22 are variance-covariance matrices of Z1 and Z2, respectively. Let B be inverse matrix of V and assume, B=(n1+n2+n3)(B11B12B12TB22).

Then the quadratic form is, (A1) Q=(n1+n2+n3)(Z1,Z2)(B11B12B12TB22)(Z1TZ2T)=(n1+n2+n3)(Z1B11Z1T+Z2B12TZ1T+Z1B12Z2T+Z2B22Z2T).

The quadratic form follows χk2 under θ = θ0. When our interest is on a part of the parameters, new constraint becomes θ2=θ02, where θ = (θ1, θ2), and θ1 = (θ1, ⋯, θl), θ2 = (θl+1, ⋯, θk). We can optimize the likelihood ratio by profiling out l nuisance parameters. When we use the profile empirical likelihood, we minimize −2 logR for l parameters. Thus to minimize the quadratic form over l nuisance parameters, we may take partial derivatives of Q for the parameters in θ1 and set that equal to zero, B11Z1T+B12Z2T=0.

Solving for Z1, one obtains Z1T=−B11−1B12Z2T.

Plugging this into equation (A1), we have, minθ1,⋯,θlQ=(n1+n2+n3)Z2(B22−B12TB11−1B12)Z2T.

Now V22 is the covariance matrix of (n1+n2+n3)Z2. From the property of the inverse of a square matrix, we have, V22=(B22−B12TB11−1B12)−1 and it is a (k – l) dimensional matrix. Using the Central Limit Theorem, (n1+n2+n3)Z2 converges to the multivariate normal distribution with covariance matrix V22 under θ2=θ02. Thus, minθ1,⋯,θlQ→χk−l2.

Under the conditions of the theorem, we consider θ = (c1, c2, P2). We profile out two nuisance parameters (c1, c2). −2logR(P2)=minθ1,θ2{−2logR(θ1,θ2,θ3)}=minc1,c2{−2logR(c1,c2,P2)}.

Hence, according to previous result and Corollary 5 of Qin and Lawless,38 under P2 = P20 we have −2logR(P20)→Dχ12.

□

B PROOF OF THEOREM 2

To sketch the proof of the theorem, we first need to show that the −2 log Ra(θ0) approximates to χ32 distribution. Then using the similar argument as in the proof of Theorem 1, we can show that, when we profile out the nuisance parameters, the EL converges to χ12 distribution. Let us denote la(θ0) = −2 log Ra(θ0). For the adjusted empirical likelihood procedure, we have, la(θ0)=−2∑i=1n1+1log((n1+1)pi)+(−2)∑j=1n2+1log((n2+1)qj)+(−2)∑k=1n3+1log((n3+1)rk)=2∑i=1n1+1log(1+λ1aH1(Xi))+2∑j=1n2+1log(1+λ3aH3(Yj))+2∑k=1n3+1log(1+λ2aH2(Zk))=Q1+Q2+Q3.

Now we can show that the expression converges to a sum of three independent χ12. Like Chen et al.,32 we can show that λ1a=Op(n1−1/2). Here λ1a is the solution of the following equation 0=1n1∑i=1n1+1H1i1+λ1aH1i=H¯1n1−λ1aV^1n1+op(n1−1/2),

where V^1n1=n1−1∑i=1n1H1i2. Thus λ1a≈V^1n1−1H¯1n1 for n1→ ∞. Q1=2∑i=1n1+1{λ1aH1(Xi)−(λ1aH1(Xi))2/2}+op(1)=∑i=1n1+1λ1aH1(Xi)+op(1)=n1H¯1n1V^1n1−1H¯1n1+op(1).

Therefore, Q1 converges to χ12. Similarly, we can show that Q2 and Q3 independently converge to χ12. Thus, la(θ0) converges to a sum of three independent χ12 variables, i.e., χ32 when min(n1, n2, n3)→ ∞. Then, following the same procedure like the proof of Theorem 1, we can show that, −2 logRa(θ) ≈ Q, where Q is the quadratic form of rank three. After profiling out two nuisance parameters, −2 log Ra(P20(P1,P3)) converges to χ12. □

DATA AVAILABILITY STATEMENT

The data that support the findings of this study are available on request from the corresponding author. The data are not publicly available due to privacy restrictions.

FIGURE 1 Density plots of the biomarkers from different disease groups for the simulation studies.

FIGURE 2 Plots of average length (smaller is better) and coverage probability (closer to 0.95 is better) for simulation results from Normal distribution (corresponding to Table 1).

FIGURE 3 Plots of average length (smaller is better) and coverage probability (closer to 0.95 is better) for simulation results FROM Beta distribution (corresponding to Table 2).

FIGURE 4 Plots of average length (smaller is better) and coverage probability (closer to 0.95 is better) for simulation results from mixture of gamma (non-disease), log-normal (early disease) and Weibull (disease) distributions(corresponding to Table 3).

FIGURE 5 Box plots for considered biomarkers from three disease groups.

TABLE 1 Coverage probability (and average length) of 95% confidence intervals for normally distributed samples.

(n1, n2, n3)	PEL 1	ELB 2	AEL 3	PB 4	
P2 = 0.5	
(30, 30, 30)	0.949 (0.532)	0.955 (0.513)	0.957 (0.560)	0.962 (0.594)	
(50, 50, 50)	0.942 (0.424)	0.963 (0.427)	0.955 (0.439)	0.975 (0.471)	
(100, 100, 100)	0.944 (0.294)	0.949 (0.313)	0.944 (0.299)	0.961 (0.331)	
(50, 30, 30)	0.948 (0.521)	0.964 (0.510)	0.956 (0.550)	0.965 (0.593)	
(100, 50, 50)	0.949 (0.412)	0.962 (0.418)	0.958 (0.426)	0.979 (0.465)	
(100, 100, 50)	0.938 (0.357)	0.941 (0.376)	0.944 (0.367)	0.968 (0.413)	
P2 = 0.8	
(30, 30, 30)	0.934 (0.392)	0.964 (0.396)	0.956 (0.427)	0.971 (0.488)	
(50, 50, 50)	0.943 (0.311)	0.966 (0.316)	0.943 (0.322)	0.976 (0.362)	
(100, 100, 100)	0.934 (0.212)	0.957 (0.227)	0.934 (0.217)	0.965 (0.248)	
(50, 30, 30)	0.955 (0.389)	0.972 (0.397)	0.957 (0.432)	0.960 (0.483)	
(100, 50, 50)	0.960 (0.311)	0.968 (0.312)	0.961 (0.323)	0.967 (0.355)	
(100, 100, 50)	0.931 (0.261)	0.957 (0.280)	0.931 (0.268)	0.972 (0.317)	
95% confidence intervals using

1 empirical likelihood procedure

2 scaled chi-square procedure using Bootstrap variance estimation

3 adjusted empirical likelihood procedure

4 bootstrap procedure using percentiles.

TABLE 2 Coverage probability (and average length) of 95% confidence intervals for beta distributed samples.

(n1, n2, n3)	PEL 1	ELB 2	AEL 3	PB 4	
P2 = 0.5	
(30, 30, 30)	0.965 (0.499)	0.965 (0.492)	0.976 (0.527)	0.972 (0.533)	
(50, 50, 50)	0.964 (0.397)	0.955 (0.390)	0.975 (0.412)	0.971 (0.422)	
(100, 100, 100)	0.954 (0.286)	0.943 (0.284)	0.959 (0.293)	0.966 (0.296)	
(50, 30, 30)	0.954 (0.485)	0.959 (0.476)	0.974 (0.515)	0.978 (0.544)	
(100, 50, 50)	0.963 (0.387)	0.959 (0.382)	0.969 (0.402)	0.963 (0.415)	
(100, 100, 50)	0.951 (0.334)	0.948 (0.334)	0.958 (0.345)	0.963 (0.360)	
P2 = 0.8	
(30, 30, 30)	0.968 (0.350)	0.971 (0.364)	0.979 (0.381)	0.967 (0.426)	
(50, 50, 50)	0.954 (0.287)	0.971 (0.283)	0.960 (0.299)	0.970 (0.316)	
(100, 100, 100)	0.953 (0.202)	0.943 (0.204)	0.962 (0.207)	0.958 (0.216)	
(50, 30, 30)	0.959 (0.332)	0.965 (0.361)	0.970 (0.382)	0.970 (0.421)	
(100, 50, 50)	0.959 (0.284)	0.961 (0.284)	0.968 (0.296)	0.965 (0.315)	
(100, 100, 50)	0.970 (0.238)	0.952 (0.237)	0.976 (0.246)	0.970 (0.264)	
95% confidence intervals using

1 empirical likelihood procedure

2 scaled chi-square procedure using Bootstrap variance estimation

3 adjusted empirical likelihood procedure

4 bootstrap procedure using percentiles.

TABLE 3 Coverage probability of (and average length) 95% confidence intervals for samples from mixture of gamma (non-disease), log-normal (early disease) and Weibull (disease) distributions.

(n1, n2, n3)	PEL 1	ELB 2	AEL 3	PB 4	
P2 = 0.5	
(30, 30, 30)	0.962 (0.461)	0.959 (0.449)	0.967 (0.489)	0.971 (0.453)	
(50, 50, 50)	0.950 (0.360)	0.964 (0.361)	0.960 (0.373)	0.947 (0.364)	
(100, 100, 100)	0.933 (0.249)	0.947 (0.260)	0.934 (0.254)	0.965 (0.264)	
(50, 30, 30)	0.956 (0.461)	0.958 (0.447)	0.966 (0.489)	0.964 (0.510)	
(100, 50, 50)	0.941 (0.361)	0.946 (0.360)	0.960 (0.374)	0.968 (0.363)	
(100, 100, 50)	0.939 (0.302)	0.957 (0.309)	0.939 (0.310)	0.960 (0.325)	
P2 = 0.8	
(30, 30, 30)	0.966 (0.368)	0.974 (0.356)	0.975 (0.394)	0.967 (0.426)	
(50, 50, 50)	0.964 (0.279)	0.961 (0.282)	0.967 (0.289)	0.970 (0.316)	
(100, 100, 100)	0.946 (0.193)	0.964 (0.201)	0.946 (0.197)	0.958 (0.216)	
(50, 30, 30)	0.954 (0.377)	0.967 (0.357)	0.967 (0.402)	0.970 (0.421)	
(100, 50, 50)	0.955 (0.285)	0.964 (0.283)	0.955 (0.296)	0.965 (0.315)	
(100, 100, 50)	0.952 (0.235)	0.958 (0.236)	0.954 (0.241)	0.970 (0.264)	
95% confidence intervals using

1 empirical likelihood procedure

2 scaled chi-square procedure using Bootstrap variance estimation

3 adjusted empirical likelihood procedure

4 bootstrap procedure using percentiles.

TABLE 4 Estimated sensitivities to the early disease stage and 95% confidence intervals for ADNI data.

Diagnostic test	Estimated P2	PEL 1	ELB 2	AEL 3	PB 4	
ADAS13	0.479	(0.411, 0.524)	(0.421, 0.537)	(0.411, 0.524)	(0.405, 0.528)	
FDG	0.078	(0.015, 0.160)	(0.026, 0.170)	(0.015, 0.160)	(0.005, 0.158)	
Tau/AB42	0.097	(0.002, 0.251)	(0.021, 0.252)	(0.002, 0.254)	(0.000, 0.234)	
95% confidence intervals using

1 empirical likelihood procedure

2 scaled chi-square procedure using Bootstrap variance estimation

3 adjusted empirical likelihood procedure

4 bootstrap procedure using percentiles.

2 Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf


References

1. Pepe MS . The Statistical Evaluation of Medical Tests for Classification and Prediction. New York: Oxford University Press Inc.. 2003.
2. Zhou XH , Obuchowski NA , McClish DK . Statistical Methods in Diagnostic Medicine. New York: John Wiley &amp; Sons. 2002.
3. Zou KH , Liu A , Bandos AI , Ohno-Machado L , Rockette HE . Statistical Evaluation of Diagnostic Performance: Topics in ROC Analysis. Boca Raton, FL: Chapman &amp; Hall/CRC. 2011.
4. James MT , Hemmelgarn BR , Tonelli M . Early recognition and prevention of chronic kidney disease. The Lancet 2010; 375 (9722 ): 1296–1309.
5. Levey AS , Coresh J , Balk E , National Kidney Foundation practice guidelines for chronic kidney disease: evaluation, classification, and stratification. Annals of Internal Medicine 2003; 139 (2 ): 137–147.12859163
6. Insel RA , Dunne JL , Atkinson MA , Staging presymptomatic type 1 diabetes: a scientific statement of JDRF, the Endocrine Society, and the American Diabetes Association. Diabetes Care 2015; 38 (10 ): 1964–1974.26404926
7. Sharma N , Singh AN . Exploring biomarkers for Alzheimer’s disease. Journal of Clinical and Diagnostic Research: JCDR 2016; 10 (7 ): KE01–KE06.
8. Nestor PJ , Scheltens P , Hodges JR . Advances in the early detection of Alzheimer’s disease. Nature Medicine 2004; 10 (7s ): S34–s41.
9. Caroli A , Frisoni G , Initiative ADN . The dynamics of Alzheimer’s disease biomarkers in the Alzheimer’s Disease Neuroimaging Initiative cohort. Neurobiology of Aging 2010; 31 (8 ): 1263–1274.20538373
10. Kuhlmann T , Lingfeld G , Bitsch A , Schuchardt J , Brück W . Acute axonal damage in multiple sclerosis is most extensive in early disease stages and decreases over time. Brain 2002; 125 (10 ): 2202–2212.12244078
11. Dreiseitl S , Ohno-Machado L , Binder M . Comparing three-class diagnostic tests by three-way ROC analysis. Medical Decision Making 2000; 20 (3 ): 323–331.10929855
12. Nakas CT , Yiannoutsos CT . Ordered multiple-class ROC analysis with continuous measurements. Statistics in Medicine 2004; 23 (22 ): 3437–3449.15505886
13. Kang L , Tian L . Estimation of the volume under the ROC surface with three ordinal diagnostic categories. Computational Statistics &amp; Data Analysis 2013; 62 : 39–51.
14. Heckerling PS . Parametric three-way receiver operating characteristic surface analysis using mathematica. Medical Decision Making 2001; 21 (5 ): 409–417.11575490
15. Luo J , Xiong C . DiagTest3Grp: an R package for analyzing diagnostic tests with three ordinal groups. Journal of Statistical Software 2012; 51 (3 ): 1–24.23504300
16. Kapasnỳ J , Řezáč M . Three-way ROC analysis using SAS Software. Acta Universitatis Agriculturae et Silviculturae Mendelianae Brunensis 2013; 61 (7 ): 2269–2275.
17. Xiong C , Luo J , Chen L , Estimating diagnostic accuracy for clustered ordinal diagnostic groups in the three-class case-application to the early diagnosis of Alzheimer disease. Statistical Methods in Medical Research 2018; 27 (3 ): 701–714.29182052
18. Zhou XH , Qin G . Improved confidence intervals for the sensitivity at a fixed level of specificity of a continuous-scale diagnostic test. Statistics in Medicine 2005; 24 (3 ): 465–477.15635678
19. Qin G , Davis AE , Jing BY . Empirical likelihood-based confidence intervals for the sensitivity of a continuous-scale diagnostic test at a fixed level of specificity. Statistical Methods in Medical Research 2011; 20 (3 ): 217–231.19654172
20. Tian L . Confidence interval estimation for sensitivity at a fixed level of specificity for combined biomarkers. Journal of Biopharmaceutical Statistics 2013; 23 (3 ): 499–512.23611191
21. Dong T , Tian L , Hutson A , Xiong C . Parametric and non-parametric confidence intervals of the probability of identifying early disease stage given sensitivity to full disease and specificity with three ordinal diagnostic groups. Statistics in Medicine 2011; 30 (30 ): 3532–3545.22139763
22. Dong T , Tian L . Confidence interval estimation for sensitivity to the early diseased stage based on empirical likelihood. Journal of Biopharmaceutical Statistics 2015; 25 (6 ): 1215–1233.25372999
23. Liu X , Zhao Y . Semi-empirical likelihood inference for the ROC curve with missing data. Journal of Statistical Planning and Inference 2012; 142 (12 ): 3123–3133.
24. Yang H , Zhao Y . Smoothed empirical likelihood for ROC curves with censored data. Journal of Multivariate Analysis 2012; 109 : 254–263.
25. Yang H , Zhao Y . Smoothed jackknife empirical likelihood inference for the difference of ROC curves. Journal of Multivariate Analysis 2013; 115 : 270–284.
26. Yang H , Zhao Y . Smoothed jackknife empirical likelihood inference for ROC curves with missing data. Journal of Multivariate Analysis 2015; 140 : 123–138.
27. Yang H , Lu K , Zhao Y . A nonparametric approach for partial areas under ROC curves and ordinal dominance curves. Statistica Sinica 2017; 27 : 357–371.
28. Wang D , Tian L , Zhao Y . Smoothed empirical likelihood for the Youden index. Computational Statistics &amp; Data Analysis 2017; 115 : 1–10.
29. Owen AB . Empirical likelihood ratio confidence intervals for a single functional. Biometrika 1988; 75 (2 ): 237–249.
30. Owen AB . Empirical likelihood ratio confidence regions. The Annals of Statistics 1990; 18 (1 ): 90–120.
31. Owen AB . Empirical Likelihood. New York: Chapman and Hall/CRC. 2001.
32. Chen J , Variyath AM , Abraham B . Adjusted empirical likelihood and its properties. Journal of Computational and Graphical Statistics 2008; 17 (2 ): 426–443.
33. Agresti A , Coull BA . Approximate is better than “exact” for interval estimation of binomial proportions. The American Statistician 1998; 52 (2 ): 119–126.
34. Chen SX , Hall P . Smoothed empirical likelihood confidence intervals for quantiles. The Annals of Statistics 1993; 21 (3 ): 1166–1181.
35. Dong T , Kang L , Hutson A , Xiong C , Tian L . Confidence interval estimation of the difference between two sensitivities to the early disease stage. Biometrical Journal 2014; 56 (2 ): 270–286.24265123
36. Cheng Y , Zhao Y . Bayesian jackknife empirical likelihood. Biometrika 2019; 106 (4 ): 981–988.
37. Zhao Y . Statistical inference on trimmed means, Lorenz curves, and partial area under ROC curves by empirical likelihood method. Theses and Dissertations–Statistics, University of Kentucky 2016; 24.
38. Qin J , Lawless J . Empirical likelihood and general estimating equations. The Annals of Statistics 1994; 22 (1 ): 300–325.
