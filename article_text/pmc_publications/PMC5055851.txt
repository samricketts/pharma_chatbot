LICENSE: This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.


0370625
1170
Biometrics
Biometrics
Biometrics
0006-341X
1541-0420

27061414
5055851
10.1111/biom.12526
NIHMS778174
Article
Single-index Varying Coefficient Model for Functional Responses
Luo Xinchao 12
Zhu Lixing 3
Zhu Hongtu 2
1 School of Finance and Statistics, East China Normal University, Shanghai, China
2 Department of Biostatistics and Biomedical Research Imaging Center, University of North Carolina at Chapel Hill, Chapel Hill, North Carolina, U.S.A
3 Department of Mathematics, Hong Kong Baptist University, Hong Kong, China
16 4 2016
08 4 2016
12 2016
01 6 2017
72 4 12751284
This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.
Summary

Recently, massive functional data have been widely collected over space across a set of grid points in various imaging studies. It is interesting to correlate functional data with various clinical variables, such as age and gender, in order to address scientific questions of interest. The aim of this paper is to develop a single-index varying coefficient (SIVC) model for establishing a varying association between functional responses (e.g., image) and a set of covariates. It enjoys several unique features of both varying-coefficient and single-index models. An estimation procedure is developed to estimate varying coefficient functions, the index function, and the covariance function of individual functions. The optimal integration of information across different grid points are systematically delineated and the asymptotic properties (e.g., consistency and convergence rate) of all estimators are examined. Simulation studies are conducted to assess the finite-sample performance of the proposed estimation procedure. Furthermore, our real data analysis of a white matter tract dataset obtained from the Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI) study confirms the advantage and accuracy of SIVC model over the popular varying coefficient model.

Functional response
Image analysis
Single index
Uniform convergence
Varying coefficient

1. Introduction

As a semiparametric regression modelling strategy, single-index modelling has attracted much attention in the literature due to its balance between exibility and fidelity. A classical single-index model is often written as (1) Y=g(XTÎ²)+Îµ,

where Y is a response variable, g(Â·) is an unknown index function, X is a covariate vector, and Îµ is an error term such that E(Îµ|X) = 0. See Horowitz (2009) for a comprehensive review of various estimation methods for single-index models and references therein (Li, 1991; Cook and Weisberg, 1991; Zhu et al., 2010; Xia et al., 2002; Xia, 2007; Ma and Zhu, 2012, 2013). For instance, dimension reduction approaches, such as likelihood-based methods (Cook and Forzani, 2009), are also commonly used for estimation. However, the existing literature primarily considers univariate response observed from cross-sectional studies.

This paper is motivated by the analysis of a real diffusion weighted imaging (DWI) data set with n = 213 subjects collected from NIH Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI) study. For each subject, we calculated a Fractional Anisotropy (FA) curve at all the 83 grid points along the skeleton of the midsagittal corpus callosum as shown in Figure 1. We are interested in establishing an association between FA curves and several covariates of interest, such as age and gender. To establish such association, standard grid-wise methods are to fit a linear model to functional observations at each grid point as responses and clinical variables, such as age and gender, as covariates, and to generate a statistical map of test statistics or p-values across all grid points (Lazar, 2008;Worsley et al., 2004). These grid-wise methods have several major limitations. First, compared with model (1), the classical linear model used in the neuroimaging literature is often restrictive, since it assumes that the index function g(Â·) is an identity function. When g(Â·) is truly nonlinear, directly fitting a classical linear model can cause substantial efficiency loss and reduce prediction accuracy. Second, since the grid-wise methods treat all grid points as independent units, they ignore two key functional features of functional data including spatial smoothness and spatial correlation.

Some advanced methods have been developed to specifically incorporate these features by using function-on-scalar regression under the functional data analysis (FDA) framework (Zhu et al., 2012; Ramsay and Silverman, 2005; Staicu et al., 2010; Zhang and Chen, 2007; Reiss et al., 2010). Some important estimation methods for FDA include adaptive smoothing methods (Polzehl and Spokoiny, 2006; Li et al., 2011), the integration of FDA and adaptive smoothing methods (Zhu et al., 2014), and spatial priors within the Bayesian framework (Gossl et al., 2001; Penny et al., 2005; Bowman et al., 2008; Smith and Fahrmeir, 2007; Yue et al., 2010; Miranda et al., 2013), among others. See Morris (2015) and Wang et al. (2015) for a comprehensive review of various FDA models for functional responses. However, to the best of our knowledge, none of the references cited above address the two functional features and estimate the nonparametric index function simultaneously.

The aim of this paper is to develop a single-index varying coefficient (SIVC) model to establish a varying association between functional responses and a set of covariates. Specifically, we extend the single-index model (1) to SIVC for functional responses as follows: (2) Y(s)=g(XTÎ²(s))+Îµ(s),

where {Y (s) : s âˆˆ ğ’®} is an observed stochastic process on a compact set ğ’® and Îµ(s) is a random function characterizing the within-subject correlations and measurement errors at different grid points such that E{Îµ(s)|X} = 0 for all s âˆˆ ğ’®. The Î²(s) allows us to characterize the dynamic association between covariate X and functional response, whereas g(Â·) is a nonparametric function, while being fixed across all s âˆˆ ğ’®. Model (2) differs from the functional single index model in Jiang and Wang (2011), in which g(Â·) varies across s, but Î²(s) is assumed to be stationary. When g(x) = x, model (2) reduces to standard voxel-wise methods based on linear model and the most popular FDA model considered in (Zhang and Chen, 2007; Ramsay and Silverman, 2005; Zhu et al., 2014). For notational simplicity, we set ğ’® = [0, 1]. The results can be readily extended to more general cases with compact subset ğ’® of the Euclidean space.

Compared with the existing literature, we make several unique contributions. (i) We develop a new estimation procedure to estimate various parametric and non-parametric components of SIVC. (ii) Theoretically, we delineate the integration of information across all grid points by using an optimal weight function and then establish the asymptotic properties of various estimates for SIVC. (iii) Our analysis of the ADNI data confirms the advantage and accuracy of SIVC model over the popular varying coefficient model (Zhang and Chen, 2007; Ramsay and Silverman, 2005; Zhu et al., 2014).

The rest of this paper is organized as follows. Section 2 introduces the estimation procedure to estimate varying coefficient functions, index function, and the covariance function of individual functions. Section 3 systematically investigates the asymptotic properties of all estimators. A simulation study and a real data analysis of Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI) study are presented in Section 4 to demonstrate the finite sample performance of SIVC. Section 5 concludes with some discussions.

2. Methods

2.1 Single-index Varying Coefficient Model

We formally introduce the single-index varying coefficient model as follows. Consider {({Yi(s) : s âˆˆ ğ’®}, Xi) : i = 1, â€¦, n} from n independent subjects, where ğ’® is a compact set that characterizes the range of all possible grid points. Our single-index varying coefficient (SIVC) model is written as (3) Yi(s)=g(XiTÎ²(s))+Î·i(s)+Îµi(s)Â for allÂ sâˆˆğ’®Â andÂ i=1,â€¦,n,

where Xi is a p Ã— 1 covariate vector, Î²(s) = (Î²1(s), â‹¯, Î²p(s))T is a p Ã— 1 vector of varying coefficient functions, g(Â·) is an unknown index function, Î·i(s) characterizes individual curve variations, and Îµi(s) is a random function of measurement errors. The process {Î·i(s) : s âˆˆ ğ’®} is assumed to be a Gaussian process with zero mean and a covariance function Î£Î·(s, t) = Cov{Î·(s), Î·(t)}. The error terms Îµi(s) follow a Gaussian process with zero mean and a diagonal covariance function Cov{Îµ(s), Îµ(t)}. That is, Îµi(s) and Îµi(t) are assumed to be independent for s â‰  t, and Cov{Îµ(s), Îµ(t)} takes the form of ÏƒÎµ2(s)1(s=t), where 1(Â·) is an indicator function. Moreover, Yi(s) are usually measured at the same set of locations for all subjects and exhibit both the within-curve and between-curve dependence structures. Thus, without loss of generality, it is assumed that Yi(s) are observed on M grid points ğ’®M = {sm : 0 = s1 â‰¤ â‹¯ â‰¤ sM = 1} for all subjects.

For single index model, Î²(s) is not identifiable since g(XT Î²(s)) and g(Î²0(s) + Î´XT Î²(s)) are not distinguishable, where Î´ is any nonzero scalar. A simple solution in the literature (Zhu et al., 2010; Xia et al., 2002) is to impose some constraints on Î²0(s) and Î²(s), such as Î²0(s) = 0 and Î²(s)T Î²(s) = 1. Therefore, throughout the paper, it is assumed that X does not contain the intercept, Î²(s)T Î²(s) = 1 holds for all s âˆˆ ğ’® and the first entry of Î²(s) is positive at each s.

2.2 Estimation Procedure

Our estimation procedure consists of three steps for estimating the varying coefficient functions Î²(Â·), the index function g(Â·), and the covariance function Î£Î·(s, sâ€²).

Step I. Estimating varying coefficient Î²(s)

We first consider the estimation of Î²(sm) at each given grid point sm. Model (3) reduces to a classical single-index model given by (4) Yi(sm)=g(XiTÎ²(sm))+Îµi*(sm),

where Îµi*(sm)=Î·i(sm)+Îµi(sm) such that E{Îµi*(sm)|Xi}=0 and Var{Îµ*(sm)|Xi}=ÏƒÎµ*2(sm)=Î£Î·(sm,sm)+ÏƒÎµ2(sm). The likelihood function of a random observation (X, Y (sm)) in model (4) is given by f1(X)Ã—f2(Y(sm)âˆ’g(XTÎ²(sm));X),

where f1 is the probability density function of X, and f2 is the conditional probability density function of Îµ*(sm) = Y (sm) âˆ’ g(XT Î²(sm)) given X.

Based on the Gaussian assumption of Îµi*(s), f2(Â·) is the normal density with mean zero and variance ÏƒÎµ*2(sm). Thus, the score function for Î²(sm), denoted as S(Î²(sm)), is given by (5) S(Î²(sm))=âˆ’XÎµ*(sm)Ä¡(XTÎ²(sm))/ÏƒÎµ*2(sm)=(I)+(II)=âˆ’E{X|XTÎ²(sm)}Îµ*(sm)Ä¡(XTÎ²(sm))/ÏƒÎµ*2(sm)âˆ’[Xâˆ’E{X|XTÎ²(sm)}]Îµ*(sm)Ä¡(XTÎ²(sm))/ÏƒÎµ*2(sm),

where Ä¡(t) = dg(t)/dt. Following the reasoning in Ma and Zhu (2014), (I) and (II) belong to the tangent space of model (4) with respect to Î²(sm), denoted by Î›g(sm), and its orthogonal component, denoted by Î›g(sm)âŠ¥, respectively. For any s âˆˆ ğ’®, Î›g(s) and Î›g(s)âŠ¥ are, respectively, given by Î›g(s)={âˆ’Îµ*(s)h(XTÎ²(s))/ÏƒÎµ*2(s):âˆ€h(â‹…)},

Î›g(s)âŠ¥={âˆ’Îµ*(s)[Î±(X)âˆ’E{Î±(X)|XTÎ²(s)}]:âˆ€Î±(Â·)}.

Therefore, the efficient score function for Î²(sm) is given by (6) Seff(Î²(sm);X,Y(sm))=Îµ*(sm)[Xâˆ’E{X|XTÎ²(sm)}]Ä¡(XTÎ²(sm)).

To calculate an efficient estimator of Î²(sm), denoted as Î²Ìƒ(sm), we can solve (7) âˆ‘i=1nSeff(Î²Ëœ(sm);Xi,Yi(sm))=0.

Since Seff depends on three unknown quantities E{X|XT Î²(sm)}, g(XT Î²(sm)) and Ä¡(XT Î² (sm)), we construct their nonparametric estimators as follows (Ma and Zhu, 2013). The Nadaraya-Watson kernel estimator of E{X|XT Î²(sm)} is given by (8) ÃŠ{X|XTÎ²(sm)}=âˆ‘i=1nXiKhx(XiTÎ²(sm)âˆ’XTÎ²(sm))âˆ‘i=1nKhx(XiTÎ²(sm)âˆ’XTÎ²(sm)),

where Kh(Â·) = K(Â·/h)/h is a kernel function and hx is a given bandwidth. We can calculate the estimates of g(XT Î²(sm)) and Ä¡(XT Î²(sm)) at XT Î²(sm) = XT Î²0(sm), denoted by Ä(XT Î²0(sm)) and Ä¡Ì‚(XT Î²0(sm)), by minimizing (9) âˆ‘i=1n[Yi(sm)âˆ’g(XTÎ²0(sm))âˆ’Ä¡(XTÎ²0(sm))(XiTâˆ’XT)Î²0(sm)]2Khy((XiTâˆ’XT)Î²0(sm)).

As suggested by Ma and Zhu (2014), we set hx = cnâˆ’1/3 and hy = cnâˆ’1/5, where c is the average standard deviation of X. By plugging ÃŠ{X|XT Î²(sm)}, Ä(XT Î²0(sm)) and Ä¡Ì‚(XT Î²0(sm)) into (7), we get an estimate of Seff(Î²(sm); Xi, Yi(sm)), denoted by Åœeff(Î²(sm); Xi, Yi(sm)) and then calculate Î²Ìƒ(sm) by solving (10) âˆ‘i=1nÅœeff(Î²Ëœ(sm);Xi,Yi(sm))=0.

To estimate Î²(s) at any s âˆˆ ğ’®, we need to construct a weighted estimating equation to borrow information across all grid points by using the functional features of imaging data (Wang et al., 2004; Zhu et al., 2012; Polzehl and Spokoiny, 2006; Li et al., 2011). Specifically, the weighted estimating equation is given by (11) ÅœnM(Î²(s);w)=âˆ‘i=1nâˆ‘m=1Mw(sm,s)Åœeff(Î²(s);Xi,Yi(sm)),

where w(sm, s) is a weight function of (sm, s) and may depend a few parameters, such as bandwidth. Then, we calculate an estimate of Î²(s), denoted by Î²Ì‚(s), by solving the following equation: (12) ÅœnM(Î²^(s);w)=0.

A critical issue in (12) is how to select an optimal weight function w(sm, s). Theoretically, one may choose w(sm, s) by minimizing the mean integrated squares error (MISE) of Î²Ì‚(Â·) for all s âˆˆ ğ’®, but it can be challenging due to the lack of precise information about Î²(s). Without such information, a simple solution is to set w(sm, s) = K((sm âˆ’ s)/h)/h and then select the bandwidth h by using some criteria, such as cross-validation method, based on MISE. Furthermore, when Î²(s) is a piecewise constant function, we will show how to optimally select w(sm, s) in Section 3.

Step II. Estimating the unknown index function g(Â·)

We use the local linear technique to estimate g(XT Î²(s)). We define Zi,m(s)=(1,{XiTÎ²(sm)âˆ’XTÎ²(s)}/h1)T and Î£(XTÎ²(s),h1)=âˆ‘i=1nâˆ‘m=1MKh1(XiTÎ²(sm)âˆ’XTÎ²(s))Zi,m(s)âŠ—2,

where ZâŠ—2 = ZZT for any vector Z. By replacing Î²(sm) by Î²Ì‚(sm), we get áºi,m(s) and Î£Ì‚(XT Î²(s), h1). Denote G(XT Î² (s)) = (g(XT Î² (s)), h1Ä¡(XT Î² (s)))T, we directly minimize a weighted least square function given by (13) âˆ‘i=1nâˆ‘m=1M{Yi(sm)âˆ’áºi,m(s)TG(XTÎ²(s))}2Kh1(XiTÎ²^(sm)âˆ’XTÎ²(s)).

Thus, we have Ä(XT Î²(s)) = [1 0]Äœ (XT Î² (s)), where Äœ (XT Î²(s)) is given by Î£^(XTÎ²(s),h1)âˆ’1âˆ‘i=1nâˆ‘m=1MKh1(XiTÎ²^(sm)âˆ’XTÎ²(s))áºi,m(s)Yi(sm).

The bandwidth h1 is chosen by using the cross-validation method.

Step III. Estimating the covariance function Î£Î·(s, t)

Let di(s) = (Î·i(s), h2Î·Ì‡i(s))T, Wm,s = (1, (smâˆ’s)/h2)T, and Yi*(sm)=Yi(sm)âˆ’Ä(XiTÎ²^(sm)). We minimize the following function (14) âˆ‘m=1M{Yi*(sm)âˆ’Wm,sTdi(s)}2Kh2(smâˆ’s)

to obtain d^i(s)=(âˆ‘m=1MKh2(smâˆ’s)Wm,sâŠ—2)âˆ’1âˆ‘m=1MKh2(smâˆ’s)Wm,sYi*(sm).

Then, Î·(s) can be estimated by (15) Î·^i(s)=[10]d^i(s)=âˆ‘m=1MKËœh2(smâˆ’s)Yi*(sm),

where KËœh2(smâˆ’s)=[10](âˆ‘m=1MKh2(smâˆ’s)Wm,sâŠ—2)âˆ’1Kh2(smâˆ’s)Wm,s, which is the empirical equivalent kernel. The bandwidth h2 is also chosen by using the cross-validation method.

We consider the spectral decomposition of Î£Î·(s, t) and its approximation. Suppose Î£Î·(s, t) is continuous on ğ’®2, then according to Mercerâ€™s theorem, it can be decomposed as Î£Î·(s,t)=âˆ‘k=1âˆÎ»kÏˆk(s)Ïˆk(t),

where Î»1 â‰¥ Î»2 â‰¥ â‹¯ â‰¥ 0 are ordered eigenvalues and Ïˆk(s) are the corresponding orthonormal eigenfunctions. Furthermore, the eigenfunctions form an orthonormal system on the space of square-intergrable function on ğ’®, and Î·i(s) admits the Karhunen-Loeve expansion as Î·i(s)=âˆ‘k=1âˆÎ¾ikÏˆk(s), where Î¾ik=âˆ«01Î·i(s)Ïˆk(s)ds is the k-th functional principal component scores of the i-th subject. For a fixed i, Î¾ik are uncorrelated random variables with mean zero and variance Î»k.

By following Rice and Silverman (1991), the covariance matrix Î£Î·(s, t) and its spectral decomposition can be estimated by (16) Î£^Î·(s,t)=(nâˆ’p)âˆ’1âˆ‘i=1nÎ·^i(s)Î·^i(t)=âˆ‘k=1âˆÎ»^kÏˆ^k(s)Ïˆ^k(t),

where Î»Ì‚1 â‰¥ Î»Ì‚2 â‰¥ â‹¯ â‰¥ 0 are estimated eigenvalues and ÏˆÌ‚k(s) are the corresponding estimated eigenfunctions. Moreover, the k-th functional principal component scores can be computed by Î¾^ik=âˆ‘m=1MÎ·^i(sm)Ïˆ^k(sm)(smâˆ’smâˆ’1) for i = 1, â€¦, n.

2.3 Simultaneous Confidence Bands

Given a Confidence level Î±, we construct a simultaneous Confidence band for each Î²l(s) such that P(Î²^lL,Î±(s)&lt;Î²l(s)&lt;Î²^lU,Î±(s)Â for allÂ sâˆˆğ’®)=1âˆ’Î± where Î²^lL,Î±(s) and Î²^lU,Î±(s) are the lower and upper limits of simultaneous Confidence band, respectively. Specifically, we set (17) Î²^lL,Î±(s)=Î²^l(s)âˆ’bias(Î²^l(s))âˆ’CÎ²l(Î±)Â andÂ Î²^lU,Î±(s)=Î²^l(s)âˆ’bias(Î²^l(s))+CÎ²l(Î±),

where bias(Î²Ì‚l(s)) is the bias of Î²Ì‚l(s) at s âˆˆ ğ’® and CÎ²l(Î±) is a scalar. By following the arguments in Zhu et al. (2012), we use the local polynomial technique to estimate bias(Î²Ì‚l(s)) for each l and then approximate CÎ²l(Î±) by using the wild bootstrap as follows: Step 1: We calculate r^i(sm)=Yi(sm)âˆ’Ä(XiTÎ²^(sm)) for all i and m.

Step 2: For q = 1, â€¦, Q, we independently generate {Ï„i(q) : i = 1, â‹¯, n} from N(0, 1) and construct Å¶i(sm)(q)=Ä(XiTÎ²^(sm))+Ï„i(q)r^i(sm). Then, based on {Yi(sm)(q)}, we recalculate Î²Ì‚(s)(q), and obtain a stochastic process GÎ²l(s)(q) = | Î²Ì‚l(s)(q) âˆ’ Î²Ì‚l(s)(q)| for each l.

Step 3: For all q, we calculate the 1 âˆ’ Î± empirical percentile of GÎ²l(s)(q), denoted by CÎ²l(s, Î±), at each s âˆˆ ğ’®. Finally, an estimate of CÎ²l(Î±) is sups |CÎ²l(s, Î±)|.

Similarly, for a given Î±, we construct a simultaneous Confidence band for g(Â·) as follows: P(ÄL,Î±(u)&lt;g(u)&lt;ÄU,Î±(u)Â for allÂ uâˆˆğ’°)=1âˆ’Î±,

where ğ’° is a compact set in R and ÄL,Î±(u) and ÄU,Î±(u) are the lower and upper limits of simultaneous Confidence band, respectively. Specifically, we set (18) ÄL,Î±(u)=Ä(u)âˆ’bias(Ä(u))âˆ’Cg(Î±)Â andÂ ÄU,Î±(u)=Ä(u)âˆ’bias(Ä(u))+Cg(Î±).

Subsequently, we use the local polynomial technique to estimate bias(Ä(u)) and then use the wild bootstrap to approximate Cg(Î±) as follows: Step 1: We calculate r^i(sm)=Yi(sm)âˆ’Ä(XiTÎ²^(sm)) for all i and m.

Step 2: For q = 1, â€¦, Q, we independently generate {Ï„i(q) : i = 1, â‹¯, n} from N(0, 1) and construct Å¶i(sm)(q)=Ä(XiTÎ²^(sm))+Ï„i(q)r^i(sm). Then, based on {Yi(sm)(q)}, we recalculate Ä(Â·)(q), and obtain a stochastic process Gg(u)(q) = |Ä(u) âˆ’ Ä(u)(q)|.

Step 3: For all â„“, we calculate the 1 âˆ’ Î± empirical percentile of Gg(u)(q) denoted by Cg(u, Î±) at every u âˆˆ ğ’° and then approximate Cg(Î±) by using supuâˆˆğ’° |Cg(u, Î±)|.

3. Theoretical Results

3.1 Optimal weight Functions

We consider a challenging issue of optimally selecting the weight function w(sm, s) in order to gain efficiency, since ÅœnM (Î²(s);w) for a given weight function w(Â·, Â·) may not be an efficient estimating equation. Specifically, Î›g(s) and Î›g(sâ€²) may interact with each other for s â‰  sâ€² by noting that (19) E[Îµ*(s)h(XTÎ²(s))Îµ*(sâ€²)hâ€²(XTÎ²(sâ€²))/{ÏƒÎµ*2(s)ÏƒÎµ*2(sâ€²)}]=Î£Î·(s,sâ€²)E{h(XTÎ²(s))hâ€²(XTÎ²(sâ€²))}/{ÏƒÎµ*2(s)ÏƒÎµ*2(sâ€²)}.

Therefore, Î›g(s) and Î›g(sâ€²) are orthogonal with each other for s â‰  sâ€² if and only if Î£Î·(s, sâ€²) = 0 for s â‰  sâ€². It also holds for {Î›g(s)âŠ¥ : s âˆˆ ğ’®} due to Î›g(s) âŠ¥ Î›g(s)âŠ¥.

First, we consider how to choose the weight function when Î²(s) = Î²0 does not vary across s âˆˆ ğ’®. With some calculations, we can show that the covariance matrix of nÎ²^(s) can be approximated by (20) Î£1(w(s))=D(w(s))E{[Xâˆ’E{X|XTÎ²0}]âŠ—2Ä¡(XTÎ²0)2}âˆ’1,

where w(s) = (w(s1, s), â‹¯, w(sM, s))T and D(w(s)) is given by (21) âˆ‘m,mâ€²=1Mw(sm,s)w(smâ€²,s){Î£Î·(sm,smâ€²)+ÏƒÎµ2(sm)1(sm=smâ€²)}{âˆ‘m=1Mw(sm,s)}2.

We can obtain an optimal weight vector, denoted by w*, by minimizing Î£1(w(s)) such that w* = argminw(s)Î£1(w(s)). As shown in Theorem 1 (i) below, w* is associated with the eigenvalue-eigenvector pairs of Î›Îµ*,Mâˆ’1/2Î£Î·*,MÎ›Îµ*,Mâˆ’1/2, denoted by {(Î»m*,M, Ïˆm*,M) : m = 1, â€¦, M}, where Î£Î·*,M = (Î£Î·(sm, smâ€²)) and Î›Îµ*,M=(ÏƒÎµ2(sm)1(sm=smâ€²)) are two M Ã— M matrices.

Second, we set Ï‰(sm, s) as Kh(sm âˆ’ s), which is a kernel function of (sm âˆ’ s)/h, when Î²(s) may vary across s âˆˆ ğ’®. If h â†’ 0, then it can be shown that the covariance matrix of nÎ²^(s) can be approximated by Î£1(w(s)) in (20). We will show in Theorem 1 (ii) that the use of the kernel function can lead to substantial efficiency gain even under this general scenario.

Theorem 1

We have the following results. Suppose that Î²(s) = Î²0 does not vary across s âˆˆ ğ’®. The optimal w* is given by (22) w*=âˆ‘Îµ*,Mâˆ’11M/â€–âˆ‘Îµ*,Mâˆ’11Mâ€–2,

where â€–Â·â€–2 is the Euclidean norm of a vector, Î£Îµ*,M = Î£Î·*,M+Î›Îµ*,M is an MÃ—M matrix, and 1M is an M Ã— 1 vector of ones. Thus, the optimal D(w) is given by D(w*)=(1MTâˆ‘Îµ*,Mâˆ’11M)âˆ’1 and is independent of s. The Î£Îµ*,Mâˆ’11M can be written as (23) {IMâˆ’Î›Îµ*,Mâˆ’1/2âˆ‘m=1MÎ»m*,M1+Î»m*,MÏˆm*,MÏˆm*,MTÎ›Îµ*,Mâˆ’1/2}Î›Îµ*,Mâˆ’1/21M.

Suppose that Î²(s) may vary across s âˆˆ ğ’®. Under Assumptions (C6) and (C7), if w(sm, s) = Kh(sm âˆ’ s), h â†’ 0, and Mh â†’ âˆ, then D(w(s)) can be approximated by Î£Î·(s, s).

Theorem 1 has several interesting implications. If Î£Î·(s, sâ€²) = 0 for any s â‰  sâ€², then w* is proportional to Î›Îµ*,Mâˆ’11M=(ÏƒÎµâˆ’2(s1),â‹¯,ÏƒÎµâˆ’2(sM))T. In this case, we can set w(sm,s)=ÏƒÎµâˆ’2(sm) for all m, and then the optimal weighted estimating equation is given by ÅœnM(Î²(s);w*)=âˆ‘m=1Mâˆ‘i=1nÎµi*(sm)[Xiâˆ’E{Xi|XiTÎ²(sm)}]Ä¡(XiTÎ²(sm))/ÏƒÎµ*2(sm).

Theorem 1 (i) also implies that in general cases, w(sm, s) is given by ÏƒÎµâˆ’2(sm)âˆ’ÏƒÎµ(sm)âˆ’1âˆ‘mâ€²=1MÎ»mâ€²*,M1+Î»mâ€²*,MemTÏˆmâ€²*,MÏˆmâ€²*,MTÎ›Îµ*,Mâˆ’1/21M,

where em is an M Ã— 1 vector with the m-th element one and zero otherwise. For functional data, it is common to assume that Î»m*,M = 0 for all m &gt; K, where K is a positive integer. Under some additional conditions, it can be shown that Î»m*,M and Ïˆm*,M converge to the mâ€“th eigenvalue and its corresponding eigenfunction of the covariance function Î£Î·(s, sâ€²)/{ÏƒÎµ(s)ÏƒÎµ(sâ€²)}, respectively.

Another implication of Theorem 1 (i) is the lower bound of D(w*). Specifically, D(w*) is given by (24) (1MTÎ›Îµ*,Mâˆ’11Mâˆ’1MTÎ›Îµ*,Mâˆ’1/2âˆ‘m=1MÎ»m*,M1+Î»m*,MÏˆm*,MÏˆm*,MTÎ›Îµ*,Mâˆ’1/21M)âˆ’1,

which is greater than (1MTÎ›Îµ*,Mâˆ’11M)âˆ’1=1/{âˆ‘m=1MÏƒÎµâˆ’2(sm)}. In general, the presence of spatial correlation increases the covariance matrix of Î²Ì‚(s). Such lower bound is achievable only when Î£Î·(s, sâ€²) = 0 holds for any s â‰  sâ€². Furthermore, such lower bound is asymptotically achievable when Î»1*,M = op(1), since x/(1 + x) is a monotone function of x.

Theorem 1 (ii) implies that the use of w(sm, s) = Kh(sm âˆ’ s) can lead to substantial efficiency gain if there are substantial measurement errors. Specifically, if w(sm) = em, then we consider information at the mâ€“th grid point. In this case, we have D(e(sm))=Î£Î·(sm,sm)+ÏƒÎµ2(sm), which can be much larger than Î£Î·(sm, sm) when ÏƒÎµ2(m) is much larger than Î£Î·(sm, sm). The ratio of Î£1(e(sm)) over Î£1((Kh(smâˆ’s))) is equal to 1+ÏƒÎµ2(sm)/Î£Î·(sm,sm). Therefore, the efficiency gain of using w(sm, s) = Kh(sm âˆ’ s) can be substantial if the value of ÏƒÎµ2(sm)/Î£Î·(sm,sm) is large.

3.2 Asymptotic Properties

Second, we investigate the asymptotic properties of Î²Ì‚(Â·), Ä(XT Î²Ì‚(s)) and Î£Ì‚Î·(s, t) when we set w(sm, s) = Kh(sm âˆ’ s). For any smooth function f(s) and g(s, t), define á¸Ÿ(s) = df(s)/ds, fÌˆ(s) = d2 f(s)/ds2 and g(a,b)(s, t) = âˆ‚a+bg(s, t)/âˆ‚saâˆ‚tb, where a and b are any nonnegative integers. We state the following theorems regarding the weak convergence of {Î²Ì‚(s) : s âˆˆ ğ’®} and Ä(XT Î²Ì‚(s)), whose detailed assumptions and proofs can be found in Web Appendix. Moreover, we also include additional asymptotic properties and their proofs in the same Web Appendix.

Theorem 2

Under Assumptions (C1)â€“(C11), as n, M â†’ âˆ, we have the following results. n(Î²^(s)âˆ’Î²0(s)âˆ’0.5h2An(s)âˆ’1{1nâˆ‘i=1nBi(s)[gÂ¨(XiTÎ²(s))+2Ä (XiTÎ²(s))Ï€Ë™(s)/Ï€(s)]Î¼2(K)}) converges weakly to a Gaussian process with mean zero and covariance function, which is the limiting function of An(s)âˆ’1[nâˆ’1âˆ‘i=1nBi(s)Î£Î·(s,t)Bi(t)T]An(t)âˆ’1, where An(Â·) and Bi(Â·) are defined in Web Appendix.

n[Ä(XTÎ²^(s))âˆ’g(XTÎ²(s))âˆ’0.5h12Î¼2(K)gÂ¨(KTÎ²(s))] converges weakly to a Gaussian process with mean zero and covariance function Î£Î·(s, t).

Theorem 2 ensures that we can make formal statistical inference on Î²(Â·) and g(Â·). Based on Theorem 2 (i) and (ii), we develop a wild bootstrap method to construct the Confidence bands of Î²Ì‚(Â·) and Ä(Â·) and include it in Section B of Web Appendix.

4. Numerical Studies

4.1 Simulation Results

We generated Y (sm) according to model (3) with Îµ(sm) ~ N(0, Ïƒ2 = 0.32) and Xi ~ N(0p, Î£), where Î£ is a 4 Ã— 4 matrix with elements Ï|jâ€²âˆ’j| for j, jâ€² = 1, â€¦, 4. Moreover, the varying coefficients Î²j(s) are, respectively, given by Î²1(s)=1+s2,â€ƒâ€ƒÎ²2(s)=(1âˆ’s)2,

Î²3(s)=4s(1âˆ’s),â€ƒâ€ƒÎ²4(s)=âˆ’1+4(sâˆ’0.5)2,

and then they were scaled as Î²(s)/â€–Î²(s)â€–2. We set sm as M = 50 equidistant grid points in [0,1] with s1 = 0 and sM = 1. We set Î·i(s)=âˆ‘k=12Î¾ikÏˆk(s) with Î¾i1~N(0,Î»12=1),Î¾i2~N(0,Î»22=0.52),Ïˆ1(s)=2Â sin(2Ï€s), and Ïˆ2(s)=2Â cos(2Ï€s). We consider two index functions including g1(XiTÎ²(sm))=sin(2XiTÎ²(sm))+2Â cos(2+XiTÎ²(sm)) and g2(XiTÎ²(sm))=XiTÎ²(sm).

We conducted extensive simulation studies under different settings, but we only report some representative results for the sake of space. First, we set n = 40 and 200 and simulated data sets from model (3) for the first index function g1(XiTÎ²(sm)) as described above. We fitted SIVC to each simulated data set and calculated all unknown quantities. Table 1 summarizes the mean absolute error (MAE) and root mean square error (RMSE) of all parameter estimates and the mean integrated absolute error (MIAE) and mean integrated squared error (MISE) of all estimated functions based on 200 simulations. The results in Table 1 indicate satisfactory performance of our estimators since all MAE, MSE, MIAE and MISE values are quite small. Typical estimated functions with mean performance are displayed in Figure 2 and Figure 1 in Web Appendix. The estimated curves (broken lines) closely resemble the corresponding true functions (solid lines) in these figures. As expected, all the errors increase as sample size decreases. Moreover, Table 2 includes the coverage probabilities of the simultaneous Confidence bands of Î²l(s)s and g1(u) for n = 40 and 200 based on 500 simulations. These coverage probabilities get close to the specified Confidence level 95% as sample size increases, while the results for g(Â·) are slightly worse than those of Î²l(s).

Second, we illustrate the superiority of SIVC over multivariate varying coefficient model (MVCM) in Zhu et al. (2012) in terms of prediction accuracy. We set n = 200 and then simulated data sets from model (3) for both g1(XiTÎ²(sm)) and g2(XiTÎ²(sm)). For each simulated dataset, we randomly split it into a training set and a test set according to proportions Ï€ and 1 âˆ’ Ï€, respectively. We used the training set to estimate all unknown parameters, and then predicted the responses in the test set. Finally, we calculated the prediction error for each simulated dataset. We consider three values of Ï€ including 0.3, 0.5, and 0.7. For each case, we repeated 200 times. Table 3 reports the MAE and RMSE of prediction errors for SIVC and MVCM. For the index function g1(XiTÎ²(sm)), SIVC significantly outperforms MVCM with smaller MAE and RMSE. Even for g2(XiTÎ²(sm)), SIVC is slightly better than MVCM. It may indicate that SIVC is a useful tool for modeling functional data.

4.2 Real Data Analysis

We applied model (3) to the DWI data set described in Section 1. One goal of NIH ADNI is to test whether genetic, structural and functional neuroimaging, and clinical data can be integrated to measure the progression of mild cognitive impairment (MCI) and early Alzheimerâ€™s disease (AD). We downloaded the structural brain MRI data and corresponding clinical and genetic data from baseline and follow-up from the ADNI publicly available database (http://adni.loni.usc.edu/).

The DWI data were processed by using a FSL TBSS pipeline (Smith et al., 2006) to register DTIs from multiple subjects to create a mean image and a mean skeleton. We used FMRIB software library to compute maps of fractional anisotropy (FA) for all subjects from the DTI after eddy current correction and automatic brain extraction. Then, we fed FA maps into the TBSS tool, which is also part of FSL. In the TBSS analysis, we aligned the FA data of all the subjects into a common space by using non-linear registration and created and thinned the mean FA image to obtain a mean FA skeleton, which represents the centers of all white matter tracts common to the group. Subsequently, we projected each subjectâ€™s aligned FA data onto this skeleton. Finally, we obtained the FA template curve measured at all the 83 grid points along the skeleton of the midsagittal corpus callosum as shown in Figure 2 in Appendix for all the subjects.

We are interested in establishing an association between FA and seven covariates including the gender variable (123 male and 91 female, coded by a dummy variable indicating for male), the age of the subject (years, ranges from 48.4 (years) to 90.4, mean 73.20), an indicator for handiness (193 right-hand and 20 left-hand, coded by a dummy variable indicating for left-hand), the education level (years, ranges from 9 to 20 (years), mean 15.91), an indicator for Alzheimerâ€™s disease (AD) status (19.6%), an indicator for mild cognitive impairment (MCI) status (55.1%) and Mini-Mental State Exam (MMSE) of ADNI. We standardized all variables to have mean zero and variance one. We fitted model (3) and applied the estimation procedure in Section 2 to the data set. Figure 3 presents the estimated varying coefficients corresponding to age, education, AD status, and MCI status. The results reveal that MMSE, age, education, MCI, and AD are the most important factors. Moreover, gender and handiness have little effects on FA.

We estimated the single index function g(Â·) and the covariance function Î£Î·(s, sâ€²) and its associated eigenvalues and eigenfunctions. See Figure 1 for details. The single index function shows a pattern with small values at the left and right ending points of the range XiTÎ²(s) and two modes in the middle of the range. The top five non-zero eigenvalues of Î£Ì‚Î·(s, sâ€²) are 0.8628, 0.2771, 0.0284, 0.0142, and 0.0102, respectively. The first two eigenvalues account for 94.8% of the total variability, while the remaining eigenvalues rapidly drop to zero. The first eigenfunction, with a dominant eigenvalue accounting for 71.8% of the total variation, is simple in structure and resembles a single cycle of a sine wave. The remaining eigenfunctions are also quite simple and roughly sinusoidal representing additional functional structure that cannot be captured by the mean structure of model (3).

Finally, we compared the prediction accuracy of SIVC with that of MVCM in Zhu et al. (2012). We randomly split the 213 subjects into a training set and a test set with corresponding proportions Ï€ and 1 âˆ’ Ï€, respectively. We used the training set to estimate all the parameters, and then predicted the responses of the testing set. The 100 replications were used to calculate the prediction errors corresponding Ï€ = 0.3, 0.5 and 0.7. Table 4 reports the MAE and RMSE of the prediction errors and indicates that SIVC significantly outperforms MVCM in terms of both MAE and RMSE.

5. Conclusion

In this paper we have developed a single-index varying coefficient model for establishing a varying association between functional responses (e.g., image) and a set of covariates. We have developed an estimation procedure to estimate varying coefficient functions, link function, and the covariance function of individual functions. We have investigated a strategy to integrate the information across all grid points. We have used simulations and real data analysis to demonstrate that SIVC is a useful tool for modeling functional data.

Many important issues need to be addressed in future research. First, the computational burden associated with SIVC can be quite heavy making it infeasible for large-scale imaging data at this moment. We will develop more computationally efficient algorithms to address such challenge. Second, we need to develop an effective procedure to carry out statistical inference, such as hypothesis test. Third, it is scientifically interesting to extend SIVC to carry out regression analysis of longitudinal functional data.

Supplementary Material

su

The research of Dr. Zhu was supported by NIH grants 1UL1TR001111, MH086633, and EB005149-01 and NSF grants SES-1357666 and DMS-1407655. Data used in preparation of this article were obtained from the Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI) database (http://adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf.

Figure 1 ADNI data analysis: raw FA curves measured at 83 grid points (upper-left panel), the estimated index function with the broken red lines representing 95% simultaneous Confidence bands (upper-right panel), the estimated accumulative proportion of estimated eigenvalues (lower-left panel) and estimated eigenfunctions (lower-right panel) corresponding to the five largest eigenvalues.

Figure 2 Simulation results for model (3) with the first index function and n = 200: the true and estimated varying coefficient functions and the true and estimated index functions. In each panel, the solid line represents the true function, the broken line represents the estimated function, and the red broken lines are the corresponding 95% simultaneous Confidence bands.

Figure 3 ADNI data analysis: the four estimated varying coefficients for Age, Education, AD, and MCI. The black solid lines are estimated coefficients and the red broken lines are their corresponding 95% simultaneous Confidence bands.

Table 1 Estimation results from 200 simulated data sets corresponding to the index function g1(XT Î²(s)) with n = 200 and 40. MAE is mean absolute error, RMSE is root mean square error, MIAE is mean integrated absolute error, and MISE is mean integrated square error

	n = 200			n = 40		
Parameters	Î»1	Î»2	Ïƒ2		Î»1	Î»2	Ïƒ2	
MAE	0.1551	0.0621	0.0202		0.1773	0.1026	0.0594	
RMSE	0.1724	0.0726	0.0267		0.2108	0.1212	0.0667	
	
n = 200	
Functions	Î²1(s)	Î²2(s)	Î²3(s)	Î²4(s)	Ïˆ1(s)	Ïˆ2(s)	g(Â·)	
MIAE	0.0288	0.0440	0.0403	0.0119	0.0137	0.0138	0.1063	
MISE	0.0042	0.0077	0.0093	0.0039	0.0033	0.0034	0.0412	
	
n = 40	
Functions	Î²1(s)	Î²2(s)	Î²3(s)	Î²4(s)	Ïˆ1(s)	Ïˆ2(s)	g(Â·)	
MIAE	0.0488	0.0751	0.0686	0.0597	0.0291	0.0299	0.2536	
MISE	0.0121	0.0249	0.0251	0.0241	0.0061	0.0060	0.1530	

Table 2 Coverage probabilities of simultaneous confidence bands for n = 40 and 200 based on 500 simulated data sets. The confidence level is 95%.

n	Î²1(s)	Î²2(s)	Î²3(s)	Î²4(s)	g(Â·)	
40	0.936	0.932	0.944	0.934	0.924	
200	0.946	0.954	0.948	0.946	0.942	

Table 3 Prediction results corresponding to both index functions g1(XT Î²(s)) and g2(XT Î²(s)) with n = 200. MAE is mean absolute error and RMSE is root mean square error.

	g1(XT Î²(s))	g2(XT Î²(s))	
	MAE	RMSE	MAE	RMSE	
	Ï€ = 0.3	
SIVC	1.013 (0.040)	1.282 (0.052)	0.987 (0.042)	1.248 (0.053)	
MVCM	1.215 (0.040)	1.530 (0.053)	1.031 (0.045)	1.299 (0.060)	
	
	Ï€ = 0.5	
SIVC	1.002 (0.049)	1.267 (0.063)	0.978 (0.048)	1.236 (0.060)	
MVCM	1.207 (0.048)	1.520 (0.065)	1.014 (0.050)	1.278 (0.062)	
	
	Ï€ = 0.7	
SIVC	0.977 (0.068)	1.236 (0.081)	0.966 (0.065)	1.222 (0.082)	
MVCM	1.188 (0.063)	1.494 (0.082)	1.006 (0.068)	1.268 (0.084)	

Table 4 Prediction results for the ADNI data analysis. EST is estimate, SE is standard error, MAE is mean absolute error, and RMSE is root mean square error.

		MAE	RMSE	MAE	RMSE	MAE	RMSE	
		Ï€ = 0.3	Ï€ = 0.5	Ï€ = 0.7	
SIVC	EST	0.114	0.1374	0.113	0.136	0.112	0.135	
	SE	0.001	0.001	0.002	0.002	0.002	0.002	
MVCM	EST	0.599	0.650	0.599	0.627	0.600	0.628	
	SE	0.012	0.028	0.014	0.022	0.013	0.016	

Supplementary Materials

All Web Appendices, Tables, and Figures referenced in Sections 3.2 and 4.2 are available with this paper at the Biometrics website on Wiley Online Library. In addition, we will develop a companion software for SIVC and release it to the public through http://www.nitrc.org/ and https://www.nitrc.org/projects/fadtts/.


References

Bowman FD Caffo B Bassett SS Kilts C A bayesian hierarchical framework for spatial modeling of fmri data NeuroImage 2008 39 146 156 17936016
Cook RD Forzani L Likelihood-based sufficient dimension reduction Journal of the American Statistical Association 2009 104 197 208
Cook RD Weisberg S Discussion of â€œsliced inverse regression for dimension reductionâ€ Journal of the American Statistical Association 1991 86 328 332
Gossl C Auer DP Fahrmeir L Bayesian spatiotemporal inference in functional magnetic resonance imaging Biometrics 2001 57 554 562 11414583
Horowitz JL Semiparametric and Nonparametric Methods in Econometrics 2009 Springer
Jiang CR Wang JL Functional single index models for longitudinal data Annals of Statistics 2011 39 362 388
Lazar N The Statistical Analysis of Functional MRI Data 2008 New York Springer
Li K-C Sliced inverse regression for dimension reduction Journal of the American Statistical Association 1991 86 316 327
Li Y Zhu H Shen D Lin W Gilmore JH Ibrahim JG Multiscale adaptive regression models for neuroimaging data Journal of the Royal Statistical Society: Series B 2011 73 559 578
Ma Y Zhu L A semiparametric approach to dimension reduction Journal of the American Statistical Association 2012 107 168 179 23828688
Ma Y Zhu L Efficient estimation in sufficient dimension reduction Annals of statistics 2013 41 250 268 24058219
Ma Y Zhu L On estimation efficiency of the central mean subspace J. R. Stat. Soc. Ser. B Stat. Methodol 2014 76 885 901
Miranda MF Zhu H Ibrahim JG Bayesian spatial transformation models with applications in neuroimaging data Biometrics 2013 69 1074 1083 24128143
Morris JS Functional regression Annual Review of Statistics and Its Application 2015 2 321 359
Penny WD Trujillo-Barreto NJ Friston KJ Bayesian fmri time series analysis with spatial priors NeuroImage 2005 24 350 362 15627578
Polzehl J Spokoiny VG Propagation-separation approach for local likelihood estimation Probab. Theory Relat. Fields 2006 135 335 362
Ramsay JO Silverman BW Functional Data Analysis 2005 second New York Springer
Reiss PT Huang L Mennes M Fast function-on-scalar regression with penalized basis expansions The International Journal of Biostatistics 2010 6
Rice JA Silverman BW Estimating the mean and covariance structure nonparametrically when the data are curves Journal of the Royal Statistical Society. Series B 1991 53 233 243
Smith M Fahrmeir L Spatial bayesian variable selection with application to functional magnetic resonance imaging Journal of the American Statistical Association 2007 102 417 431
Smith S Jenkinson M Johansen-Berg H Rueckert D Nichols T Mackay C Watkins K Ciccarelli O Cader M Matthews P Behrens T Tract-based spatial statistics: voxelwise analysis of multi-subject diffusion data NeuroImage 2006 31 1487 1505 16624579
Staicu A Crainiceanu C Carroll R Fast analysis of spatially correlated multilevel functional data Biostatistics 2010 11 177 194 20089508
Wang J-L Chiou J-M Mueller H-G Review of functional data analysis 2015
Wang X van Eeden C Zidek JV Asymptotic properties of maximum weighted likelihood estimators Journal of Statistical Planning and Inference 2004 119 37 54
Worsley KJ Taylor JE Tomaiuolo F Lerch J Unified univariate and multivariate random field theory NeuroImage 2004 23 189 195
Xia Y A constructive approach to the estimation of dimension reduction directions The Annals of Statistics 2007 35 2654 2690
Xia Y Tong H Li W Zhu L-X An adaptive estimation of dimension reduction space Journal of the Royal Statistical Society: Series B 2002 64 363 410
Yue Y Loh JM Lindquist MA Adaptive spatial smoothing of fMRI images Statistics and its Interface 2010 3 3 14
Zhang J Chen J Statistical inferences for functional data The Annals of Statistics 2007 35 1052 1079
Zhu H Fan J Kong L Spatially varying coefficient model for neuroimaging data with jump discontinuities Journal of the American Statistical Association 2014 109 1084 1098 25435598
Zhu H Li R Kong L Multivariate varying coefficient model for functional responses Annals of Statistics 2012 40 2634 2666 23645942
Zhu L Wang T Zhu L FerrÃ© L Sufficient dimension reduction through discretization-expectation estimation Biometrika 2010 97 295 304
