LICENSE: This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.


100960938
32989
Comput Stat Data Anal
Comput Stat Data Anal
Computational statistics &amp; data analysis
0167-9473
1872-7352

30613119
6317381
10.1016/j.csda.2018.07.012
NIHMS1501966
Article
Permutation Tests for General Dependent Truncation
Chiou Sy Han a1*
Qian Jing b
Mormino Elizabeth c
Betensky Rebecca A. a
Alzheimer’s Disease Neuroimaging Initiative2
Australian Imaging Biomarkers Lifestyle Flagship Study of Aging3
Harvard Aging Brain Study
a Department of Biostatistics, Harvard T.H. Chan School of Public Health, Boston, MA 02115, U.S.A.
b Department of Biostatistics and Epidemiology, University of Massachusetts Amherst, Amherst, MA 01003, U.S.A.
c Department of Neurology, School of Medicine, Stanford University, Stanford, CA 94305, U.S.A.
* Corresponding author, schiou@utdallas.edu, tel: 972.883.6362, fax: 972.883.6622.
31 7 2018
29 7 2018
12 2018
01 12 2019
128 308324
This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.

Truncated survival data arise when the event time is observed only if it falls within a subject-specific region, known as the truncation set. Left-truncated data arise when there is delayed entry into a study, such that subjects are included only if their event time exceeds some other time. Quasi-independence of truncation and failure refers to factorization of their joint density in the observable region. Under quasi-independence, standard methods for survival data such as the Kaplan–Meier estimator and Cox regression can be applied after simple adjustments to the risk sets. Unlike the requisite assumption of independent censoring, quasi-independence can be tested, e.g., using a conditional Kendall’s tau test. Current methods for testing for quasi-independence are powerful for monotone alternatives. Nonetheless, it is essential to detect any kind of deviation from quasi-independence so as not to report a biased Kaplan–Meier estimator or regression effect, which would arise from applying the simple risk set adjustment when dependence holds. Nonparametric, minimum p-value tests that are powerful against non-monotone alternatives are developed to offer protection against erroneous assumptions of quasi-independence. The use of conditional and unconditional methods of permutation for evaluation of the proposed tests are investigated in simulation studies. The proposed tests are applied to a study on the cognitive and functional decline in aging.

Kendall’s tau
minimally selected test
monotone dependence
quasi-independence

1. Introduction

Truncated survival data arise when the event time is observed only if it falls within a subject-specific region, known as the truncation set. They are frequently encountered in observational studies conducted in many fields, including biomedical science, astronomy, and social science. One such example is an integrated observational aging study in which a large number of cognitively normal older individuals were recruited across three independent observational cohort studies (Mormino et al., 2014). These individuals had a global Clinical Dementia Rating (CDR) of 0 at the baseline testing session. In one sub-study, researchers were interested in the association between the accumulation of abnormally folded beta-amyloid protein in the brain and the risk of cognitive decline. Participants were only included in the amyloid sub-study if they had beta-amyloid measured at or after the baseline testing session by a Positron Emission Tomography (PET) scan and remained at CDR of 0 at that time. Cognitive decline was measured by progression to a global CDR of 0.5, which is left-truncated by the time from baseline to PET scan time. Additionally, there was right censoring for subjects who did not progress by the end of the follow-up.

Most existing methods for the analysis of truncated data assume quasi-independence between the truncation and event times (e.g., Klein and Moeschberger, 2003; Lagakos et al., 1988; Tsai et al., 1987; Turnbull, 1976; Wang, 1991; Woodroofe, 1985). Quasi-independence refers to the factorization of the joint density of truncation and event within the observable region. This condition may not hold in many realistic settings. For example, PET scans for amyloid may be prioritized according to some assessment of the cognitive trajectory of the subject, which could lead to many possible monotone and non-monotone dependencies between time to PET scan and time to cognitive decline.

Quasi-independence of truncation and event times, unlike independence of censoring and event times, is a condition that can be tested with observed data. Tsai (1990) proposed a test of quasi-independence in the setting of left-truncation and right censoring based on a conditional Kendall’s tau. Martin and Betensky (2005) extended the method to the settings of double-truncation and interval censoring. Austin and Betensky (2014) proposed inverse probability weighted versions of the conditional Kendall’s tau estimators to remove the effects of censoring. Emura and Wang (2010) constructed a weighted log-rank type statistic, with optimal weights constructed from the odds ratio function considered in Chaieb et al. (2006). Chen et al. (1996) suggested a conditional version of Pearson’s product-moment correlation coefficient, but this does not accommodate right censoring. Jones and Crowley (1992) proposed a class of nonparametric tests under a proportional hazards assumption. These tests are powerful for monotone alternatives, but are not generally powerful for detecting non-monotone alternatives that are commonly encountered in real applications.

Some recent proposals have attempted to accommodate non-monotone alternatives through local versions of Kendall’s tau tests. de Uña-Álvarez (2012), and Rodríguez-Girondo and de Uña-Álvarez (2012) proposed a test for the Markov assumption embedded in the illness-death model based on the maximum of a sequence of Kendall’s tau statistics at predetermined time grids. These tests are inefficient under heavy censoring or with small sample sizes. Rodríguez-Girondo and de Uña-Álvarez (2016) proposed a weighted version of the local Kendall’s tau as an attempt to handle heavy censoring. All of these methods are quite computationally intensive as they rely on a bootstrap method to approximate the distribution of the local test statistics and an additional bootstrap for evaluation of the critical value of the maximum local test statistics. Furthermore, the performance of the tests depends on the pre-selected time grids. Nonetheless, this notion of optimization of a sequence of local tests seems promising for achieving high power for general dependence alternatives and is similar in flavor to the ideas suggested by Heller et al. (2012, 2016); Kaufman et al. (2013).

Evaluation of tests of quasi-independence is challenging given the restriction associated with the observed data. Although analytical evaluation is possible for simple tests in the setting of one-sided truncation, it is not possible for more complex sampling restrictions or for maximized sequences of test statistics, such as nonparametric tests that have high power for non-monotone alternatives. Previous authors developed algorithms for permutation that yield pairs that all satisfy the truncation restriction (Efron and Petrosian, 1992, 1994; Tsai, 1990). This permutation method is valid, in that each permutation is exchangeable, though it is limited in that it holds fixed the size of each risk set, and thus is not useful for evaluation of any statistic that is largely a function of risk set size, such as the Kaplan–Meier estimator. Because of this property, we refer to this method of permutation as the conditional permutation. An alternative method of permutation would simply permute all of the truncation times, and retain only those pairs that satisfy the truncation restriction. Strictly speaking, this is not a valid procedure in that the permutations have variable sample size and are not exchangeable, though in large enough samples and for standardized test statistics, this may not be problematic. We refer to this method of permutation as the unconditional permutation.

In this paper, we propose two minimum p-value tests of quasi-independence that have high power for nonmonotone alternatives and are less computationally intensive than the double bootstrap approach used by Rodríguez-Girondo and de Uña-Álvarez (2012); de Uña-Álvarez (2012) and Rodríguez-Girondo and de Uña- Álvarez (2016). For evaluation of these tests, we investigate and compare the use of conditional and unconditional permutation. We conduct extensive simulation studies to examine the properties of these tests and apply the tests to assess quasi-independence in the cognitive and functional decline in an aging study. We developed a publicly available R package, permDep (Chiou, 2017a), which is able to implement the proposed methods efficiently by embedding C/C++ subroutines to speed up computationally intensive components and by executing permutations in a parallel fashion.

2. Test Statistics

In the setting of left truncated and right censored data, we let X denote the time to event (e.g., time from baseline testing to CDR progression), C denote time to censoring (e.g., time from baseline testing to end of follow-up) and T denote the time to study entry (e.g., time from baseline testing to PET scan). Let Y = min(X,C) and Δ denote the observation time and censoring indicator, respectively, where Δ = I(X ≤ C), and I(·) is the indicator function. Due to truncation, Y is observable only if T ≤ Y. We further assume that C is independent of (X,T), but allow X and T to be dependent. The observed data then comprise n independent and identically replicates of {Y,T,Δ | Y ≥ T} = {(Yi,Ti,Δi | Yi ≥ Ti),i = 1,…,n}.

Quasi-independence of X and T (Tsai, 1990) is necessary for straightforward application of standard methods in survival analysis to truncated data. Let FX and FT denote the right continuous marginal distribution functions for X and T, respectively. The null hypothesis of quasi-independence as formulated by Tsai (1990) is: (1) H0:H(x,t)=∫∫u&gt;v,u≤x,v≤tdFX(u)dFT(v)∫∫x&gt;tdFX(x)dFT(t),

where H(X,T) = Pr(X ≤ X,T ≤ t | X ≥ T). Under H0, the joint density of X and T in the observable region (T ≤ X) is proportional to the product of dFX and dFT. The null hypothesis, (1), is weaker than global independence because it does not require independence of X and T in the unobservable region.

2.1. Conditional Kendall’s Tau Test

We present the conditional Kendall’s tau test of quasi-independence because of its simplicity and ubiquity in applications. Given random vectors (Xi,Ti) and (Xj,Tj), Tsai (1990) and Martin and Betensky (2005) proposed tests based on a conditional Kendall’s tau defined as τc = E[sgn{(Xi − Xj)(Ti − Tj)} | Ωij], where sgn(a) = I(a &gt; 0) − I(a &lt; 0) and Ωij = {max(Ti,Tj) ≤ min(Xi,Xj)}. The Kendall’s tau, τc, is well defined because Ωij implies that the pair (i,j) is observable and comparable. Furthermore, the null hypothesis of quasi-independence implies that τc = 0 (Tsai, 1990). In the presence of right censoring, τc is modified to be τc*=E[sgn{(Yi−Yj)(Ti−Tj)}|Λij], where Λij={max(Ti,Tj)≤min(Yi,Yj)}∩{Δij(1)=1} and Δij(1) is the censoring indicator for min(Yi,Yj) defined as Δij(1)=ΔiI(Yi&lt;Yj)+ΔjI(Yi&gt;Yj)+max(Δi,Δj)I(Yi=Yj). In the case when Yi = Yj but Δi ≠ Δ j, we follow the usual convention and set Δij(1)=1. In addition to requiring observability and comparability, Λij additionally requires orderability of (i,j). Under the null hypothesis of quasi-independence, Martin and Betensky (2005) showed that τc*=τc=0 and proposed a consistent estimator of τc* (2) τ^c*=1M∑i=1n−1∑j=1nsgn{(Yi−Yj)(Ti−Tj)}I(Λij),

where M is the number of comparable and orderable pairs defined as M=∑i=1n−1∑j=1nI(Λij). Martin and Betensky (2005) recognized τ^c* as a U-statistic, and used this framework to establish asymptotic properties and derive a general variance formula.

2.2. Minimally Selected p-value Tests

Traditional tests of quasi-independence between truncation time and failure time, such as the conditional Kendall’s tau test, are based on measures of correlation or their rank-based counterparts. Thus, these tests are powerful for detection of monotone relationships, but may have minimal power for detection of non-monotone relationships. It is possible to incorporate data-dependent weights in existing methods, e.g., Equation (2), to improve power, as in the weighted log-rank test proposed by Emura and Wang (2010). The performance of the weighted version of the existing methods depends on the selection of optimal weights, which are unknown in practice. Because it is essential to detect violations of quasi-independence, and because non-monotone dependence likely occurs in many studies due to various plausible sampling strategies, we develop tests to address these situations. To do so, we extend ideas from two areas of investigation in the literature. The first is a collection of papers on optimally selected statistics for testing for association among continuous variables (e.g., Betensky, 2001; Betensky and Rabinowitz, 1999, 2000; Halpern, 1999; Hothorn and Lausen, 2003; Koziol, 1991; Lausen and Schumacher, 1992; Miller and Siegmund, 1982; Rabinowitz and Betensky, 2000). These papers calculated a minimum p-value from a collection of test statistics derived from a sequence of thresholds applied to one of the continuous variables and then derived asymptotic approximations for the true p-value that accounted for the minimization. The second set of related papers considered similar exploration of calculation of test statistics over multiple partitions of data, with the goal of attaining high power for general dependence alternatives (Heller et al., 2012). Following the spirit of this previous work, we propose two minimum p-value tests that aim to detect non-monotone dependencies for left-truncated and right-censored data.

Our first proposed test, minp1, considers a sequence of cutpoints, t ⊂ {T1,…,Tn}, that we use to partition the observations into two groups according to whether T ≤ t or T &gt; t. The underlying principle of our test is that under the null hypothesis of quasi-independence, for every t, the population distribution of X | T ≤ t should be no different from that of X | T &gt; t in the overlap of their supports. This is exactly what is tested by the log-rank test that adjusts for quasi-independent left truncation and independent right censoring. For a given cut-point t, the log-rank statistic under the null of independent left truncation and in the presence of right censoring for groups defined by T ≤ t or T &gt; t follows a χ2 distribution, with one degree of freedom (equivalently, the standardized log-rank statistic followed a standard normal distribution) (Pan, 1998; Shen, 2015; Therneau and Grambsch, 2013). We implemented this using the R package survival (Therneau, 2015). The overall null hypothesis of quasi-independence is then tested by selecting the minimum log-rank p-value, among all candidate cut-points, t. The distribution of this minimally selected statistic is quite complicated and so we evaluate it relative to its permutation distribution. To obtain meaningful test statistics, we consider only those thresholds t that yield at least E events in each group. We typically take E = 10 to achieve reasonable power. In summary, the minp1 test consists of the following steps:

Initialize m = 0.

Set m = m + 1 and partition the data into two groups: {j : Tj ≤ Tm} and {j : Tj &gt; Tm}.

If E≤∑i=1nΔiI(Ti&lt;Tm)≤n−E, calculate the log-rank statistic, χm2, for comparing {X|T ≤ Tm} and {X|T &gt; Tm}. If this condition does not hold, set χm2=0.

If m &lt; n, return to Step 2.

Set minp1 = minm P(Q &gt; χm2), where Q is a χ2 distributed random variable with one degree of 120 freedom.

Although the minp1 test captures some non-monotone dependencies, it will not detect dependencies that are highly non-monotone such that distribution of X | T ≤ t is similar to that of X | T &gt; t, even though there are strong relationships among X and T within the two partitions. As an alternative, we propose a second test, minp2, that for a given (ϵ, t) divides the data into two groups according to whether or not T ∈ (t−ϵ t+ϵ) We allow the partitions to degenerate into one-sided intervals, which are subsets of partitions considered in minp1 procedure. The half-width of the interior interval, ϵ, plays a crucial role in determining the number of partitions that degenerate into one-sided intervals; the larger the ϵ, the more partitions degenerate into one-sided intervals. In the simulation and data analysis, we chose an ϵ that generates a mixture of both types of partitions. The general idea is to first compute the smallest possible subject-specified ϵ for each given cutpoint such that there are at least E events inside and outside of the interval. This will result in the largest number of non-degenerate partitions. The maximum of these subject-specified ϵ’s provides a mixture of non-degenerate and degenerate partitions, with most non-degenerate partitions occurring at the middle valued cutpoints. The aforementioned procedure provides one of the many possible ways to construct the desired partitions. Once ϵ is determined, we calculate the log-rank p-value for each t, and then minimize.them over all t. In summary, the minp2 test consists of the following steps:

Initialize m = 0.

Set m = m + 1 and partition the data into two groups: {j : Tj ∈ (Tm − ϵm, Tm + ϵm)} and {j : Tj ∉ (Tm − ϵm, Tm + ϵm)}, where ϵm is the minimum ϵ &gt; 0 such that there are at least E events in {j : Tj ∈ (Tm − ϵm, Tm + ϵm)} and at least E events in {j : Tj ∉ (Tm − ϵm, Tm + ϵm)}.

If m &lt; n, return to Step 2.

Set ϵ = maxm ϵm and re-initialize m = 0.

Set m = m + 1. If E≤∑i=1nΔiI(Tm−ϵ&lt;Ti&lt;Tm+ϵ)≤n−E, calculate the log-rank statistic, χm2, for comparing {X|T ∈ (Tm − ϵ, Tm + ϵ)} and {X|T ∉ (Tm − ϵ, Tm + ϵ)}. If this condition does not hold, set χm2=0.

If m &lt; n, return to Step 6.

Set minp2 = minm P(Q&gt;χm2).

3. Permutation for evaluation of minimum p-value tests

Much of the literature on optimally selected statistics is focused on theoretical derivations of a correct p-value that accounts for the optimization. In many cases, this supremum statistic was shown to follow a Brownian Bridge process after transformation to a time scale indexed by the thresholding variable (e.g., Betensky and Rabinowitz, 1999; Miller and Siegmund, 1982; Rabinowitz and Betensky, 2000). This finding then enabled calculation of the corrected p-value through established approximations to tail probabilities of the Brownian Bridge. Unfortunately, in the setting of truncation, this convenient result does not hold due to the dual role of the truncation variable. Not only does the thresholding variable divide the data into two groups, but it also serves as the variable that defines the risk sets that underlie construction of the log-rank statistic.

Given the absence of an asymptotic reference distribution for the minp1 and minp2 tests, we employ permutation methods. A sufficient condition for a permutation test to be exact and unbiased is the exchangeability of each permutation (Good, 2013). This is achieved by the conditional permutation method that we consider, as it is designed to yield permutations that satisfy the truncation constraint for each observation (i.e., T ≤ X). The unconditional permutation method that we consider has the advantages of speed and a larger permutation space, but does not satisfy exchangeability exactly. However, under the null hypothesis, it does satisfy exchangeability approximately. For simplicity, we first describe the methods in the absence of censoring, and then discuss the extension to censored data.

3.1. Conditional Permutation

In the absence of censoring, the observed data can be expressed as n independent and identically copies of {X,T | X ≥ T} = {(Xi,Ti | Xi ≥ Ti),i = 1,…,n}. Let j = {j(1),…,j(n)} be a permutation of (1,…,n). If Xi ≥ Tj(i) for all i = 1,…,n, j is a valid permutation. Construction of a valid permutation can be achieved by sampling T’s from risk sets for each Xi. The following algorithm (Efron and Petrosian, 1992, 1994; Tsai, 1990) summarizes the steps to obtain a valid permutation:

Order the Xi’s such that X(1) &lt; X(2) &lt;·· &lt; X(n).

Initialize i = 0.

Set i = i + 1 and for X(i), randomly select a Tj(i) such that j(i) ∈ {k : Tk ≤ X(i)}.

Remove j(i) from {1,…,n} and return to Step 3 if i &lt; n.

This algorithm involves many comparisons and the overall computational time is substantial. Chen and Liu (2007) proposed an importance sampling strategy to generate permutations under truncation constraints efficiently. The main idea of this strategy is to first construct an n by n constraint matrix whose (i,j)th cells takes a value of 1 if the truncation constraint is satisfied and 0 otherwise. Each j(i)’s is then chosen from a multinomial distribution with event probabilities constructed from the constraint matrix. This achieves considerable efficiency in the context of double truncation, but reduces to the conditional permutation algorithm given above in the setting of one-sided truncation.

In addition to the computation time, another potential drawback of this permutation method is that it holds the risk set sizes fixed. The risk set size for Xi is given by #{(Xj,Tj) : Xj ≥ Xi ≥ Tj}. It is straightforward to see that the size of the risk set for Xi remains fixed across all valid conditional permutations. For the original data, Ni=#{(Xj,Tj):Xj≥Xi≥Tj}=#{j:Xj≥Xi}+#{j:Xi≥Tj}−#{j:(Xj≥Xi)∪(Xi≥Tj)}=#{j:Xj≥Xi}+#{j:Xi≥Tj}−n+#{j:Xj&lt;Xi&lt;Tj}=#{j:Xj≥Xi}+#{j:Xi≥Tj}−n.

Similarly, for permuted data, {(Xi,Tj(i)|Xi≥Tj(i)),i=1,…,n}, the number of subjects at risk at time Xi is #{(Xj,Tj(j)):Xj≥Xi≥Tj(j)}=#{j:Xj≥Xi}+#{j:Xi≥Tj(j)}−n=#{j:Xj≥Xi}+#{j:Xi≥Tj}−n=#{(Xj,Tj):Xj≥Xi≥Tj}.

Thus, by construction, conditional permutation results in observations that all satisfy the truncation constraint, and it holds the size of each risk set fixed. The latter property precludes its use for evaluation of an estimator that is based solely on risk set sizes, such as the Kaplan–Meier estimator, and likely for other estimators or tests that are partly based on risk set sizes.

In the presence of right censoring, we replace Xi with Yi in the permutation algorithm. Since we assume that Ci is independent of Ti, the permutation applied to (Yi,Ti) is appropriate under the null. We then use the permuted data (Yi,Δi,Tj(i)) to compute the permutation log-rank test statistics. In this case, the risk set sizes are still fixed, and risk-set-based estimators remain infeasible under this permutation method.

3.2. Unconditional Permutation

Due to the computational demands of conditional permutation, along with its inapplicability for test statistics that are risk-set based, we consider an alternative permutation approach. Specifically, we consider all n! possible permutations of (T1,…,Tn), which we pair with the original vector (X1 …,Xn), and delete the resulting inadmissible observations that Tj(i) ≥ Xi. The permuted data consist of pairs (Xi, Tj(i)) that satisfy Tj(i) ≤ Xi. This procedure is considerably faster than the conditional permutation procedure as it requires checking the truncation criteria only once, after permutation. This approach does not hold the original risk set sizes fixed. Strictly speaking, this is not a proper permutation method, as it does not yield exchangeable permutations due to the varying sample sizes. Heuristically, we expect that under the null hypothesis, the normalized log-rank statistic (and p-value), are invariant to sample size for samples that are large enough to be well approximated by asymptotics. Likewise, under the null, the minimized log-rank p-value across the restricted range of truncation times, which ensures a minimum of events in each segment, should be approximately invariant to sample size. Thus, we expect the unconditional permutation to provide a valid approximation to the theoretically correct conditional permutation (Chen and Liu, 2007; Efron and Petrosian, 1992, 1994; Tsai, 1990). This is supported empirically in our simulation results under the null in Table 1 (τ = 0) and Figures 1a and 1b. As for conditional permutation, we replace Xi with Yi in the presence of independent right censoring.

3.3. Empirical Approximation of p-value

The permutation two-sided p-value is defined as p=∑i=1NI(|zi|≥|zobs|)/N, where z1,z2,…,zN are all possible permutation test statistics. For small data sets, the complete permutation distribution of the statistic can be enumerated and an exact permutation test is obtained. However, in practical situations it is not feasible to perform all possible permutations. For example, the cognitive and functional decline study consists of 490 samples, and admits over 1.8×10308 permutations of size 490. In such cases, the permutation p-value is approximated through sampling N*of the N permutations. In practice, the value of N* can be calculated through a Bernoulli approximation under the null to obtain a selected margin of error, M, with 95% confidence as (1.96p(1−p)/M)2, which is maximized at p = 0.5.

4. Simulation Studies

We conducted two simulation studies under monotone dependence, and two under non-monotone dependence. Our objectives were to investigate the performance of the minimum p-value tests in both scenarios and to compare the conditional and unconditional methods of permutation. We implemented these tests in a publicly available R package permDep (Chiou, 2017a).

4.1. Small Scale Simulation: Monotone Dependence

We first simulated data under a monotone dependence model. To mimic the distributions of lifetime data that are commonly encountered in practice, we generated X from a Weibull distribution with shape parameter 3 and scale parameter 8.5 and T from an exponential distribution with mean 5. We specified the dependence of (X,T) through a normal copula model, which can be easily generated from a bivariate normal distribution; the dependence is determined by the correlation parameter ρ, or equivalently by Kendall’s tau, since ρ = sin(τ × π/2). The normal copula is appealing for its familiarity and its previous use for modeling survival data e.g., Huang and Berry (2006); Li et al. (2008) and Othus and Li (2010). We considered two levels of dependence for T and X, as measured by the pre-truncation unconditional Kendall’s tau of τ = 0 and τ = 0.4. Since τ is related to correlation coefficient in standard bivariate normal distribution, i.e., the ρ = sin(τ × π/2), it is easily interpreted and understood. We first examined two small simulated datasets (n = 10), one with no dependence (τ = 0) and one with moderate dependence (τ = 0.4), with no censoring, in which the complete permutation space can be generated and the exact permutation p-value can be calculated accordingly. The simulated datasets are presented in Supplementary Figures 1(a) and 1(b). We calculated conditional and unconditional permutation p-values for our proposed tests, minp1 and minp2, using E = 2 and optimized ϵ = 0.36 and 1.42 for τ = 0 and τ = 0.4, respectively.

There are 2,903,040 and 153,600 admissible permutations of size 10 under the conditional permutation procedure for the simulated dataset with τ = 0 and 0.4, respectively. There are 10! = 3,628,800 unconditional permutations, which are of varying sample sizes. The asymptotic conditional Kendall’s tau p-value and the exact permutation p-values are listed in Table 1. For both datasets, the conditional and unconditional permutation p-values are close, as are the entire permutation distributions (shown in Figure 1 for minp1). This is interesting given that the unconditional space is an order of magnitude larger than the conditional space for τ = 0.4. In this example of monotone dependence, minp2 is more conservative than minp1, likely due to the suboptimal comparison of small and large X’s to moderate X’s conducted by minp2. 245 Figure 2 (for τ = 0.4) and Supplementary Figure 2 (for τ = 0.0) indicate the exact permutation p-values with the horizontal line, and display 1000 traces of permutation p-values in gray lines. Each trace mimics a practical use of the permutation test and is computed with a unique random seed. We also include a band reflecting a 0.01 margin of error calculated from the normal approximation to the binomial and using the exact p-value. For example, the exact minp1 p-value under τ = 0.4 is 0.217, suggesting a minimum of 1.962 ×0.217×(1−0.217)/0.012 ≈ 6522 permutation samples are required to achieve a 0.01 margin of error with a significance level of 0.05. As expected, the permutation p-values converge to the exact p-value with a margin of error of 0.01 close to the expected sample size.

4.2. Large Scale Simulation: Monotone Dependence

We next conducted a large scale simulation study with 1,000 replications, 100 and 200 observations and 5000 permutations, under the data generation model described in Section 4.1 extended to include a wide range of dependence: −0.8 ≤ τ ≤ 0.8. Additionally, we generated censoring times from a uniform distribution over (0,c), where c was tuned to yield censoring percentages of 25% and 50%. In the presence of censoring, the average truncation rates range from 11% to 42% under the scenarios considered. With the larger sample size, we set E = 10. Figure 3 displays the rejection proportions at a 0.05 significance level for the conditional and unconditional permutation minp1 and minp2 tests, as well as for the asymptotic Kendall’s tau test. As expected in this case of monotone dependence, the Kendall’s tau test has the highest power. For the minp1 and minp2 tests, the rejection proportions of the conditional permutation method are quite close to those of the unconditional permutation method and all decrease slightly with increased censoring. When τ = 0, all rejection proportions are close to the nominal level of 0.05. As |τ| increases, both minp1 and minp2 increase in power, with slightly higher power for minp1 than for minp2 for positive τ. This is because for positive τ, minp2 divides the X’s into one group consisting of the low and high X’s and a second group consisting of the moderate X’s, which results in lower power than minp1 that divides the X’s into one group consisting of the low X’s and a second group consisting of the high X’s. For negative τ, we observe many minp2 reduces to minp1 because there tends to be too few events in the rightmost partition.

Figure 4 displays the computing time (in seconds) required by the conditional and unconditional minp1 and minp2, averaged across 100 replicates with n = 100 and 0% censoring, using 1 CPU with AMD Opteron 6274 Processor and 1 GB memory. The unconditional permutation procedure is always faster than the conditional procedure due to its reduced number of comparisons within the permutation algorithm and the smaller sample sizes of its permutations, especially with larger τ. The speed of the conditional permutation procedure does not change with τ because the permutation sample sizes are fixed. While the conditional permutation space is considerably smaller than the unconditional space, this has no practical effect on the speed given that both spaces must be sampled for p-value approximation. The minp2 test is slightly slower than the minp1 test; because of its selection of ϵ, minp2 requires calculation of more log-rank tests.

In summary, under a monotone dependence alternative, our findings suggest that the unconditional permu- tation procedure has comparable power to the conditional permutation procedure, while holding the promise of greater computational efficiency. We also find that minp1 and minp2 have very similar power curves, with an advantage for minp1 under positive monotone dependence, although with lower power than Kendall’s tau test.

4.3. Large Scale Simulation: Non-monotone Dependence

We next evaluated the performance of the tests under non-monotone dependence alternatives. We used a normal copula to specify the joint distribution of (X,| T − 0.5 |), with X drawn from Weibull(0.5,4) and T drawn from Uniform(0,1). This implies that the pre-truncation unconditional Kendall’s taus have different signs for T &lt; 0.5 and T ≥ 0.5. We subjected X to independent censoring using a uniform distribution over (0,c), with c calibrated to achieve the desired censoring rate. Supplementary Figure 3 displays the 290 non-monotone dependence between X and T; we order the scenarios by τ+, defined as the pre-truncation unconditional Kendall’s tau between X and T in the T ≥ 0.5 region. There is a clear non-monotonic relationship between X and T when | τ+ |&gt; 0.4, in which cases, we expect the minp tests to have a substantial advantage over the conditional Kendall’s tau in detecting departures from quasi-independence. For all scenarios, the average truncation rate is roughly 24% to 33%. We used E = 10 for minp1 and minp2.

Figure 5 displays rejection proportions at a significance level of 0.05 for sample sizes n = 100 and 200 based on 1,000 replicates, using 5,000 permutations. As in the monotone dependence simulation, there is no noticeable difference in power between the conditional and unconditional permutation procedures. Under the null hypothesis of τ = 0, all rejection proportions are close to the nominal level of 0.05. However, as τ+ deviates from 0, the asymptotic conditional Kendall’s tau test (Martin and Betensky, 2005) displays very low power. Nonetheless, the rejection proportion from the asymptotic conditional Kendall’s tau test seems to slightly increase with censoring because the dependence structure is weakened by the censoring effect. This improvement, however, is still not satisfactory for practical use. In contrast, both minp tests exhibit high power when τ+ ≠ 0. For the non-monotone alternative considered here, we expect minp2 to be more powerful than minp1 because, by design, the failure times associated with interior T’s are substantially different from these associated with the low and high T’s when τ+ ≠0. As shown in Figure 5, minp2 is more powerful than minp1 in all scenarios except for n = 100 and 0% censoring with τ+ &lt; 0. The truncation effect explains this exception; when τ+ &lt; 0, the right tail is more likely to be truncated and minp1 is more likely than minp2 to partition the data into set of failure times that are more easily distinguished by a log-rank test. However, the differences between minp1 and minp2 in the τ+ &lt; 0 cases become negligible with larger sample sizes or in the presence of censoring, because the large X values associated with interior T’s are more like to be censored when τ+ &lt; 0.

In summary, these results demonstrate that the proposed minp tests are powerful for substantially nonmonotone alternatives, while Kendall’s tau test is not. Among the two minp tests, minp2 is more robust to non-monotonicity. The simulation results also demonstrate the similarity of the conditional and unconditional permutation procedures in practice.

For comparison to the test proposed by Rodríguez-Girondo and de Uña-Álvarez (2016) under a nonmonotone alternative, we replicated one of their simulation studies. Their test is for the Markov condition in the context of the illness-death model, and is analogous to the quasi-independence condition in the context of left truncation (Keiding and Gill, 1990; Rodríguez-Girondo and de Uña-Álvarez, 2012, 2016). In particular, we compared the performances of our proposed minimum p-value tests and the asymptotic conditional Kendall’s tau test, to their weighted integral local Kendall’s tau test, which attains the highest power among all of the tests that they considered in their simulations. We also implemented the weighted log-rank type test proposed by Emura and Wang (2010) using the R package depend.truncation (Emura, 2018). Following Emura and Wang (2010), we used weights that are optimal for Clayton copula type dependence (Lρ=0), Frank copula type dependence (Lρ=1), and Gumbel copula type dependence (Llog). The variance of the weighted log-rank statistics are obtained with a Jackknife approach as in Emura and Wang (2010). Using the simulation settings in Rodríguez-Girondo and de Uña-Álvarez (2016), we generated the truncation time T from a Uniform(0, 2), and given T = t, we generated X using an accelerated failure time model, with log(X −T) = −(T −1)2 +log(ξ), where ξ is drawn from a standard exponential distribution. Supplementary Figure 4 displays the scatterplot of (T,X) for one simulated dataset in the absence of censoring; X and T appear to be negatively associated for T ∈ (0,1) and positively associated for T ∈ (1,2). We considered three levels of censoring, 10%, 30% and 50%, generated from an independent uniform distribution over (0,c), with c set to achieve the desired censoring rates. For the permutation tests, we used E = 10 and 5,000 permutation samples.

Table 2 lists the rejection proportions based on 500 simulated datasets with n = 100 and 250. The results for the weighted local Kendall’s tau for this same simulation setting were reported in Rodríguez-Girondo and de Uña-Álvarez (2016, Table III). The asymptotic conditional Kendall’s tau test and the Lρ=1 and Llog weighted log-rank tests display the lowest power at all levels of censoring and sample sizes. The Lρ=0 test has intermediate power, and the Markov condition test of Rodríguez-Girondo and de Uña-Álvarez (2016) and the minp2 test display the highest power, with the minp2 having superior power at 10% censoring and higher. When n = 100, minp2 exhibits higher power than minp1, and the unconditional permutation approach appears to have higher power than the conditional permutation approach. However, the minp tests and the two permutation approaches yield more comparable power when n = 250. Overall, the power for these tests decreases as censoring increases, likely due to the increasing monotonicity of the dependence relationship in the presence of increasing censoring (see Supplementary Figure 4). Interestingly, the power for all tests drops considerably at the highest level of censoring (50%), likely due to the attenuation of the dependent truncation in the presence of heavy independent censoring. In summary, our proposed minp tests are competitive with the competing tests that also accommodate non-monotone dependence, and even show superior performance in the presence of moderate censoring.

5. Cognitive and Functional Decline in Aging Study

In a study conducted by Mormino et al. (2014), cognitively normal older individuals were aggregated from three independent observational cohort studies: Alzheimer’s Disease Neuroimaging Initiative (ADNI), Australian Imaging Biomarkers and Lifestyle Study of Ageing (AIBL), and Harvard Aging Brain Study (HABS). The AIBL study methodology has been reported previously (Ellis et al., 2009).

These individuals had a global Clinical Dementia Rating (CDR) of 0 at the baseline testing session. In one sub-study, researchers were interested in the association between the accumulation of abnormally folded beta-amyloid protein in the brain and the risk of cognitive decline. Participants were only included in the amyloid sub-study if they had beta-amyloid measured at or after the baseline testing session by a Positron Emission Tomography (PET) scan. Cognitive decline was measured by progression to a global CDR of 0.5, which is left-truncated by the time from baseline to PET scan time. Because the CDR was assessed only at scheduled visits, the time to CDR of 0.5 is necessarily interval-censored between two consecutive visit times. However, because visits were pre-scheduled at fixed times, there is no advantage to treating the times as interval-censored, even in the presence of random scheduling factors that rendered visit times to be slightly discrepant from the scheduled timepoints. This has been observed in similar studies (e.g., Blacker et al., 2007; Lindsey and Ryan, 1998) and is due to the low degree of overlap among intervals. For these reasons, because the visits were scheduled annually for HABS and ADNI, but only once every 18 months for AIBL, we carried out separate analyses for each of the three study and HABS and ADNI combined. However, if the visits were truly random across individuals and there were considerable overlap of the between-visit intervals, our discrete time interpretation would be valid for inference on time to first study visit following an increase in CDR to 0.5, but not on time to increase in CDR to 0.5. If the latter is all that is of interest, the interval censoring would have to be accommodated directly. Unfortunately, there is no software currently available that implements log-rank tests in the presence of interval censoring and random left truncation. Because we are in effect imputing the right endpoint of the interval as the observed time of CDR 0.5, we may be underestimating the variance of the log-rank statistic. This, however, would be a conservative approach, as it would serve to increase the power of the minp tests and lead us to reject quasi-independence more than we should. While this leads to more complicated analyses of the time to events that require modeling of dependence on time to study entry, it will not lead to invalid conclusions if the models used are sufficiently flexible. Additionally, there was right censoring for subjects who did not progress by the end of the follow-up. Of the 490 subjects included in the study, only 41 experienced CDR progression by the end of follow-up and all but 46 had PET scans post-baseline. Figure 6 displays scatterplots of the time (in days) from baseline to CDR progression and the time (in days) from baseline to PET scan for each of the three studies separately, and for HABS and ADNI combined.

The plot for the ADNI cohort (Figure 6(a)) suggests a possible quadratic relationship, though the data are sparse and this is uncertain. There are no apparent dependencies for the other studies. We applied the 385 proposed permutation tests with 5,000 permutation samples, as well as the asymptotic Kendall’s tau test to the data. Since there is a high censoring rate, instead of setting E = 10, we used E = 3 for the tests for each of the three cohorts and E = 8 for HABS and ADNI combined. For comparison, we also applied the weighted log-rank test of Emura and Wang (2010) with the correction for ties, since times were recorded in days and there are ties in both the truncation times and survival times.

The rejection proportions for the hypothesis tests are listed in Table 3. The asymptotic Kendall’s tau test and the weighted log-rank tests (Emura and Wang, 2010) failed to reject the null hypothesis of quasiindependence for the cohorts separately and combined. However, the Lρ=1 test of Emura and Wang (2010) is suggestive of dependence within the ADNI cohort, with its p-value of 0.081. For this cohort, the minp2 test, with its p-values of 0.019 and 0.018, provides slightly stronger evidence in favor of dependent truncation than minp1, with its p-values of 0.111 and 0.082. For the ADNI cohort, the minimum log-rank p-value occurs at T = 238 when using the minp1 approach. This indicates that the distributions of X|T ≤ t and of X|T &gt; t are maximally different at t = 238. Based on minp2, the distributions of X|2 ≤ T ≤ 170 and of X|T &lt; 2 ∪ T &gt; 170 are maximally different. The conditional Kendall’s tau for the two partitions have opposite signs: 0.360 and −0.032, respectively. This finding indicates that among these who received PET scan between {2 ≤ T ≤ 170} days, the time to CDR of 0.5 tends to be positively associated with the time to PET scan. Figure 7 displays the histograms of the permuted log-rank test statistics for minp1 and minp2 under conditional and unconditional permutation. The distributions are very similar across the two permutation procedures as expected based on our simulation studies. There is also possible dependence for the HABS cohort, with minp2 p-values of about 0.075. The minp1 p-values are approximately 0.70, and are not sensitive to any dependence in that cohort. This example reinforces the results of our simulations that the conditional and unconditional permutation tests have similar performances. It also reflects the limitations of Kendall’s tau test and highlights differences between minp1 and minp2.

6. Discussion

The goal of this paper was to propose and investigate tests of quasi-independence that are sensitive to non-monotone alternatives. These tests are needed given that non-monotone dependence structures exist in real studies and that it is necessary to establish quasi-independence before applying the straightforward risk-set adjusted analyses that assume it. We have proposed two such tests that are based on an optimal selection after examination of several thresholded versions of the data. These are just two examples, and many others could be considered. For example, the method of selecting the window width,, for minp2 could be altered. We considered using a constant window width across cutpoints, but a more general approach is to consider different window width for different cutpoints. Moreover, adjustments can be made to E. More than two groups after thresholding could be considered. The more that is known a priori about the nature of the dependence, the better the test can be designed to maximize its power.

In addition to the more flexible tests that we have proposed, we also investigated the use of two methods of permutation: conditional and unconditional. The conditional permutation is theoretically valid and produces exchangeable permutations. It is computationally costly due to its algorithm and its retention of the full sample size, and so we investigated the unconditional alternative. This should be approximately valid under the null hypothesis and for asymptotic test statistic such as the log-rank test.

In extensive simulation studies, we have demonstrated that our proposed tests do attain higher power than the conditional Kendall’s tau test under non-monotone alternatives. Moreover, in some scenarios they may offer improvement over the flexible test proposed by Rodríguez-Girondo and de Uña-Álvarez (2016) and the weighted log-rank tests proposed by Emura and Wang (2010). Additionally, we have shown that the unconditional permutation method does in fact offer computational advantages over the conditional permutation method, while maintaining very similar operating characteristics. We affirmed these findings in our analysis of the cognitive and functional decline in aging study.

It is of interest to consider the application of modern bootstrap methods to our testing problem to complement the permutation tests that we have investigated. Lin (1997) considers the wild bootstrap for analyzing the cumulative incidence function in competing risk data. Given the martingale representation of the Aalen– Johansen estimator, the wild bootstrap in Lin (1997) replaces the martingale increments by the increments of the empirical counting processes, weighted by standard normal random variables. Since the minimally selected test statistics in our paper do not have a nice martingale representation form and are not normally distributed, the wild bootstrap method of Lin (1997) is not applicable to our setting. The weird bootstrap and its extensions (Andersen et al., 2012; Dobler et al., 2017) consider the Nelson–Aalen estimator, and replace the counting process increments at time t with binomial random variables with size Y (t) and success probability dÂ(t), where Y (t) is the size of risk set at t and dÂ(t) is the empirical hazard function at t. It is possible that we could generate a weird bootstrap replication through this process – under the null of quasi-independence – and use it to form a reference distribution for our test statistics. A simpler alternative is the so-called “obvious” bootstrap Gross and Lai (1996), which may offer the most promising bootstrap test of quasi-independence. It involves estimating the truncation-adjusted Kaplan–Meier estimators for the event time and the truncation time under the assumption of quasi-independence, and then generating event times and truncation times independently from the estimators. Like our unconditional permutation, generated pairs do not always satisfy the truncation condition, and so must be discarded. In future work we plan to compare the obvious bootstrap approach with the unconditional permutation approach.

If quasi-independence is not rejected, simple risk-set adjustments can be made to the standard Kaplan– Meier and Cox regression estimators. If quasi-independence is rejected, a dependence model must be assumed; one such model is a copula-based model (e.g Chaieb et al., 2006; Emura and Murotani, 2015; Emura and Pan, 2017; Emura and Wang, 2012; Emura et al., 2011). The depend.truncation package (Emura, 2018) offers a comprehensive collection of the copula-based estimators. As an alternative to the copula-based estimators, Efron and Petrosian (1994) proposed a structural transformation model that estimates latent quasi-independent truncation times to be used in replace of the dependent truncation times. The structural transformation model was extended to accommodate right censoring by Chiou et al. (2018) with implementations available in Chiou (2017b). When covariate effects are of interest, Jones and Crowley (1992) directly models the effect of truncation under the proportional hazards assumptions using the dependent truncation as an additional covariate. Emura and Wang (2016) proposed a semiparametric accelerated failure time model that includes the truncation time as a covariate in the failure time model, similar to the approach of Jones and Crowley (1992) within the framework of the Cox model.

Supplementary Material

1

2

3

Acknowledgements

This research was supported in part by the Harvard NeuroDiscovery Center, the Harvard Clinical and Translational Science Center NIH UL1 TR001102, NIH CA075971, NIH NS094610, and NIH NS048005, NIH P50AG005134, NIH P01AG036694 and K01 AG051718.

The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer’s disease (AD).

Data collection and sharing for this project was funded by the Alzheimer’s Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12–2-0012). ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from the following: AbbVie, Alzheimer’s Association; Alzheimer’s Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research &amp; Development, LLC.; Johnson &amp; Johnson Pharmaceutical Research &amp; Development LLC.; Lumosity; Lundbeck; Merck &amp; Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda Pharmaceutical Company; and Transition Therapeutics. The Canadian Institutes of Health Research is providing funds to support ADNI clinical sites in Canada. Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org). The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimer’s Therapeutic Research Institute at the University of Southern California. ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California.

Figure 1: Side-by-side histograms of log-rank p-value from the minp1 test (E = 2) for the small simulated dataset; black vertical line is the observed log-rank test statistics; (a) and (c) are log-rank p-values from complete permutation (b) and (d) are log-rank p-values from 5000 permutation.

Figure 2: Convergence trace map based on the small simulated dataset with τ = 0.4; gray lines are the convergence traces; black lines are the exactdashed lines are the asymptotic sample size for 0.01 margin of error with 0.05 significance level.p-value computed from all permutation; horizontal dashed lines are the exact p-value±0.01; vertical

Figure 3: Rejection proportion based on 1,000 repetitions at 0.05 significance level when (X,T) was generated under a monotone dependence structure; black lines are based on the conditional procedure and the gray lines are based on the unconditional procedure. Each repetition consists of 5,000 permutation.

Figure 4: Summary of timing results in seconds. The sample size was 100 with 0% censoring. The timing results are averaged from 100 repetitions, each with 5,000 permutations; black lines are based on the conditional procedure and the gray lines are based on the unconditional procedure.

Figure 5: Rejection proportion based on 1,000 replications at 0.05 significance level when (X,T) was generated under a non-monotone dependence structure; black lines are based on the conditional procedure and the gray lines are based on the unconditional procedure. Each repetition consists of 5,000 permutation.

Figure 6: Cognitive and functional decline in aging study (× uncensored event; ◦: censored event).

Figure 7: Histograms of log-rank p-value for the ADNI cohort with black vertical black line indicating the observed log-rank test statistics. (a) and (b) display the log-rank p-value from minp1 test with E = 3. (c) and (d) display the log-rank p-value from minp2 test with with E = 3 and ε = 84.

Table 1: Summary of p-values for the small simulated data presented in Supplementary Figure 1; τ^c* is the conditional Kendall’s tau, ACK is the asymptotic conditional Kendall’s tau test from Martin and Betensky (2005), minp1 is the permutation test based on minp1 test statistics, minp2 is the permutation test based on minp2 test statistics.

	Conditional	Unconditional	
	τ^c*	ACK	Minp1	minp2	Minp1	minp2	
τ = 0	−0.069	0.579	0.706	0.852	0.699	0.853	
τ = 0.4	0.385	0.341	0.217	0.319	0.269	0.299	

Table 2: Summary of rejection proportions based on the simulation settings in Rodríguez-Girondo and de Uña-Álvarez (2016); MC is the Markov condition testing procedure based on the weighted integral local Kendall’s tau test statistic reported in Rodríguez-Girondo and de Uña-Álvarez (2016, Table III), ACK is the asymptotic conditional Kendall’s tau test from Martin and Betensky (2005), Lρ=0, Lρ=1, Llog are the weighted log-rank type statistics proposed by Emura and Wang (2010), minp1 is the permutation test based on minp1 test statistics, minp2 is the permutation test based on minp2 test statistics. The rejection proportions are computed based on 100 and 250 observations with 500 replications.

	Emura and Wang	Conditional	Unconditional	
Censoring	MC	ACK	Lρ=0	Lρ=1	L log	minp1	minp2	minp1	minp2	
					n = 100					
0%	0.354	0.042	0.128	0.052	0.072	0.248	0.300	0.266	0.324	
10%	0.236	0.054	0.130	0.050	0.066	0.146	0.242	0.176	0.294	
30%	0.172	0.056	0.134	0.032	0.082	0.120	0.216	0.152	0.296	
50%	0.138	0.070	0.142	0.018	0.074	0.054	0.190	0.066	0.228	
					n = 250					
0%	0.750	0.058	0.280	0.096	0.122	0.610	0.621	0.604	0.590	
10%	0.682	0.084	0.312	0.086	0.138	0.566	0.602	0.564	0.582	
30%	0.516	0.112	0.320	0.054	0.164	0.474	0.590	0.484	0.576	
50%	0.342	0.156	0.368	0.038	0.174	0.344	0.448	0.398	0.435	

Table 3: Summary of p-values for the Cognitive and Functional Decline in Aging Study; τ^c* is the conditional Kendall’s tau, ACK is the asymptotic conditional Kendall’s tau test from Martin and Betensky (2005), Lρ=0, Lρ=1, Llog are the weighted log-rank type statistics proposed by Emura and Wang (2010), minpi is the permutation test based on minpi test statistics, minp2 is the permutation test based on minp2 test statistics. 5,000 permutation were used for each permutation test.

	Emura and Wang	Conditional	Unconditional	
	n	Δ = 1	τ^c*	ACK	Lρ=0	Lρ=1	L log	minp1	minp2	minp1	minp2	
ADNI+HABS	329	26	0.102	0.293	0.881	0.572	0.746	0.409	0.456	0.357	0.448	
ADNI	198	17	0.149	0.292	0.313	0.081	0.755	0.111	0.019	0.082	0.018	
HABS	161	15	−0.174	0.276	0.429	0.620	0.750	0.703	0.076	0.692	0.074	
AIBL	131	9	−0.185	0.418	0.674	0.562	0.742	0.456	0.317	0.257	0.308	

1 Present address: Department of Mathematical Sciences, The University of Texas at Dallas; 800 W. Campbell Road, Richardson, TX 75080, U.S.A.

2 Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf

3 Data used in the preparation of this article was obtained from the Australian Imaging Biomarkers and Lifestyle flagship study of ageing (AIBL) funded by the Commonwealth Scientific and Industrial Research Organisation (CSIRO) which was made available at the ADNI database (www.loni.usc.edu/ADNI). The AIBL researchers contributed data but did not participate in analysis or writing of this report. AIBL researchers are listed at www.aibl.csiro.au.

Supplementary Materials

The online supplementary materials contain the small scale simulation dataset used in Section 4.1 and additional simulation results.

This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.


Reference

Andersen PK , Borgan O , Gill RD , Keiding N , 2012 Statistical models based on counting processes. Springer Science &amp; Business Media.
Austin MD , Betensky RA , 2014 Eliminating bias due to censoring in Kendall’s tau estimators for quasi-independence of truncation and failure. Computational Statistics &amp; Data Analysis 73 , 16–26.24505164
Betensky RA , 2001 Optimally selected Chi-square statistics for equivalence testing. Journal of Statistical Planning and Inference 93 , 247–257.
Betensky RA , Rabinowitz D , 1999 Maximally selected χ2 statistics for k × 2 tables. Biometrics 55 , 317–320.11318175
Betensky RA , Rabinowitz D , 2000 Simple approximations for the maximal transmission/disequilibrium test with a multi-allelic marker. Annals of Human Genetics 64 , 567–574.11281219
Blacker D , Lee H , Muzikansky A , Martin EC , Tanzi R , McArdle JJ , Moss M , Albert M , 2007 Neuropsychological measures in normal individuals that predict subsequent cognitive decline. Archives of neurology 64 , 862–871.17562935
Chaieb LL , Rivest LP , Abdous B , 2006 Estimating survival under a dependent truncation. Biometrika 93 , 655–669.
Chen CH , Tsai WY , Chao WH , 1996 The product-moment correlation coefficient and linear regression for truncated data. Journal of the American Statistical Association 91 , 1181–1186.
Chen Y , Liu JS , 2007 Sequential Monte Carlo methods for permutation tests on truncated data. Statistica Sinica, 857–872.
Chiou SH , 2017a permDep: Permutation tests for general dependent truncation. R package version 1.0–0.
Chiou SH , 2017b tranSurv: Estimating a survival distribution in the presence of dependent left truncation and right censoring. R package version 1.1–5.
Chiou SH , Austin M , Qian J , Betensky RA , 2018 Transformation model estimation of survival under dependent truncation and independent censoring Unpublished article.
Dobler D , Beyersmann J , Pauly M , 2017 Non-strange weird resampling for complex survival data. Biometrika 104 , 699–711.
Efron B , Petrosian V , 1992 A simple test of independence for truncated data with applications to redshift surveys. The Astrophysical Journal 399 , 345–352.
Efron B , Petrosian V , 1994 Survival analysis of the gamma-ray burst data. Journal of the American Statistical Association 89 , 452–462.
Ellis K , Rowe C , Masters C , Martins R , Hudson P , Milner A , Bevege L , Ames D , 2009 Baseline data from the Australian imaging biomarkers and lifestyle flagship study of ageing. Alzheimer’s &amp; Dementia 5 , e4.
Emura T , 2018 depend.truncation: Statistical methods for the analysis of dependently truncated data. URL: https://CRAN.R-project.org/package=depend.truncation. R package version 3.0.
Emura T , Murotani K , 2015 An algorithm for estimating survival under a copula-based dependent truncation model. TEST 24 , 734–751.
Emura T , Pan CH , 2017 Parametric likelihood inference and goodness-of-fit for dependently lefttruncated data, a copula-based approach. Statistical Papers, 1–23.
Emura T , Wang W , 2010 Testing quasi-independence for truncation data. Journal of Multivariate Analysis 101 , 223–239.
Emura T , Wang W , 2012 Nonparametric maximum likelihood estimation for dependent truncation data based on copulas. Journal of Multivariate Analysis 110 , 171–188.
Emura T , Wang W , 2016 Semiparametric inference for an accelerated failure time model with dependent 535 truncation. Annals of the Institute of Statistical Mathematics 68 , 1073–1094.
Emura T , Wang W , Hung HN , 2011 Semi-parametric inference for copula models for truncated data. Statistica Sinica 21 , 349–67.
Good P , 2013 Permutation tests: A practical guide to resampling methods for testing hypotheses. Springer Science &amp; Business Media.
Gross ST , Lai TL , 1996 Bootstrap methods for truncated and censored data. Statistica Sinica, 509–530.
Halpern AL , 1999 Minimally selected p and other tests for a single abrupt changepoint in a binary sequence. Biometrics, 1044–1050.11315046
Heller R , Heller Y , Gorfine M , 2012 A consistent multivariate test of association based on ranks of distances. Biometrika 100 , 503–510.
Heller R , Heller Y , Kaufman S , Brill B , Gorfine M , 2016 Consistent distribution-free K-sample and independence tests for univariate random variables. Journal of Machine Learning Research 17 , 1–54.
Hothorn T , Lausen B , 2003 On the exact distribution of maximally selected rank statistics. Computational Statistics &amp; Data Analysis 43 , 121–137.
Huang Y , Berry K , 2006 Semiparametric estimation of marginal mark distribution. Biometrika 93 , 895–910.
Jones MP , Crowley J , 1992 Nonparametric tests of the Markov model for survival data. Biometrika 79 , 513–522.
Kaufman S , Heller R , Heller Y , Gorfine M , 2013 Consistent distribution-free tests of association between univariate random variables. arXiv preprint arXiv:1308.1559.
Keiding N , Gill RD , 1990 Random truncation models and Markov processes. The Annals of Statistics, 582–602.
Klein JP , Moeschberger ML , 2003 Survival analysis: Techniques for censored and truncated data. Springer Science &amp; Business Media.
Koziol JA , 1991 On maximally selected Chi-square statistics. Biometrics, 1557–1561.
Lagakos S , Barraj L , De Gruttola V , 1988 Nonparametric analysis of truncated survival data, with application to AIDS. Biometrika 75 , 515–523.
Lausen B , Schumacher M , 1992 Maximally selected rank statistics. Biometrics 48 , 73–85.
Li Y , Prentice RL , Lin X , 2008 Semiparametric maximum likelihood estimation in normal transformation models for bivariate survival data. Biometrika 95 , 947–960.19079778
Lin D , 1997 Non-parametric inference for cumulative incidence functions in competing risks studies. Statistics in medicine 16 , 901–910.9160487
Lindsey JC , Ryan LM , 1998 Methods for interval-censored data. Statistics in medicine 17 , 219–238.9483730
Martin EC , Betensky RA , 2005 Testing quasi-independence of failure and truncation times via conditional Kendall’s tau. Journal of the American Statistical Association 100 , 484–492.
Miller R , Siegmund D , 1982 Maximally selected Chi square statistics. Biometrics 38 , 1011–1016.
Mormino EC , Betensky RA , Hedden T , Schultz AP , Ward A , Huijbers W , Rentz DM , Johnson KA , Sperling RA , , 2014 Amyloid and apoe ε4 interact to influence short-term decline in preclinical Alzheimer disease. Neurology 82 , 1760–1767.24748674
Othus M , Li Y , 2010 A gaussian copula model for multivariate survival data. Statistics in biosciences 2 , 575 154–179.22162742
Pan W , 1998 Rank invariant tests with left truncated and interval censored data. Journal of Statistical Computation and Simulation 61 , 163–174.
Rabinowitz D , Betensky RA , 2000 Approximating the distribution of maximally selected McNemar’s statistics. Biometrics, 897–902.10985234
Rodríguez-Girondo M , de Uña-Álvarez J , 2012 A nonparametric test for Markovianity in the illness-death model. Statistics in Medicine 31 , 4416–4427.22975898
Rodríguez-Girondo M , de Uña-Álvarez J , 2016 Methods for testing the Markov condition in the illness– death model: A comparative study. Statistics in Medicine 35 , 3549–3562.26990971
Shen PS , 2015 Nonparametric tests for left-truncated and interval-censored data. Journal of Statistical Computation and Simulation 85 , 1544–1553.
Therneau TM , 2015 A package for survival analysis in S. URL: https://CRAN.R-project.org/package=survival. version 2.38.
Therneau TM , Grambsch PM , 2013 Modeling survival data: Extending the Cox model. Springer Science &amp; Business Media.
Tsai WY , 1990 Testing the assumption of independence of truncation time and failure time. Biometrika 77 , 169–177.
Tsai WY , Jewell NP , Wang MC , 1987 A note on the product-limit estimator under right censoring and left truncation. Biometrika 74 , 883–886.
Turnbull BW , 1976 The empirical distribution function with arbitrarily grouped, censored and truncated 595 data. Journal of the Royal Statistical Society. Series B (Methodological) 38 , 290–295.
de Uña-Álvarez J , 2012 On the Markov three-state progressive model, in: Recent Advances in System Reliability. Springer, pp. 269–281.
Wang MC , 1991 Nonparametric estimation from cross-sectional survival data. Journal of the American Statistical Association 86 , 130–143.
Woodroofe M , 1985 Estimating a distribution function with truncated data. The Annals of Statistics 13 , 163–177.
