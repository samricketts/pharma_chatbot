LICENSE: This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.


101208441
32337
Curr Alzheimer Res
Curr Alzheimer Res
Current Alzheimer research
1567-2050
1875-5828

33032509
7719300
10.2174/1567205017666201008110854
NIHMS1644406
Article
Automatic Assessment of Cognitive Tests for Differentiating Mild Cognitive Impairment: A Proof of Concept Study of the Digit Span Task
Asgari Meysam 14*
Gale Robert 14
Wild Katherine 24
Dodgeb Hiroko 234
Kaye Jeffrey 24
1 Center for Spoken Language Understanding, Oregon Health &amp; Science University (OHSU), Portland, Oregon, USA
2 Department of Neurology, NIA-Layton Aging and Alzheimer’s Disease Center, Oregon Health &amp; Science University (OHSU), Portland, Oregon, USA
3 Department of Neurology, Michigan Alzheimer’s Disease Center, University of Michigan, Ann Arbor, Michigan, USA
4 Department of Neurology, Oregon Center for Aging &amp; Technology (ORCATECH), Oregon Health &amp; Science University (OHSU), Portland, Oregon, USA
* Address correspondence to this author at the Center for Spoken Language Understanding, Oregon Health &amp; Science University (OHSU), Portland, Oregon, USA and Department of Neurology, Oregon Center for Aging &amp; Technology (ORCATECH), Oregon Health &amp; Science University (OHSU), Portland, Oregon, USA; asgari@ohsu.edu
25 11 2020
2020
06 12 2020
17 7 658666
This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.
Background:

Current conventional cognitive assessments are limited in their efficiency and sensitivity, often relying on a single score such as the total correct items. Typically, multiple features of response go uncaptured.

Objectives:

We aim to explore a new set of automatically derived features from the Digit Span (DS) task that address some of the drawbacks in the conventional scoring and are also useful for distinguishing subjects with Mild Cognitive Impairment (MCI) from those with intact cognition.

Methods:

Audio-recordings of the DS tests administered to 85 subjects (22 MCI and 63 healthy controls, mean age 90.2 years) were transcribed using an automatic speech recognition (ASR) system. Next, five correctness measures were generated from Levenshtein distance analysis of responses: number correct, incorrect, deleted, inserted, and substituted words compared to the test item. These per-item features were aggregated across all test items for both forward digit span (FDS) and backward digit span (BDS) tasks using summary statistical functions, constructing a global feature vector representing the detailed assessment of each subject’s response. A support vector machine classifier distinguished MCI from cognitively intact participants.

Results:

Conventional DS scores did not differentiate MCI participants from controls. The automated multi-feature DS-derived metric achieved 73% on AUC-ROC of the SVM classifier, independent of additional clinical features (77% when combined with demographic features of subjects); well above chance, 50%.

Conclusion:

Our analysis verifies the effectiveness of introduced measures, solely derived from the DS task, in the context of differentiating subjects with MCI from those with intact cognition.

Neuropsychological tests
Short term memory
Digit Span
Biomarkers
Mild cognitive impairment (MCI)
Computerized assessment

1. INTRODUCTION

The assessment of cognitive change is fundamental in determining whether an individual may be developing mild cognitive impairment (MCI), the clinical transition state often precedent to dementia. There are a number of specific settings where this cognitive assessment is a priority. These range from screening for MCI for case identification in clinical trials, to substantiating progressive cognitive decline for diagnostic purposes. Typically, cognitive assessment has been conducted during a clinical visit either in a screening mode by a clinician using a familiar brief test battery, or for a more expansive assay of the patient’s cognitive landscape by a trained psychometrician or neuropsychologist who administers a battery of standardized tests that survey multiple domains of cognitive function. Full assessment may take from 90 minutes to several hours. In general, these assessments have a number of constraints including requiring a trained clinician, only occurring at a time and location convenient to both the patient and the assessor, and being relatively “noisy” in terms of high intra- and inter- individual variability.

Recent years have seen several advances to improve the shortcomings of current conventional cognitive assessments for MCI and dementia [1–4]. Among these are innovations in the administration and evaluation of the responses that are afforded by the digital recording of spoken language responses to cognitive tests coupled with automated speech and language analysis of the captured audio files [5]. This approach provides greater quality control of the testing itself, as well as the ability to quantitate not only traditional metrics such as total number of correct responses, but meta-data regarding, for example, the timing of responses, the character of the speech and language contained within each response, and automated analysis across multiple responses.

Despite the promise of these approaches, there is not yet a deep experience with or wide adoption of automated cognitive test analysis approaches. This may, in part be related to the relatively small evidence base of this assessment paradigm, the often proprietary nature of computerized cognitive tests, the comfort of continuing to use tests that have been employed for decades and are widely familiar, and the slow adoption of digital behavioral biomarkers in general. In this context, we incorporated several approaches to overcome these barriers, based on the principle that one ideally could use audio recorded legacy tests administered to patients that would provide conventional summary scores, while at the same time use the digital audio files captured during these testing sessions to analyze automated speech and language characteristics beyond the conventional scoring. This would create a “cross-walk” back to the conventional testing experience and establish the sensitivity of digital measures. In addition, in order to facilitate research development and potentially wider use and adoption, we required tests to be developed in this way should be in the public domain.

As a proof of concept of this approach, we evaluated the Digit Span (DS) task, a verbal attention and working memory task that has been widely used in the cognitive assessment of both children and adults for over a century [6]. It has been incorporated in several commonly used dementia screening batteries, such as the Montreal Cognitive Assessment [7], the Saint Louis Mental Status Examination [8] and the Neurobehavioral Cognitive Status Examination [9]. Here, “short-term memory” accounts for a person’s ability to recall a few presented digits, in a given short period of time [10]. In administering the DS task, an examiner aurally presents sequences of digits with increasing length (e.g., “2 5 6 8”, etc.) to subjects who are then asked to recite the digits in the same exact order. The test consists of two parts: repeating digit sequences in the order presented (forward digit span or FDS) and then a second series of digits backward in reverse order (backward digit span or BDS).

Conventionally, the total number of correctly recalled digit spans from both FDS and BDS comprises the final score of the test. One drawback of this composite score is the equal attention to the correctly recalled digits of both FDS and BDS for its final assessment, given the fact that the two tests measure different cognitive capacities. The BDS requires an additional mental process of not only remembering the forward sequence as presented, but of transposing digits into the reverse order. Prior studies have shown that FDS is associated more with attention, while BDS is more closely related to working memory [11].

In clinical practice, the conventional total scores on the DS tests (forward and/or backward) by themselves are relatively insensitive to early cognitive decline [12]. This lack of sensitivity may be due to missing a wealth of other clinically relevant information that is available in the responses beyond the simple total score. In this study, we introduce automated methods for extracting new “correctness” measures from verbal responses that may add value in distinguishing participants with MCI from those with intact cognition from a conventional test session.

2. METHODS

2.1. Study Participants, Data Collection and Corpus

Subjects in this study came from existing community cohort studies of brain aging at the NIA-Layton Aging &amp; Alzheimer’s Disease Center at Oregon Health &amp; Science University (OHSU). All human data included in this manuscript were obtained in compliance with regulations of the OHSU’s Institutional Review Board (IRB); all subjects signed written informed consent. Each participant had annual standardized clinical, neurological and neuropsychological examinations, which included the Uniform Data Set battery of the National Alzheimer’s Disease Coordinating Center (Morris et al., 2006, http://www.alz.washington.edu). During the neuropsychological testing, audio recordings of the participant’s responses were made for later analysis using a professional recorder. For this study, MCI was defined according to the Clinical Dementia Rating (CDR) scale [13], based on clinical interviews with the subject and an informant, and a clinician-administered Neurobehavioral Cognitive Status Examination (Cognistat) [14], the latter used by the clinician to identify cognitive deficits without reference to the neuropsychological battery obtained separately. The reason for this is to avoid the inherent circularity of studying subjects diagnosed with MCI based on neuropsychological tests who are then experimentally identified using alternate forms of the same tests. Thus, we rely on a definition that does not initially depend on these test scores. The CDR score has been reported to have high expert inter-rater reliability [15], and, critically, provides subject assessments that are independent of the neuropsychological examinations being used in this study. For the purposes of this study, participants are diagnosed with MCI if the CDR score is 0.5 on two consecutive annual visits.

Eighty-five subjects with audio recorded DS tests were used for analysis. Out of 85 audio recordings of FDS and BDS test sessions, 63 participants had a CDR of 0 (healthy) and 22 had a CDR of 0.5 (MCI). Table 1 presents means and standard deviations (SD) for age, sex, years of education, and Mini-Mental State Examination (MMSE) score [16], and the FDS and BDS test results based on a conventional scoring method (see Section 4.2.1). The mean Mini-Mental State Examination (MMSE) [16] score at baseline was 26.6 (SD=2.1) among those with MCI; and 28.4 (SD 1.7) among those with MCI; and 28.4 (SD 1.7) among those with normal cognition.

2.2 Digit Span Test

The DS test was aurally presented as sequences of digits, ranging from 2 to 8 numbers, of increasing length. The test consists of two parts: repeating digit sequences in the order presented (forward digit span or FDS) or backward in reverse order (backward digit span or BDS). In our DS test paradigm, the FDS sequence starts at a length of three digits and the length is increased up to a span of eight digits. Then the task starts over for the BDS at a length of two digits and the subject is asked to respond with the digits in reverse order, up to a maximum span of seven digits. At each sequence length, two trials are administered; therefore, each subject can be presented with up to 24 test items (12 test items in both forward and backward tasks) upon the successful response to all items. The test administration stops after the failure of two consecutive trials of the same length.

2.2.1 Conventional Digit Span Scoring

Conventionally, the DS responses are assessed based on three scores: the total number of correctly recalled test items (digit sequences) from both FDS and BDS, and the longest digit spans correctly recalled before the termination of the test in both FDS and BDS tests. However, this assessment does not take into account other clinically relevant information that can be computed from the DS response. As an example, consider the following two responses to a BDS test item “two–eight–three”: 1) “three–eight–”, and 2) “three–three–eight–two”. From the conventional scoring prospective, both responses are incorrect. However, characteristics of these two responses differ from each other. There is a “substitution error” in the first response (“one” to “two”) while there is an “insertion error” in the second response (incorrect insertion of the “three”). In addition, conventional scoring pays an equal weight to test items summing up the binary (zero or one) item-level scores for constructing the final test score. However, summing item-level scores across all FDS and BDS test items ignores the intrinsic complexity of presented items, especially with longer digit sequences.

2.3 Proposed Measures of the DS Test

To address some of the limitations of conventional scoring, we developed a set of automatically derived item-level measures in addition to a statistical approach for summarizing item-level measures into a global subject-level feature vector used in our classification model.

2.3.1 Item-level Correctness Measures

The proposed item-level measures revolve around a notion of “correctness”. To assess the correctness of each presented test item in detail, we first compute the “edit distance” (also known as Levenshtein distance). Edit distance is a way of quantifying the minimum number of edits — the sum of words inserted, deleted, or substituted — required to transform the correct answer to the subject’s attempt. By comparing the response to the target test item, we produce five correctness measures: the number of correct, deleted, inserted, and substituted words in addition to edit distance. These five features represent our item-level evaluation of verbal responses.

2.3.2 Subject-level Correctness Features

Over the course of the DS test, each subject may respond up to 12 FDS and 12 BDS test items, producing a maximum of 24 verbal responses at the end of the test. As mentioned in Section 4.3, each verbal response to a test item is represented by a set of five correctness measures. For the purpose of training classification models, one needs to summarize this detailed assessment of each response into a global feature vector of a fixed dimension across all test items in both FDS and BDS tests. Each of the five correctness measures is summarized across all 12 FDS and BDS items in terms of five standard distribution statistics including mean, median, standard deviation, minimum, and maximum constructing 25-dimensional feature vectors. For missing test items, we imputed item-level measures by zero. Table 2 shows an example of correctness features and their aggregation for a participant performing the FDS task.

FDS and BDS tasks may draw upon different cognitive capacities (attention vs. working memory) and thus, measures extracted from FDS items may differ in nature compared to those from BDS. To preserve potential differences, item-level measures of FDS and BDS tests were separately summarized into two feature vectors. The resulting feature vectors, computed separately for the FDS and BDS, were finally aggregated into one global feature vector, comprised of up to 50-dimensions for each subject. Fig. (1) represents the block diagram of our proposed method for extracting the subject-level feature vector based on responses to DS test items.

2.3.3. Statistical Analysis of Features

To investigate the relative importance of proposed features in our classification task, we conducted a statistical test using the Student’s t-test, examining differences between MCI and cognitively intact participants Table 3. The statistical analysis using the 50 subject-level features (described at Section 2.3.2) identified a number of features which differentiated MCI and cognitively intact participants. Five of the correctness features showed a significant difference (p &lt; 0.05), including three from the DS Forward test and two from the DS Backward test.

Baseline demographic factors of participants were likewise assessed using the Student’s t-test, also presented in Table 3. The Student’s t-test shows a difference (p = .014) between the mean age of participants with MCI being older, compared to those with intact cognition; but not in years of education (p = .706). Scores for the entire MMSE unsurprisingly showed a stark difference (p&lt;0.001), but neither of the forward DS (p = .386) nor backward DS (p = .464) test scores (total number of correctly recalled test items) were significantly different between groups, confirming the insensitivity of the conventionally scored DS test to discriminate participants with MCI from those with intact cognition.

2.4. Learning Strategies

Our learning objective is to train probabilistic models (binary classifiers in this problem) that distinguish participants with MCI from those with intact cognition based on the measures (described in Section 4.3) extracted from the verbal responses of participants. This can be cast into a hypothesis test problem, in which true and null hypotheses, H1 and H0, are the prediction of the participant with MCI or cognitively intact, respectively. Both likelihoods are computed from the probabilistic model trained on training examples of both MCI and cognitively intact classes. The efficiency of a classifier ultimately depends on the quality of extracted measures as well as the discriminant power of the probabilistic model. To formulate the training process, let the D-dimensional feature vector (D is total number of measures) extracted from all verbal responses of a participant be Xi and let yi ∈ {+1, −1} represent the participant’s cognitive status (MCI as 1 and cognitively intact as −1). Also, lef(x, y) be a classification function parameterized by model parameters, θ, estimated in the training process from a set of n training examples, D = (xi, yi); i = 1,…, n. In the inference time, the model predicts the participant’s class label (MCI or cognitively intact) given the feature vector extracted from verbal responses. As a probabilistic model for distinguishing participants with MCI from those with intact cognition, we used support vector machines (SVMs) [17]. SVMs are among the best discriminate models widely used in pattern recognition, classification, and regression problems. SVMs, trained in a supervised learning fashion, construct a hyperplane in a high dimensional space such that it best discriminates data points into two classes. One of the main advantages of SVM is its effectiveness in cases where the dimension of feature vectors is greater than the number of training samples. This makes the use of SVMs particularly suitable in our experiment where there is a relatively small pool of subjects versus a relatively larger dimension of feature vectors. We train a SVM classifier with a linear kernel employed from the open-source Scikit-learn toolkit [18] independently for different sets of measures. In the machine learning literature [19], a non-linear SVM is recommended for classifying data points that are not linearly separable. For comparison purposes, we also use a random classifier, referred to as “chance” in our experiments, that randomly classifies all subjects into two classes.

2.4.1. Feature Selection

Employing a vast range of features in the training of statistical models does not necessarily lead to more effective models. In fact, it may increase the risk of overfitting due to non-informative features employed during training. Overfitting occurs when the capacity of the model is large, leading to the memorization of training examples rather than learning the broadly applicable structure of the training data. Often times, a strategically reduced subset of features can significantly improve the model’s predictive power [20]. There are a number of possible approaches for feature selection, the process of selecting a subset of relevant and more informative features. Among the techniques proposed in the literature, we limit ourselves to use two different methods in our experiments: 1) univariate feature selection, in which the best set of features are selected based on a univariate statistical test. This simple method independently applies a statistical test to each feature and retains those with high statistical scores, and 2) recursive feature elimination (RFE) that recursively removes features, constructs a smaller subset of features, and calculates the model accuracy given each remaining subset. The elimination process continues until all features are exhausted. Finally, the feature set that maximizes the model sensitivity across all feature sets is selected as the best performing feature set. For both suggested feature selection methods, we used the open-source Scikit-learn toolkit to conduct the experiments.

2.4.2. Performance Criteria

To evaluate the performance of the proposed classifier, we adopted the following evaluation metrics: (1) Accuracy - in our binary classification, accuracy is the proportion of participants that are correctly identified in both intact and MCI classes divided by the total number of participants. The accuracy itself does not properly represent the performance of the model due to the imbalanced number of MCI compared to cognitively intact participants in our cohort; (2) Sensitivity - the portion of correctly identified MCI participants (true positives). Sensitivity (SE) assesses the capability of the model to distinguish MCI from cognitively intact participants; (3) Specificity - the portion of correctly identified cognitively intact participants (true negative). Specificity (SP) measures how well the model avoids false positives; and (4) Area under the curve of receiver operating characteristics (AUC-ROC). The most common method for evaluating the performance of a binary classifier is the ROC [21], which plots the sensitivity (true positive rate) of the classifier versus 1 —specificity (false positive rate) of the classifier as the classification threshold varies. We use a classification threshold in a grid search schema to cover the most positive threshold (everything true) to the most negative threshold (everything false). In our experimental setup, for each performance criterion, we report the average results of 100 randomized and stratified trials.

2.4.3. Cross-validation on the Imbalanced Dataset

To demonstrate whether our statistical analyses and experimental results were independent of our data sets, we used cross-validation (CV) techniques in which the train and test sets are rotated over the entire data set. We used a five-fold cross validation scheme, setting all model parameters using four of the sets as the training set, and using the fifth one only for reporting the performance estimates. Parameters of the optimal SVM model were determined on the training set separately for each fold via grid search and cross-validation. Also, due to the imbalanced number of participants in our cohort, partitioning data into train and test sets via CV could result in an imbalanced test set. For example, in a 5-fold scenario, randomly assigning 20% of 85 participants, 22 with MCI and 63 cognitively intact, into the test set might result in a case where a few MCI participants coincide with more cognitively intact participants in the test set. This will result in a highly imbalanced test set in which performing CV will negatively affect the overall conclusion on the performance of the classifier. We address this potential issue through an iterative process. In the first step, we randomly permutate the entire data set, perform 5-fold CV, and accumulate averaged scores at the end of each iteration. Next, we repeat the first step and calculate the overall performance by taking the average of 5-fold CV scores across iterations. The iteration is repeated until the overall performance converges to a steady state. Our experiments showed that after about 100 iterations of 5-fold CV, the overall performance converged.

2.5. Fully Automating DS Evaluation with ASR

In order to fully automate the Digit Span task evaluation, transcripts are provided by an automatic speech recognition (ASR) system. For this process, we employed Kaldi ASR, an open source research toolkit [22]. Kaldi ASR is a multilayered system: first, acoustic probabilities are represented with Gaussian mixture models and hidden Markov models (HMM-GMM) built for the Mel-frequency cepstral coefficient (MFCC) states extracted from the audio; and second, lexical and grammatical models are represented with weighted finite state transducers (WFSTs). As we are transcribing speech with a small training set and a limited vocabulary, the Kaldi ASR model is well suited to the task. Additionally, Kaldi allows us to build a speaker-dependent acoustic model, which enables us to tune the ASR to a specific speaker. When transcribing the speech of people who are elderly or speech-impaired, speaker-dependent features are especially helpful [23]. All classification experiments were performed both with ASR and manually transcribed inputs to measure the effect of this automation.

3. RESULTS

As described in Section 4.2, subjects can receive up to 24 test items (2 test items per 6 spans in both FDS and BDS) upon the successful response to all items. Fig. (2) demonstrates the edit distance between verbal responses and test items, averaged on test items with the same span across all MCI and cognitively intact participants, as a function of item span (x-axis) in both FDS (left plot) and BDS (right plot) tasks. In these plots, n (depicted on each bar) denotes the total number of items received by all MCI or cognitively intact participants. Note that given 63 and 22 participants in MCI and intact groups, groups can receive up to 126 and 44 test items at each span, respectively. The bar graphs show that n decreases as a function of item span regardless of the cognitive state. As observed in the bar graphs, longer items (more complex) result in the higher edit distance, contradicting the assumption of conventional scoring on equal continuation of item-level scores to the final score.

3.1. Classification Models

Using the subject-level features extracted from the verbal responses, we evaluated the performance of the SVM classifiers in the context of distinguishing participants with MCI from those with intact cognition. Table 4 reports the results of classifiers, trained on different feature sets derived from both manual and ASR generated transcriptions of BDS and FDS responses. To understand the contribution of the different features, we introduced them incrementally, and their performance is shown in Table 4. Also, we only considered the most useful features chosen by automatic feature selection methods described in section Section 4.4.1 throughout our classification experiments. Results present the average sensitivity, specificity, AUC-ROC, and accuracy of test sets in randomized 5-fold cross-validations over 100 stratified repeats (as described in Section 4.4.3) for classifying 85 subjects into the two classes, MCI and cognitively intact.

In order to explore the effectiveness of correctness measures, we compared the results of our final models, trained on manually and ASR generated features, with three models. The first model, referred to as the “chance” model in the table, randomly classifies subjects into two classes. The second model is a SVM classifier trained on demographic features of subjects (referred to as Demog. in Table 4), including age, sex, and years of education. Previous studies have suggested that age, sex, and years of education are important variables in the prediction of dementia. To examine this hypothesis, we use the raw DS scores, along with the demographic features, to develop our third baseline model. Moreover, to compare the effectiveness of correctness features derived from ASR-generated transcriptions to those features derived from the manually transcribed responses, we repeat the same classification tasks over the same set of subjects.

As shown by the results (Fig. 3), all SVM classifiers — whether trained on manually transcribed or ASR generated features — outperform all three baseline models in terms of sensitivity. This indicates the discriminatory power of correctness features compare to conventional DS score used in two baseline models. In addition, it indicates that combining demographic features to the correctness features results in an additional improvement. Results also suggest that ASR-derived features yield classification performance comparable to that of manually-derived features. Comparing the classification performance within the baseline models indicates that demographic features contribute to predicting dementia as expected and has been shown by previous studies.

3.2 Effectiveness of Forward and Backward DS Tasks

As noted above, verbal responses were elicited from participants by administering the two DS tasks, FDS and BDS, which may contribute disproportionately to the discriminative power of the test. To gauge the relative importance of the FDS and BDS tasks in distinguishing MCI from cognitively intact participants, we repeated the same classification task, but using features that were separately extracted from FDS and BDS manual transcriptions. Results, reported in Table 5 show that correctness features extracted from BDS are better at distinguishing MCI from cognitively intact participants based on both sensitivity (.399 vs. .247) and AUC-ROC (.570 vs. .443) measures. When the demographic features age, sex, and years of education are added to correctness features, all measures improve, and BDS still outperforms FDS in both sensitivity (.438 vs .364) and AUC-ROC (.639 vs .582).

4. DISCUSSION

Using a computational approach for automatic assessment of verbal responses to the Digit Span (DS) task, a widely used cognitive test included in several dementia screening batteries, we identified several novel aspects relevant to cognitive test screening: 1) Whereas the conventional scoring, as expected, was insensitive to differentiating MCI from normal controls, an extended set of features beyond simple DS extracted from audio recordings could differentiate the two groups; 2) Correctness measures taking into account the differential progressive task difficulty alone were sensitive to differentiating MCI; 3) The classification results suggest that DS test is not adequate as a stand-alone test to detect, differentiate, or diagnose a patient with potential MCI. However, the fact that the usually insensitive DS test becomes “sensitive” when analyzed beyond number returned allows it to be used as a part of a brief screening battery that is easy to do complete, not too challenging or frustrating, and “amenable” to automation; 4) Automation of the task using ASR for assessing or extracting the features from digital recordings is feasible, although not as accurate, and 5) Correctness measures of FDS alone may be as sensitive as BDS suggesting an automated, briefer and less stressful version of the DS task.

Conventionally, the test score is comprised entirely of the total number of correctly produced digit spans in both forward DS and backward DS tasks. Highlighting a few limitations of current conventional assessment of the DS tasks, we proposed a new set of measures, automatically derived from the transcription of the verbal responses, that are significantly better at distinguishing participants with MCI from those with intact cognition compared to a single score derived from conventional scoring. Our experimental analysis on verbal responses of 85 older adults (22 MCI and 63 health controls) demonstrates the effectiveness of proposed measures in the context of classifying participants with MCI from those with intact cognition. One notable insight from classification results is the positive contribution of baseline features into classification performance once added to the proposed measures. The computational approach used here could be applied to preclinical trials and also for screening large number of subjects, especially since speech can be easily collected remotely across large distances using mobile devices.

CONCLUSION

Despite our preliminary results that demonstrate the feasibility of automatic assessment for this task, there are a few limitations that we plan to address in our future works. Comparing the classification results, separately obtained from manual (ground truth) and automatically generated transcriptions, we found that the ASR derived features yielded somewhat less accurate classification metrics compared to that of manually-derived features. The accuracy of our ASR system is not optimal and important work remains to improve it. Additionally, the proposed computational analysis did not incorporate many other potentially useful features, relying entirely on correctness measures. Future research would explore the feasibility of other clinically relevant features, e.g. temporal measures that can be derived from the verbal responses as well as combining measures from other complementary cognitive tests or tasks. Moreover, discovering the relationship between Correctness measures and the neural basis of MCI is a valuable avenue for future research.

ACKNOWLEDGMENTS

FUNDING

This research was supported by NIH awards 5R21AG055749, P30-AG008017, P30-AG024978. U2C AG054397.

Fig. (1). System architecture for Correctness features.

Fig. (2). Sensitivity measures for three feature groups—conventional scoring features, correctness features computed with manual transcripts, and correctness features computed with automated (ASR) transcripts—with and without the demographic features age, sex, and years of education. A chance model is shown for comparison. Results shown are the mean sensitivity over 100 stratified repeats.

Fig. (3). Edit distance between verbal responses and test items in FDS and BDS tasks across test items.

Table 1. Baseline characteristics of participants (Mean (SD)).

Variable	Intact n=63	MCI n=22	
Age	86.7 (8.9)	91.7 (4.8)	
Years of Education	14.5 (2.5)	14.8 (3.0)	
Sex (% Female)	68%	50%	
MMSE	28.3 (1.6)	26.6 (2.1)	
DS Forward Score	6.7 (1.2)	6.5 (1.2)	
DS Backward Score	4.9 (1.2)	4.7 (1.1)	

Table 2. Example scoring and aggregation of correctness features (correct, edits, insertions, replacements, deletions) for a participant performing the Forward Digit Span task.

Len.	Prompt	Response	Cor.	Edits	Ins.	Rep.	Del.	
3	“375”	“375”	3	0	0	0	0	
3	“629”	“629”	3	0	0	0	0	
4	“5471”	“5471”	4	0	0	0	0	
4	“8396”	“8396”	4	0	0	0	0	
5	“36925”	“36925”	5	0	0	0	0	
5	“69471”	“69471”	5	0	0	0	0	
6	“635482”	“654382”	5	2	1	0	1	
6	“918427”	“948127”	4	2	0	2	0	
7	not attempted	not attempted	0	0	0	0	0	
7	not attempted	not attempted	0	0	0	0	0	
8	not attempted	not attempted	0	0	0	0	0	
8	not attempted	not attempted	0	0	0	0	0	
min			0	0	0	0	0	
max			5	2	1	2	1	
mean			2.75	0.33	0.08	0.17	0.08	
median			3.50	0.00	0.00	0.00	0.00	
std			2.05	0.75	0.28	0.55	0.28	

Table 3. Statistical analysis of the relationship between MCI and best proposed subject-level features as compared to conventional scoring and demographic features.

Category	Test	Feature	Intact mean (std.)	MCI mean (std.)	T-value	
Correctness	DS Backward	SD subject correct	1.7 (.3)	1.5 (.4)	−2.118 (p = .037)	
DS Backward	max subject deletions	.7 (.9)	1.2 (1.3)	2.016 (p = .047)	
DS Forward	max subject insertions	.6 (.8)	1.3 (2.4)	2.074 (p = .041)	
DS Forward	max subject edit distance	2.7 (1.8)	3.9 (3.0)	2.091 (p = .040)	
DS Forward	SD subject edit distance	.8 (.5)	1.1 (.8)	2.118 (p = .037)	
Conventional	DS Forward	Score	6.7 (1.2)	6.5 (1.2)	−.871 (p = .386)	
DS Backward	Score	4.9 (1.2)	4.7 (1.1)	−.736 (p = .464)	
Other		MMSE Score	28.3 (1.6)	26.6 (2.1)	−3.836 (p &lt; .001)	
	Age	86.7 (8.9)	91.7 (4.8)	2.501 (p = .014)	
	Male	32%	50%	1.535 (p = .129)	
	Years of Education	14.5 (2.5)	14.8 (3.0)	.378 (P = .706)	

Table 4. Classification results using automatically selected features (mean (SD) over 100 stratified repeats.

Features	Transcription	Sensitivity	Specificity	AUC-ROC	Accuracy	
Correctness, age, sex, education	Manual	.538 (.25)	.772 (.13)	.707 (.14)	.712 (.11)	
Correctness	Manual	.521 (.24)	.767 (.14)	.708 (.15)	.704 (.11)	
Correctness, age, sex, education	Automatic (ASR)	.487 (.24)	.725 (.13)	.651 (.15)	.664 (.10)	
Correctness	Automatic (ASR)	.461 (.25)	.720 (.15)	.615 (.15)	.653 (.12)	
Age, sex, education		.380 (.36)	.718 (.28)	.613 (.19)	.631 (.15)	
Conventional		.334 (.37)	.635 (.35)	.464 (.14)	.556 (.19)	
Conventional, age, sex, education		.332 (.32)	.738 (.24)	.573 (.18)	.633 (.13)	
Chance		.259 (.18)	.773 (.07)	.500 (.00)	.640 (.09)	

Table 5. Classification results using FDS and BDS automatically selected features (mean (SD) over 100 stratified repeats).

Task	Features	Sensitivity	Specificity	AUC-ROC	Accuracy	
FDS	Correctness, age, sex, education	.364 (.26)	.738 (.19)	.582 (.16)	.641 (.12)	
Correctness	.247 (.27)	.713 (.25)	.443 (.16)	.593 (.15)	
BDS	Correctness, age, sex, education	.438 (.25)	.741 (.14)	.639 (.14)	.663 (.11)	
Correctness	.399 (.27)	.697 (.17)	.570 (.15)	.619 (.12)	

ETHICS APPROVAL AND CONSENT TO PARTICIPATE

All human data included in this manuscript were obtained in compliance with regulations of the OHSU’s Institutional Review Board (IRB).

HUMAN AND ANIMAL RIGHTS

No animals were used in this research. All humans research procedures followed were in accordance with the standards set forth in the Declaration of Helsinki principles of 1975, as revised in 2008 (http://www.wma.net/en/20activities/10ethics/10helsinki/).

CONSENT FOR PUBLICATION

All subjects signed written informed consent.

DISCLAIMER: The above article has been published in Epub (ahead of print) on the basis of the materials provided by the author. The Editorial Department reserves the right to make minor modifications for further improvement of the manuscript.

AVAILABILITY OF DATA AND MATERIALS

Not applicable.

CONFLICT OF INTEREST

The authors declare no conflict of interest, financial or otherwise.


REFERENCES

[1] Asgari M , Kaye J , Dodge H . Predicting mild cognitive impairment from spontaneous spoken utterances. Alzheimer’s &amp; Dementia: Translational Research &amp; Clinical Interventions. Elsevier; 2017;3 (2 ):219–28.
[2] Lopez-de-Ipina K , Martinez-de-Lizarduy U , Calvo PM , Mekyska J , Beitia B , Barroso N , u. a. Advances on automatic speech analysis for early detection of Alzheimer disease: a non-linear multi-task approach. Current Alzheimer Research. Bentham Science Publishers; 2018;15 (2 ):139–48.
[3] Tóth L , Hoffmann I , Gosztolya G , Vincze V , Szatlóczki G , Bánréti Z , u. a. A speech recognition-based solution for the automatic detection of mild cognitive impairment from spontaneous speech. Current Alzheimer Research. Bentham Science Publishers; 2018; 15 (2 ): 130–8.
[4] Meilán JJ , Martínez-Sánchez F , Martínez-Nicolís I , Llorente TE , Carro J . Changes in the Rhythm of Speech Difference between People with Nondegenerative Mild Cognitive Impairment and with Preclinical Dementia. Behavioural Neurology. Hindawi; 2020;2020.
[5] Roark B , Mitchell M , Hosom J-P , Hollingshead K , Kaye J . Spoken language derived measures for detecting mild cognitive impairment. IEEE Transactions on Audio, Speech, and Language Processing. IEEE; 2011;19 (7 ):2081–90.
[6] Humpstone H Memory span tests. The psychological clinic. Biomedical Journal Digitization Project; 1919; 12 (5-9 ): 196.
[7] Nasreddine ZS , Phillips NA , Bédirian V , Charbonneau S , Whitehead V , Collin I , u. a. The Montreal Cognitive Assessment, MoCA: a brief screening tool for mild cognitive impairment. Journal of the American Geriatrics Society. Wiley Online Library; 2005;53 (4 ):695–9.
[8] Osmon DC , Smet IC , Winegarden B , Gandhavadi B . Neurobehavioral Cognitive Status Examination: Its use with unilateral stroke patients in a rehabilitation setting. Archives of Physical Medicine and Rehabilitation. Elsevier; 1992;73 (5 ):414–8.
[9] Tariq SH , Tumosa N , Chibnall JT , Perry MH III , Morley JE . Comparison of the Saint Louis University mental status examination and the mini-mental state examination for detecting dementia and mild neurocognitive disorder—a pilot study. The American journal of geriatric psychiatry. Elsevier; 2006;14 (11 ):900–10.
[10] Richardson JT . Measures of short-term memory: a historical review. Cortex. Elsevier; 2007;43 (5 ):635–50.
[11] Banken JA . Clinical utility of considering Digits Forward and Digits Backward as separate components of the wechsler adult intelligence Scale-Revised. Journal of Clinical Psychology. Wiley Online Library; 1985;41 (5 ):686–91.
[12] Carlesimo GA , Fadda L , Lorusso S , Caltagirone C . Verbal and spatial memory spans in Alzheimer’s and multi-infarct dementia. Acta Neurologica Scandinavica. Wiley Online Library; 1994;89 (2 ): 132–8.
[13] Morris JC , Ernesto C , Schafer K , Coats M , Leon S , Sano M , u. a. Clinical Dementia Rating training and reliability in multicenter studies The Alzheimer’s Disease Cooperative Study experience. Neurology. AAN Enterprises; 1997;48 (6 ):1508–10.
[14] Kiernan RJ , Mueller J , Langston JW , Van Dyke C . The Neurobehavioral Cognitive Status Examination: A brief but differentiated approach to cognitive assessment. Annals of internal medicine. Am Coll Physicians; 1987;107 (4 ):481–5.
[15] Morris JC . The Clinical Dementia Rating (CDR): current version and scoring rules. Neurology. Lippincott Williams &amp; Wilkins; 1993;
[16] Cockrell JR , Folstein MF . Mini-mental state examination. Principles and practice of geriatric psychiatry. Citeseer; 2002;140–1.
[17] Smola AJ , Schölkopf B . A tutorial on support vector regression. Statistics and computing. Springer; 2004;14 (3 ):199–222.
[18] Pedregosa F , Varoquaux G , Gramfort A , Michel V , Thirion B , Grisel O , u. a. Scikit-learn: Machine learning in Python. The Journal of Machine Learning Research. JMLR. org; 2011;12 :2825–30.
[19] Schölkopf B , Smola AJ . Learning with kernels: support vector machines, regularization, optimization, and beyond. MIT press; 2002.
[20] Hastie T , Tibshirani R , Friedman J , Franklin J . The elements of statistical learning: data mining, inference and prediction. The Mathematical Intelligencer. Springer; 2005;27 (2 ):83–5.
[21] Hanley JA , McNeil BJ . The meaning and use of the area under a receiver operating characteristic (ROC) curve. Radiology. 1982;143 (1 ):29–36.7063747
[22] Povey D , Ghoshal A , Boulianne G , Burget L , Glembek O , Goel N , u. a. The Kaldi speech recognition toolkit. IEEE 2011 workshop on automatic speech recognition and understanding. IEEE Signal Processing Society; 2011.
[23] Vipperla R , Renals S , Frankel J . Ageing voices: The effect of changes in voice parameters on ASR performance. EURASIP Journal on Audio, Speech, and Music Processing. Springer; 2010;2010 (1 ):525783.
