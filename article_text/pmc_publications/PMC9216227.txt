LICENSE: This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.


0012737
4157
IEEE Trans Biomed Eng
IEEE Trans Biomed Eng
IEEE transactions on bio-medical engineering
0018-9294
1558-2531

32340935
9216227
10.1109/TBME.2020.2990734
NIHMS1811798
Article
Deep convolutional neural networks and transfer learning for measuring cognitive impairment using eye-tracking in a distributed tablet-based environment
Haque Rafi U. *Department of Neurology, Emory School of Medicine

Pongos Alvince L. *Department of Neurology, Emory School of Medicine

Manzanares Cecelia M. Department of Neurology, Emory School of Medicine

Lah James J. Department of Neurology, Emory School of Medicine

Levey Allan I. Department of Neurology, Emory School of Medicine

Clifford Gari D. Department of Biomedical Informatics, Emory, School of Medicine and the Department of Biomedical Engineering, Emory, University and Georgia Institute of Technology

* Contributed Equally

10 6 2022
1 2021
21 12 2020
22 6 2022
68 1 1118
This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.
Objective:

Alzheimer’s disease (AD) is a neurodegenerative disorder that initially presents with memory loss in the presence of underlying neurofibrillary tangle and amyloid plaque pathology. Mild cognitive impairment is the initial symptomatic stage, which is an early window for detecting cognitive impairment prior to progressive decline and dementia. We recently developed the Visuospatial Memory Eye-Tracking Test (VisMET), a passive task capable of classifying cognitive impairment in AD in under five minutes. Here we describe the development of a mobile version of VisMET to enable efficient and widespread administration of the task.

Methods:

We delivered VisMET on iPad devices and used a transfer learning approach to train a deep neural network to track eye gaze. Eye movements were used to extract memory features to assess cognitive status in a population of 250 individuals.

Results:

Mild to severe cognitive impairment was identifiable with a test accuracy of 70%. By enforcing a minimal eye tracking calibration error of 2cm, we achieved an accuracy of 76% which is equivalent to the accuracy obtained using commercial hardware for eye-tracking.

Conclusion:

This work demonstrates a mobile version of VisMET capable of estimating the presence of cognitive impairment.

Significance:

Given the ubiquity of tablet devices, our approach has the potential to scale globally.

Alzheimer’s Disease
Convolutional Neural Networks
Eye Tracking
Memory
Mild Cognitive Impairment

pmcI. Introduction

Mild Cognitive Impairment (MCI) refers to a phase of Alzheimer’s disease (AD) characterized by mild cognitive changes in the presence of underlying amyloid plaque and neurofibrillary tangle pathology. An important clinical criteria for the diagnosis of MCI is objective cognitive impairment, which is primarily detected using general cognitive screenings such as the Montreal Cognitive Assessment and Mini-Mental Status Exam [1], [2]. Although successful in detecting cognitive impairment in AD, these screening methods have limitations that prevent more widespread use. For example, these tasks are generally administered with pencil and paper or computer adaptations, requiring 10–15 minutes to administer and a number of trained personnel. Individuals are often intimidated by perceived impairment on these tests, at times to a point of discontinuation [3], [4]. As a result, large-scale screening for memory loss remains challenging.

To overcome these limitations, we developed the Visuospatial Memory Eyetracking Test (VisMET), an eye-movement based memory paradigm that provides a passive, efficient, and sensitive assessment of cognitive impairment in Alzheimer’s disease [5]. During this task, participants are presented with a set of images followed by the same set of images with an object removed or added (Fig. 1). The amount of time viewing these manipulations compared to unchanged parts of the images has been used to measure memory of either a previously viewed object and location (removed condition), or a new object and location (added condition) [5]–[8]. Participants with cognitive impairment can be differentiated from healthy controls by measuring the amount of time viewing the manipulated images after a delay. The current administration of VisMET requires the use of an external eye tracker, which is often expensive and widely unavailable. To increase the availability of VisMET, the aim of this study was to develop a tablet-based version of VisMET capable of detecting cognitive impairment.

We developed an iPad based version of VisMET that utilizes iTracker, a convolutional neural network (CNN) architecture used to track eye movements on Apple devices [9]. We trained this architecture using a public data set of over 1.5 million images of human faces unrelated to our study population [9], and then updated the network using a transfer learning paradigm using approximately 90,000 images collected from our research and clinical populations. We administered a four-minute memory test to 250 healthy controls (MoCA &gt; 24) and cognitively-impaired participants (MoCA ≤ 24) recruited from the Emory Healthy Brain Study (EHBS) and the Goizueta Alzheimers Disease Research Center (ADRC) at Emory.

We compared memory performance between these two populations and identified whether performance could be used as a screening tool for identifying cognitive impairment.

II. Materials and Methods

A. Participants

A total of 552 participants were recruited from the EHBS and ADRC and were administered the VisMET using either the tablet or the EyeTribe, a commercial physical eye-tracking device (Table I). Participants who were administered VisMET on the EyeTribe were a different group than those administered VisMET on the tablet.

All participants received detailed evaluations that included neuropsychological testing, although the specific batteries varied. The Montreal Cognitive Assessment (MoCA) was used as a common test to evaluate cognitive performance across the EHBS and ADRC populations. A MoCA greater than 24 was considered normal, and MoCA less than or equal to 24 was indicative of cognitive impairment [10]–[12]. The research participants from the EHBS were further classified as healthy controls, MCI, or Alzheimer’s Disease by an interdisciplinary team of research neurologists, geriatricians, and neuropsychologists, and clinical participants from the ADRC received a clinical diagnosis by a sub-specialty trained cognitive neurologist. Participants recruited from the EHBS primarily consisted of healthy controls whereas participants recruited from the ADRC consisted of participants at different stages of mild cognitive impairment and AD.

All procedures followed were in accordance with the ethical standards of the responsible committee on human experimentation.

B. Mobile Device Data Capture

VisMET [5] was presented to participants using iPad Air 9.7” tablets with a viewable screen area of 154 × 203 mm and a resolution of 1536 × 2048 pixels. Each iPad was running at least iOS 10, capturing a series of frames from the ‘selfie’ camera at a resolution of 720p and sampling rate of 30 Hz. The selfie camera has an aperture of f/2.2 and a focal length of 31 mm. The mobile VisMET application was programmed to maximize the iPad’s screen brightness during the entirety of the task. Data collection happened in clinical testing rooms that had both natural lighting from windows and overhead fluorescent or LED bulbs.

C. Calibration Procedure

Each participant was seated in front of an iPad that was attached to a stand in portrait orientation. A silhouette of a face appeared on the iPad screen to center the participant’s face, resulting in an approximate distance of 350 mm between the iPad and the participant’s eyes.

The calibration procedure sequentially presented 16 random numbers and letters, 35 × 35 pixels in size, at random locations on the screen. Participants were instructed to say the letters and numbers aloud to increase their attention and assess their understanding. Characters were presented for 1.3 seconds without delay, long enough for participants to find and say the character aloud. The locations of the characters served as labels to train the CNN and support vector regression (SVR) models for gaze estimation (Fig. 1). We removed frames collected during the first 250 ms of each character to reduce periods during which participants were transitioning from one character to the next. We also removed frames collected from the first four calibration dots to accommodate learning of the calibration procedure.

D. Visuospatial Memory Eye-tracking Task (VisMET)

VisMET was administered to assess memory passively using eye movements rather than explicit memory judgments. In previous work, VisMET was correlated with several cognitive domain assessments and correlated strongest with memory [5]. As MoCA was delivered to both populations within our clinic and research sites, we used MoCA as our standard of cognitive performance for this paper.

The only instruction given to participants was to enjoy the images. The task begins with the presentation of 20 images of scenes for a duration of five seconds, followed by a slightly modified set of images with either one object added (six trials) or removed (14 trials) from each image (Fig. 1). The delay between the first and second presentation of each image was approximately one minute. The images were presented on the top half of the iPad screen since eye-tracking accuracy has been shown to decrease as a function of distance from the iPad camera [9]. The images were selected to contain 2–5 objects based on visual inspection. This selection was performed prior to the collection of eye-tracking and provided a method for filtering out images that would likely lead to a diffuse pattern in viewing.

E. Regression Tree Face and Eye Detection

The CNN requires a face crop, left eye crop, right eye crop, and face grid vector to estimate eye gaze locations. In previous work, crops were detected using built-in face and eye detection libraries on Apple mobile devices but led to a significant amount of data loss (39%). To prevent such data loss, we calculated bounding boxes for crops using the dlib library [13] through OpenCV’s Python interface [14], an implementation that uses an ensemble of regression trees for facial feature alignment [15]. If three or more consecutive frames crossed an eye-aspect ratio of 0.1, the frames were omitted [16]. Frames without detected faces or eyes were also removed from future analysis. The mean percentage of valid frames per subject was 93%.

F. Convolutional Neural Network Training Datasets

We used two separate datasets for training the convolutional neural network: MIT’s GazeCapture and Emory’s ADRC set. MIT’s dataset consisted of 1450 people and approximately 1.5 million frames. To improve the accuracy of gaze estimates, significant variability was introduced when creating MIT’s data-set. By crowdsourcing administration, participants were administered the task in a wide variety of settings and lighting conditions with 75% of the participants collected through Amazon Mechanical Turk, 16% were collected from UGA’s campus, and the 9% collected using app store downloads. Participants were also asked to move continuously throughout the task and encouraged to change the orientation of the device to further increase the variability in the dataset [9].

Emory’s dataset consisted of 250 people and approximately 90,000 frames (Table I). Key differences exist between MIT’s and Emory’s datasets: (1) MIT’s was comprised of camera frames collected from both Apple mobile phone and tablet devices. (2) Face and eye crops for the MIT set were generated from Apple’s built-in face and eye detection libraries rather than the offline OpenCV python interface described above. (3) The labels of the MIT set were the location of characters using a different calibration procedure [9].

G. Convolutional Neural Network and Support Vector Regression for Tablet-Based Gaze Estimation

Method 1:

Method 1 involved training a CNN with 90,000 frames collected during the calibration phase from a total of 250 Emory participants using a five-fold cross validation procedure, where the set of images used to train the CNN is different than the set of images used to extract participant memory performance. For each fold, we randomly divided the participants into train, validation, and test splits consisting of 125, 62, and 63 participants, respectively. The features for the CNN consisted of a face crop, left eye crop, right eye crop, and face grid vector extracted from each frame (Fig. 1). The labels of the network were the locations of the character during the calibration procedure. The CNN model was implemented in PyTorch and trained for 35 epochs with a batch size of 16, a weight decay of 0.0001, and momentum of 0.9. A global learning rate of 0.0001 was used and decayed by a factor of 10 every five epochs. The model with the lowest validation error was used to generate gaze estimations. These gaze estimations were used to evaluate the average Euclidean distance to the target character for the train, validation, and test participants.

Method 2:

Method 2 involved training the network using approximately 1.5 million frames from the GazeCapture dataset [9]. This dataset consisted of 1450 participants with an 80%, 10%, 10% train, validation, and test split. For Method 2, the CNN was first trained with the inputs and labels from the MIT dataset using the same hyperparameters and training procedures as Method 1. This pre-trained model was then retrained using inputs and labels from the Emory dataset as described in Method 1.

Method 3:

Method 3 expanded on Method 1 by adding a support vector regression (SVR) layer for each participant separately. The features for the SVR were the gaze estimations from the final layer of the CNN from Method 1 while the labels were the location of the characters. These features and labels were separated into a train, validation, and test split of 50%, 25%, and 25%. For each participant, we used the estimates from the test set to calculate the average Euclidean distance to the target character. We report this average error of the test set (Table II) for the train, validation, and test participants from Method 1.

Method 4:

Rather than using the CNN described in Method 1 to generate features for the SVR, Method 4 used gaze estimates from the final layer of the CNN from Method 2 to generate features.

H. Adjustment of Gaze Estimations Between Successive Images

Between each set of images, a red fixation dot was presented in the center of the viewing frame to reset a participant’s gaze to the center. For each fixation dot, we calculated the median vertical and horizontal distance between the location of the fixation dot and the gaze estimations generated from the SVR. We shifted the estimations from the SVR for a particular image by the vertical and horizontal distances calculated from the preceding fixation dot. We performed these adjustments to account for any translations in gaze estimations throughout the task.

I. EyeTribe-Based Gaze Estimation

Using an EyeTribe Infrared Scanner (Copenhagen, Denmark), participant gaze locations were estimated at a sampling rate of 30 Hz. This eye-tracking scanner uses a linear array of infrared LEDs to illuminate the eye and allow for the capture of pupil and corneal reflection. The rotation of the eye was determined by the relative positions of the corneal and pupillary reflections. The scanner was attached to the bottom of a computer monitor that was mounted to the wall using an adjustable arm to accommodate participants of different heights. At the start of each session, participants completed the calibration procedure to convert eye rotations into a set of gaze positions relative to the screen. More information can be found in [17].

J. Feature Extraction

After gaze positions were estimated for each subject using the methods described above, the gaze estimations were converted into a set of fixations using a dispersion-based algorithm [18]. Each fixation was defined as a point of gaze continually remaining on the screen within 2 degrees of visual angle for a period of 100 ms or more. Data were analyzed off-line using custom scripts written in MATLAB and Python.

Measurement of Visual Exploration:

We developed methods to quantify visual exploration for each participant by measuring the viewing of the unmanipulated object during the first presentation of the image (Fig. 2A). To quantify viewing of unmanipulated objects during the learning phase, we identified the location of each object to be removed (”critical region”) by drawing an ellipse around the object (defined by the x,y coordinates of the center, major, and minor axis). The number of fixations within the critical region and the percentage of time viewing the critical region was calculated for each original image presentation. The percentage of time spent in the critical region was averaged across all original image presentations for each subject (Metric 1). We also calculated the percentage of all original image presentations with at least one fixation in the critical region for each subject (Metric 2).

Measurement of Memory Performance:

We developed a similar approach to quantify viewing of the region containing the manipulated objects as a measure of memory [5]. An ellipse was drawn outlining the location of the removed or added object. The number of fixations and the percentage of viewing time within the critical region was calculated for each manipulated image presentation. For each participant, we calculated the average percentage of time spent in the critical region across all manipulated images (Metric 1). We also calculated the percentage of manipulated images with at least one fixation in the critical region for each participant (Metric 2).

K. Regression Models for Detection of Cognitive Impairment

We quantified whether memory performance on the task could serve as a screening tool for cognitive impairment. The test participants from Methods 3 and 4 were further divided into a train and testing set using an 80/20 split. Logistic regression classifiers were trained using either age (as a baseline), the percentage of removed objects viewed, or viewing time of the added object. The output of the model estimated the performance on a standard measure of cognitive impairment (MoCA ≤ 24 or MoCA &gt; 24). A logistic regression classifier was also trained to differentiate controls from MCI/AD for a subset of participants for which consensus diagnosis was available (Table III). Using a five-fold cross validation procedure, the performance of the model was assessed using the the area under the receiver operating characteristic curve (AUROC).

III. Results

A. Performance Evaluation of Tablet-Based Methods for Gaze Estimation

Table II shows the average distance between the gaze estimate and the target character for each of our tablet-based methods. Implementing SVR (Methods 3 and 4) reduced testing error substantially compared to methods using the CNN alone (Methods 1 and 2). We also observed a significant improvement when training the network with both datasets rather than the Emory dataset alone (Method 2 vs 1 and Method 4 vs 3). The method implementing both of these approaches (Method 4) outperformed all other methods with a testing error of 2.72 cm, which was comparable to the 2.58 cm and 2.12 cm errors from previous studies [9]. These results replicated the feasibility of performing eye-tracking on the tablet, showing improved performance with support vector regression, and extended previous studies by implementing a transfer learning approach for gaze estimation.

B. Assessment of Visual Exploration on the Tablet and EyeTribe

Figure 2 and Table IV show substantial differences in viewing between the EyeTribe and tablet. Tablet-based methods 3 and 4 produced estimates that reduced viewing time in the critical region by 26% and 18% compared to the EyeTribe, respectively (p &lt; 10−48, unpaired t-test; p &lt; 10−29, unpaired t-test). The percentage of later removed objects viewed was also significantly reduced by 27% and 15% for tablet-based methods 3 and 4 compared to the EyeTribe (p &lt; 10−53, unpaired t-test; p &lt; 10−23, unpaired t-test, respectively).

We hypothesized the differences in viewing estimates between the tablet and EyeTribe were primarily driven by the accuracy of the gaze estimates. In support of this hypothesis, we observed that the differences in viewing time and viewed objects between the platforms were reduced to 7% and 11% after removing participants with a calibration error greater than 2cm. We also found tablet-based method 4 produced viewing estimates more comparable to the EyeTribe than tablet-based method 3 (Fig. 2), most likely due to its higher accuracy (Table II). From these observations, we conclude that the differences in viewing between the platforms are primarily driven by the accuracy of gaze estimates. These results also suggest that the calibration errors in Table II extend beyond calibration and generalize to the task.

Figure 2 and Table IV also compares viewing of the critical region during the initial presentation of images for healthy controls and cognitively-impaired participants. The differences in viewing between the populations were first evaluated using the EyeTribe, a physical eye-tracker. Healthy controls viewed 93% of the later removed objects, slightly more than the 86% viewed by cognitively impaired participants (p &lt; 10−5, unpaired t-test; Fig. 2C, Table IV). We also found significant differences between the populations when using the tablet to generate gaze estimations (p &lt; 0.05, unpaired t-test; Fig. 2C). We also compared the average viewing time of the later removed objects between healthy controls and cognitively-impaired patients (Fig. 2B, Table IV). Both EyeTribe and tablet-based methods produced differences in viewing time between healthy controls and cognitively-impaired participants (p &lt; 0.05, unpaired t-test; Fig. 2B). From these results, we conclude that initial viewing of later removed objects was different between the populations on either platform.

C. Assessment of Cognitive Impairment on the Tablet and EyeTribe

Figure 3 and Table V show the differences in viewing of the manipulated regions between cognitively-impaired participants and healthy controls. Using the EyeTribe, control participants viewed 54% of the removed objects, nearly twice as many viewed by cognitively impaired participants (p &lt; 10−14, unpaired t-test). Control participants also viewed more of the critical regions compared to cognitively-impaired participants when using tablet-based methods 3 and 4 (p &lt; 10−6, unpaired t-test; p &lt; 10−8, unpaired t-test; Fig. 3C, Table V). We also examined the % viewing time within the critical region for the added and removed conditions (Fig. 3B, Table V). Tablet-based method 3, method 4, and the EyeTribe showed increases in viewing time of the critical region when comparing to healthy controls to cognitive impaired participants, respectively (p &lt; 10−3, p &lt; 10−6, p &lt; 10−9, unpaired t-test; Table V). We found similar relative increases in viewing time of 28% and 29% for the two tablet-based methods when comparing the populations for the added condition (Cohen’s d =−.48, p &lt; 10−3, unpaired t-test; Cohen’s d = −0.54, p &lt; 10−4, unpaired t-test).

When using only participant data with a calibration error less than 2cm, healthy controls viewed nearly twice as many of the removed objects as cognitively impaired participants, comparable to differences produced by the EyeTribe. We also compared average viewing time between control and cognitively impaired participants on both platforms. Both the tablet and EyeTribe showed significant increases in viewing time for healthy controls compared to cognitively impaired participants (p &lt; 10−4, unpaired t-test; p &lt; 10−7, unpaired t-test; p &lt; 10−10, unpaired t-test). In summary, we find both platforms are capable of distinguishing control and cognitively impaired participants and produce comparable viewing of the manipulated regions, especially when the calibration errors are low.

The differences in cognitively impaired participants and controls during the recognition phase could be purely driven by fewer fixations, especially considering the lower viewing time for objects during the learning phase. We found a small but reliable decrease in the average number of fixations for cognitively impaired participants (11.0 ± 0.2) compared to healthy controls (11.8 ± 0.3) (Cohen’s d = −0.31, p &lt; 0.05, unpaired t-test). To ensure these viewing differences were not the primary driver of the differences between the two populations, we first calculated the correlation between MoCA and VisMET performance and found this correlation was 0.32 ± .01 across our testing folds. Then, we performed the same correlation partialling out the average number of fixations made by the participants during the recognition phase and found the correlation was 0.29 ± 01. These results indicate that the differences between the two groups during the recognition phase are largely driven by the memory of the manipulated objects.

D. Tablet Administration of VisMET as a Screening Tool for Cognitive Impairment

Viewing of the manipulated regions showed robust population differences between cognitively-impaired participants and healthy controls using the tablet. Based on these results, we hypothesized that viewing of the manipulated region containing the removed object could be used as a screening tool for cognitive impairment. Figure 4 shows the extent to which viewing of the manipulated region can separate healthy controls from cognitively-impaired participants. When using the EyeTribe gaze tracking hardware, viewing of the manipulated region could estimate cognitive impairment (MoCA ≤ 24) with an AUC of 0.76 compared to an AUC of 0.66 and 0.70 when using tablet-based methods 3 and 4, respectively (Fig. 4A). For the added condition, viewing time within the manipulated region could estimate cognitive impairment with an AUC of 0.64. No additional increases in performance were observed when using the added and removed condition together (AUC = 0.70). Viewing of the manipulated region could differentiate controls from MCI/AD with an AUC of 0.66 and 0.69 using tablet-based methods 3 and 4. Hence, the accuracy of classification of diagnosis was comparable to accuracy of estimating cognitive impairment.

Furthermore, we compared performance to age, a well-established risk factor for cognitive impairment. Using age as an estimate of cognitive impairment resulted in AUCs of 0.68 and 0.63 for EyeTribe and tablet-based administrations, respectively. The lower AUC with the tablet-based administration was likely driven by the larger variance and lower correlations with MoCA as described in Table VI. Overall these results suggest that VisMET outperforms age as an estimator of cognitive impairment.

Finally, we quantified testing AUC for tablet-based method 4 after removing participants that did not reach a specific calibration error threshold (Fig. 4B). If we constrained our analyses to only include participants whose calibration error was less than or equal to 2 cm, we were able to achieve a testing AUC of 0.76 for the mobile application. These results demonstrate that the mobile version of VisMET can provide equivalent accuracy of cognitive impairment estimates compared to a commercial eye-tracker.

IV. Conclusion

Currently, a key clinical criteria for the diagnosis of MCI is objective cognitive impairment, primarily detected using general cognitive screenings such as the Montreal Cognitive Assessment and Mini-Mental Status Exam. These conventional tests typically require trained personnel, a considerable amount of time to administer, and are often underused in the clinic as individuals are intimidated by perceived poor performance on such tests. To address these concerns, the novel contribution of this work was the development of a mobile version of VisMET that performs eye-tracking using the standard front-facing camera of an iPad by leveraging a deep convolutional neural network together with a transfer learning approach. Experimental validation using 250 participants from the ADRC at Emory confirms the ability to estimate cognitive impairment within a clinical setting with an AUC of 0.76 for participant’s with a calibration error less than 2cm, equivalent to the accuracy of physical eye-trackers. This study provides an easily administered, sensitive paradigm that can track cognitive impairment at multiple clinical centers.

The primary measure of cognitive impairment in this study was the amount of time participants spent viewing the manipulated region which has previously shown to correlate most with memory performance compared to performance on other cognitive domains [5]. Memory formation has been shown to rely on the entorhinal-hippocampal circuit in the brain, the initial site of cortical pathology in AD. For this reason, memory performance may precede measures of cognitive impairment and predict cognitive decline in AD earlier than the MoCA. Certain images in the task may be more or less successful in estimating cognitive impairment than others, and also show variability in identifying cognitive impairment during different stages of AD. Detailed investigation of this hypothesis remains a future direction of this work.

The purpose of this study was to validate a mobile version of VisMET that could be used to standardize assessment of cognitive performance at multiple clinical centers under the testing conditions developed in this study. However, generalization of the trained CNN beyond the conditions in this study cannot be assured. In particular, if the iPad is handheld, lighting conditions are different, or if more than one face is on the camera, we expect a drop in performance. A more challenging test of the developed algorithm would be to deliver the paradigm remotely. This is likely to require the retraining of the CNN using frames collected from a wide variety of real-world settings, with a range of lighting and background motion changes. If accurate memory performance can be obtained in such a way, we could potentially use this approach with a cloud-based pipeline for widespread screening of cognitive impairment. Moreover, the ease of administration would be conducive for longitudinal assessments to potentially track cognitive impairment over time.

V. Acknowledgments

We thank the Goizueta ADRC study team and the Emory Healthy Aging Study team for assistance with data collection. We thank Kareem Zaghloul, Annabelle Singer, and Joseph Manns for helpful discussions. This research was supported by funding from the Goizueta Foundation and the Goizueta Alzheimer Disease Research Center at Emory University (P50 AG025688). We gratefully acknowledge the support of the Emory Healthy Aging Study Investigators (healthyaging.emory.edu/team/researchers/).

This research was supported by funding from the Goizueta Foundation and the Goizueta Alzheimer Disease Research Center at Emory University (P50 AG025688).

Fig. 1: Distributed tablet-based environment for measuring memory performance.

Participants were presented with a calibration procedure followed by VisMET on iPad devices. The task consisted of the presentation of a set of images followed by the same set of images with an object added or removed. While the task was running, the mobile application sampled images of a participant’s face at 30 Hz using the built-in ‘selfie’ camera. Upon task completion, the images were uploaded to the cloud for face and eye detection, and then used to train a CNN to estimate gaze. For each subject, a support vector regression (SVR) was trained using the gaze estimations from the CNN as input. Gaze estimations from the SVR were then used to extract memory features.

Fig. 2: Visual exploration of later removed objects using a tablet or EyeTribe for gaze estimation.

(A) Participants viewed images containing an object that was removed later as indicated by the white critical region (invisible to the viewer). The red line illustrates gaze path estimations using the EyeTribe or tablet. (B) Healthy controls (MoCA &gt; 24) and cognitively impaired participants (MoCA ≤ 24) spent comparable amounts of time in the critical region across all platforms (6–8% relative change). Tablet based-methods 3 and 4 produced estimates that reduced viewing time in the critical region by 26% and 18% compared to the EyeTribe. (C) Healthy controls viewed similar number of critical regions compared to cognitively-impaired participants across all platforms (8% relative change). Tablet based-methods 3 and 4 produced estimates that reduced the percentage of critical regions viewed by 27% and 15% compared to the EyeTribe.

Fig. 3: Memory of removed objects using a tablet or EyeTribe for gaze estimation.

(A) Participants viewed images where an object was removed as indicated by the white critical region (invisible to the viewer). The red line illustrates gaze path estimations for EyeTribe and tablet-based methods. (B) Healthy controls (MoCA &gt; 24) spent a greater percentage of their time in the critical regions compared to cognitively impaired participants (MoCA ≤ 24) across platforms. (C) When using the EyeTribe, control participants (MoCA &gt; 24) viewed twice as many of the removed objects compared to cognitively impaired participants. Tablet-based methods 3 and 4 produced relative increases of 24% and 29% when comparing healthy controls to cognitively impaired participants.

Fig. 4: VisMET estimates cognitive impairment on the tablet and EyeTribe.

(A) Viewing of the removed objects could accurately estimate performance on the Montreal Cognitive Assessment (MoCA ≤ 24 and MoCA &gt; 24), a standard measure of cognitive impairment. Tablet-based method 3, tablet-based method 4, and the EyeTribe estimated MoCA performance with an AUC of 0.66, 0.70, and 0.76, respectively. (B) Participants with a calibration error less than 2 centimeters achieved an AUC of 0.76 equivalent to the performance achieved using the EyeTribe. (C) Average ROC Curve across testing folds of estimating cognitive impairment in participants with a calibration error less than 2 cm.

TABLE I: Demographics of Emory Participants

	Tablet	EyeTribe	
	
Number	250	302	
Age	72.3 ± 9.1	64.0 ± 8.1	
Gender(M/F)	111/140	115/187	
Race(C/AA/As/Oth)	199/41/8/2	274/20/4/4	
Education	16.4 ± 2.3	16.2 ± 2.5	
MoCA	20.0 ± 7.1	25.0 ± 4.8	
EHBS/ADRC	83/167	240/62	
C = Caucasian, AA=African American, As = Asian, Oth = Other

TABLE II: Performance of Tablet-Based Methods for Gaze Estimation

Method	Model	Dataset	Train Error (cm)	Valid Error (cm)	Test Error (cm)	
	
1	CNN	Emory	4.90 ± 0.18	4.83 ± 0.26	4.90 ± 0.15	
2	CNN	MIT+Emory	3.87 ± 0.20	3.75 ± 0.16	3.89 ± 0.30	
3	CNN + SVR	Emory	3.20 ± 0.06	3.20 ± 0.08	3.30 ± 0.06	
4	CNN + SVR	MIT+Emory	2.71± 0.12	2.68 ± 0.09	2.72 ± 0.15	

TABLE III: Demographics of Participants

	Control	MCI	AD	
	
Number	57	46	56	
Age	70.8 ± 8.4	73.3 ± 7.6	74 ± 9.5	

TABLE IV: Effect Size between Control and Cognitively-Impaired Populations during First Viewing

	Method	Con	CogImp	PSD	d	
Viewing Time	Tablet3	0.20	0.18	0.06	−0.17	
Tablet4	0.22	0.20	.054	−0.30	
EyeTribe	0.27	0.25	.04	−0.38	
Critical Regions Viewed	Tablet3	0.67	0.61	0.23	−0.24	
Tablet4	0.80	0.72	0.21	−0.33	
EyeTribe	0.93	0.86	0.12	−0.59	
Con - Mean for Control Group, CogImp - Mean for Cognitively Impaired Group, PSD - Pooled Standard Deviation, d - Cohen’s d

TABLE V: Effect Size between Control and Cognitively-Impaired Populations during Second Viewing

	Method	Con	CogImp	PSD	d	
Viewing Time	Tablet3	0.09	0.07	0.04	−0.49	
Tablet4	0.09	0.06	0.04	−0.64	
EyeTribe	0.12	0.06	0.08	−0.73	
Critical Regions Viewed	Tablet3	0.37	0.26	0.16	−0.67	
Tablet4	0.42	0.28	0.17	−0.79	
EyeTribe	0.54	0.30	0.26	−0.90	
Con - Mean for Control Group, CogImp - Mean for Cognitively Impaired Group, PSD - Pooled Standard Deviation, d - Cohen’s d

TABLE VI: MoCA Correlations with Age

	Tablet	EyeTribe	P-value	
	
MoCA vs. Age (Healthy)	−.04	−.25*	0.11	
MoCA vs. Age (Cog Imp)	−.08	−.45*	0.002	
MoCA vs. Age (ADRC)	−.24*	−.33*	0.52	
MoCA vs. Age (EHBS)	−.16*	−.22*	0.63	
MoCA vs. Age (Overall)	−.20*	−.45*	0.001	
Table showing correlations of MoCA and Age by subgroup. Asterisk indicates whether correlation was significant at p &lt; .05. Comparison of correlation coefficients are provided in the third column and assessed using Fisher’s r to z transformation.


References

[1] Nasreddine ZS , Phillips NA , Bédirian V , “The Montreal Cognitive Assessment, MoCA: A Brief Screening Tool For Mild Cognitive Impairment,” Journal of the American Geriatrics Society, vol. 53 , no. 4 , pp. 695–699, Apr 2005.15817019
[2] Mitchell AJ , “A meta-analysis of the accuracy of the mini-mental state examination in the detection of dementia and mild cognitive impairment,” Journal of Psychiatric Research, vol. 43 , no. 4 , pp. 411–431, Jan 2009.18579155
[3] Whitehead JC , Gambino SA , Richter JD , “Focus group reflections on the current and future state of cognitive assessment tools in geriatric health care,” Neuropsychiatric Disease and Treatment, vol. 11 , pp. 1445–1466, 2015.
[4] Taber JM , Leyva B , and Persoskie A , “Why do People Avoid Medical Care? A Qualitative Study Using National Data,” Journal of General Internal Medicine, vol. 30 , no. 3 , pp. 290–297, Mar 2015.25387439
[5] Haque RU , Manzanares CM , Brown LN , “VisMET: a passive, efficient, and sensitive assessment of visuospatial memory in healthy aging, mild cognitive impairment, and Alzheimer’s disease.” Learning &amp; Memory (Cold Spring Harbor, N.Y.), vol. 26 , no. 3 , pp. 93–100, Mar 2019.
[6] Ryan JD , Althoff RR , Whitlow S , “Amnesia is a deficit in relational memory,” Psychological Science, vol. 11 , no. 6 , pp. 454–461, 2000.11202489
[7] Smith CN , Hopkins RO , and Squire LR , “Experience-Dependent Eye Movements, Awareness, and Hippocampus-Dependent Memory,” Journal of Neuroscience, vol. 26 , no. 44 , pp. 11304–11312, 2006.17079658
[8] Smith CN and Squire LR , “Experience-Dependent Eye Movements Reflect Hippocampus-Dependent (Aware) Memory,” Journal of Neuroscience, vol. 28 , no. 48 , pp. 12825–12833, 2008.19036976
[9] Krafka K , Khosla A , Kellnhofer P ., “Eye Tracking for Everyone,” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 2176–2184, doi: 10.1109/CVPR.2016.239
[10] Borland E , Nägga K , Nilsson PM , “The Montreal Cognitive Assessment: Normative Data from a Large Swedish Population-Based Cohort,” Journal of Alzheimer’s Disease, 2017.
[11] Carson N , Leach L , and Murphy KJ , “A re-examination of Montreal Cognitive Assessment (MoCA) cutoff scores,” International Journal of Geriatric Psychiatry, 2018.
[12] Petersen RC , Aisen PS , Beckett LA , “Alzheimer’s Disease Neuroimaging Initiative (ADNI): Clinical characterization,” Neurology, 2010.
[13] King DE , “Dlib-ml: A Machine Learning Toolkit,” Tech. Rep, 2009.
[14] Bradski G , “The OpenCV Library,” Dr. Dobb’s Journal of Software Tools, 2000.
[15] Kazemi V and Sullivan J , “One millisecond face alignment with an ensemble of regression trees,” in Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2014, pp. 1867–1874.
[16] Soukupová T and Čech J , “Real-Time Eye Blink Detection using Facial Landmarks,” Tech. Rep, 2016.
[17] Ooms K , Dupont L , Lapon L , “Accuracy and precision of fixation locations recorded with the low-cost Eye Tribe tracker in different experimental set- ups,” Journal of Eye Movement Research, vol. 8 , no. 1 , pp. 1–24, 2015.
[18] Salvucci DD and Goldberg JH , “Identifying fixations and saccades in eye-tracking protocols,” in Proceedings of the symposium on Eye tracking research &amp; applications - ETRA ‘00, 2000, pp. 71–78.
