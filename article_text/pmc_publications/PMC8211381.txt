LICENSE: This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.


8215016
7188
Stat Med
Stat Med
Statistics in medicine
0277-6715
1097-0258

33617001
8211381
10.1002/sim.8758
NIHMS1704090
Article
High-dimensional integrative copula discriminant analysis for multiomics data
He Yong http://orcid.org/0000-0001-6169-9933
1
Chen Hao 2
Sun Hao 2
Ji Jiadong http://orcid.org/0000-0003-3562-8861
1
Shi Yufeng 12
Zhang Xinsheng 3
Liu Lei http://orcid.org/0000-0003-1844-338X
4
1 Shandong University, Jinan, China
2 School of Statistics, Shandong University of Finance and Economics, Jinan, China
3 School of Management, Fudan University, Shanghai, China
4 Division of Biostatistics, Washington University in St. Louis, St. Louis, Missouri
Correspondence: Jiadong Ji and Yufeng Shi, Shandong University, Jinan, Shandong, China. jjdjijiadong@163.com (J. J.) and yfshi@sdu.edu.cn (Y. S.)
8 6 2021
15 10 2020
30 12 2020
30 12 2021
39 30 48694884
This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.
Multiomics or integrative omics data have been increasingly common in biomedical studies, holding a promise in better understanding human health and disease. In this article, we propose an integrative copula discrimination analysis classifier in the context of two-class classification, which relaxes the common Gaussian assumption and gains power by borrowing information from multiple omics data types in discriminant analysis. Numerical studies are conducted to assess the finite sample performance of the new classifier. We apply our model to the Religious Orders Study and Memory and Aging Project (ROSMAP) Study, integrating gene expression and DNA methylation data for better prediction.

data mining
discriminant analysis
Gaussian copula
integrative analysis
machine learning

1 | INTRODUCTION

High-dimensional classification where the number of features p far exceeds the number of samples n has become increasingly common in many research fields such as computer science and bioinformatics. Fisher’s linear discriminant analysis (LDA) is an appealing classification approach in the low-dimensional setting, which aims to find direction vectors such that the projected data have maximal separation between the classes and minimal separation within the classes. Bickel and Levina1 showed that the classical Fisher’s LDA is asymptotically equivalent to random guess when p increases fast compared with n, after which a large amount of work on regularized sparse discriminant analysis arises. Wang and Zhu2 proposed an improved nearest shrunken centroid classifier and Fan and Fan3 proposed feature annealed independence rules, which are both based on a working independence assumption. Fan et al4 proposed the regularized optimal affine discriminant (ROAD) approach and Cai and Liu5 proposed a linear programming discriminant (LPD) rule highly related to the Dantzig selector.6 Mai et al7 proposed a direct approach based on an equivalent least square formulation of the LDA.

All aforementioned methods were mainly developed for LDA in the context of two multivariate normal classes, and the normality assumption could be restrictive in real applications. To relax the Gaussian assumption, Lin and Jeon8 first extended the underlying distributions to a semiparametric model which assumes that, after unspecified univariate monotone transformations, the class distributions are multivariate normal. Han et al9 considered the same model in the high-dimensional setting and proposed the copula discriminant analysis (CODA) rule, and showed that CODA is variable selection consistent with the parametric rate and the expected misclassification error is consistent to the Bayes risk. Similar work on the model also includes but not limited to Mai and Hui,10 Jiang and Leng,11 He et al.12 It is worth mentioning that He et al12 provided a solution for copula discriminant analysis when the sparsity assumption is violated.

In real applications, it is often the case that multisource data are available. For example, in genomic studies, we measure the gene expression, DNA copy number variation, and DNA methylation data for the same subject. The current work is motivated by the Religious Orders Study and Memory and Aging Project (ROSMAP) Study, which is a cohort study of aging and dementiathat include organ donation at death. The data set includes the Gene Expression RNA-Sequence Data, Gene Expression Microarray Data, and DNA Methylation Data for the same subject, and empirical study shows that the distribution of the data is far away from the Gaussian distribution. Alzheimer’s disease (AD) is a complex disease caused by the interaction of environmental factors and genetic factors. One question naturally arises on how to fully excavate the information in the multiomics non-Gaussian data types so that a better prediction can be achieved? A promising solution may be the increasingly popular integrative analysis technique.

Integrative analysis can deal with multiomics data sources and boost power for statistical analysis. Many scholars have also found the integrating methods can effectively improve the accuracy of biomarkers selection. For instance, Liu et al13 analyzed gene expression data set related to multiple cancers and showed that integrative analysis can identify the potential pathogenic genes of various cancers more accurately. Fan et al14 integrated multiple data types in the analysis of breast cancer gene expression data sets and conducted the approximate single factor graphical model analysis, demonstrating that integrative analysis can effectively identify genes that cause breast cancer. Other related studies include Ma et al,15 Mo et al,16 Huang et al,17 and so on. Furthermore, many studies focusing on classification concluded that integrative analysis can improve classification accuracy. For example, Shen et al18 proposed sparse integrative clustering of multiple omics data types and showed that classifiers with integrative analysis classify cancer samples to various subtypes more accurately. Cai et al19 proposed a framework of structured matrix completion for applications in genomic data integration which enables one to construct more accurate prediction rules for ovarian cancer survival. Li and Li20 proposed an integrative linear discriminant analysis (iLDA) method for multitype data and established a theoretical guarantee that iLDA achieves a smaller classification error than running linear discriminant analysis on each data type individually. There exist many related studies in the multiomics field, see also Ning et al,21 Kim et al,22 Wang et al,23 and so on. In neuroscience community, integrative analysis has also been recognized as a more powerful classification tool, see for instance, Uludag and Roebroeck,24 Zhang et al,25 Dai et al.26 In summary, integrative analysis, under which multiple-data types are analyzed together, has been shown to outperform single-data type analysis in various statistical problems.

In the article, we propose an integrative copula discrimination analysis (ICDA) for the ROSMAP study, in which a Gaussian copula model is adopted to relax the normality assumption of the data set, and the sparse group-lasso form penalty is used for fully mining information from multiomics data. The ICDA classifier has the following two main advantages compared with the existing classifiers: (i) it relaxes the Gaussian assumption and thus is more flexible in real application; (ii) it borrows information from multiple data types and thus is more accurate for classification and more powerful for identifying critical variables. Numerical studies show that the proposed ICDA procedure achieves smaller classification error rates than conducting copula discriminant analysis on each data-source separately. Besides, even if the data are truly Gaussian, the ICDA classifier works comparably with the iLDA classifier by Li and Li.20 When the data are non-Gaussian, the ICDA performs much better, which indicates that the ICDA can be used as a safe replacement of the iLDA classifier.

The rest of the article is organized as follows. In Section 2, we present the ICDA classifier for the scenarios where the data types share or do not share common variables separately. In Section 3, we conduct numerical simulations to investigate the finite sample performance of the ICDA classifier and its competitors. In Section 4, a real gene-expression data example associated with AD is given to illustrate its empirical usefulness. Finally, we give a brief discussion on possible future directions in Section 5.

2 | METHODOLOGY

In this section, we introduce the integrative copula discrimination analysis (ICDA) model. To facilitate the expression, we first introduce the nonparanormal distribution.

Definition 1 (The nonparanormal distribution). A p-dimensional random variable X = (X1, …, Xp)⊤ has a nonparanormal distribution if there exist monotone functions {fj}j=1p such that f(X)=(f1(X1),…,fp(Xp))⊤~Np(μ,Σ), where μ=(μ1,…,μp)⊤, Σ=(Σjk).

Liu et al27 showed that the nonparanormal distribution is a Gaussian copula if fj’s are monotone and differentiable. We denote X~NPN(μ,Σ,f) if X satisfies the Gaussian copula model.

Given a binary trait D, let G denote a p-dimensional random vector. Assume that conditional on D = d, G|D=d~NPN(μd,Σ,f) for d=1,2. Let Xk=(Xk1,…,Xkp)⊤∈ℝp and Yk′=(Yk′1,…,Yk′p)⊤∈ℝp be observations of G given D=1 and D=2, respectively, with Xk~NPN(μ1,Σ,f) for k=1, …, n1 and Yk′~NPN(μ2,Σ,f) for k′ = 1, …, n2. Let X=(X1,…,Xn1)⊤ and Y=(Y1,…,Yn2)⊤ denote the data matrices from two populations and μ = (μ1 + μ2)/2 and δ = μ2 − μ1. The identifiability issue and well-performed estimators for unknown parameters of (f,Σ, μ, δ) have been investigated in the literature, see Han et al9 and He et al.12 For completeness, we put the related material in Appendix A and simply denote the estimators for (f,Σ,μ,δ) as (f^,S^,μ^,δ^) hereafter.

In the model setting, the Bayes rule classifies a new observation G to class 2 if and only if” (f(G)−μ)⊤β∗≥0,

where β∗=Σ−1δ, see Han et al.9 In fact, β∗ is the Bayes classification direction that all linear discriminant classifiers tend to mimic. The corresponding Bayes error is R=Φ(−Δ/2), Δ=δ⊤Σ−1δ,

where Δ measures the normalized distance between the two centroids and in essence quantifies the difficulty of classifying the classes.

From now on, we denote the Xk and Yk′ as the vector that concatenates the variables from all data types. We assume there exist M data types and first consider the scenario in which all M types share a set of T common variables, that is, p=MT. The term “common variables” here refer to those variables that are related through some common structure in different data types. For example, common variables refer to different genetic measurements corresponding to the same gene in genomics and thus Figure 1 indeed illustrates the omics variables grouped by genes. The top panel of Figure 1 illustrates the scenario in which all data types share a set of common variables. We construct the loss function as follows: L(β)=β⊤Σβ/2−δ⊤β.

Taking the derivative for the loss function L(β) with respective to β, one would get Σβ − δ. Set the derivative to be zero, one would get the minimizer of the loss function L(β) as Σ−1δ, which is exactly the Bayes classification direction, β∗. In the high dimensional setting, we assume that β∗ is not only sparse but also has group structure such that it borrows information from multiple data types. Thus, we are motivated to estimate β∗ by replacing (δ,Σ) with (δ^,S^) in L(β) and solving the following regularized optimization problem (1) β^=argminβ∈ℝp[12β⊤S^β−δ^⊤β+λn∑j=1T{(1−α)‖βTj‖1+α‖βTj‖2}], α∈[0,1],

where Tj={j1,…,jM}, jm is the index of the jth variable in the mth data type and Tj is the set of indices corresponding to its appearance in M data types. We adopt the sparse group lasso-type penalty in (1), which is a weighted sum of ℓ1-norm and ℓ2-norm of βTj, see Simon et al.28 To better understand the advantage of the penalty, we look into two extreme cases where α = 0 and α = 1. When α = 0, the penalty term reduces to an elementwise ℓ1 penalty, that is, the classical Lasso penalty. No group structure is imposed in this case and each data type can have different sets of informative variables contributing to classification. When α = 1, the penalty degenerates to the group Lasso penalty.29 As is well known, for the group Lasso penalty, either all elements of βTj enters in the classification rule or none element of βTj does. Thus, by tuning the parameter α, ICDA offers a data-driven way to balance the two cases. In practice, the tuning parameters λn and α can be selected by cross-validation, see details below. Furthermore, the penalty degenerates to the Lasso penalty when we only have one data type with M =1.

For the scenario in which different data types do not share a common set of variables, we take the following strategy. For the variables appearing in more than one data type, we impose the weighted sum of the ℓ1 and ℓ2 norm penalty as in (1), while for those variables appearing in only one data type, we just impose the ℓ1 penalty. Specifically, let N be the set of variables that appear only in one data type, and M be the set of variables that appear in more than one data type, and T=M∪N be the set of all unique variables. See the bottom panel of Figure 1. We estimate the classification direction β∗ by solving the following regularized optimization problem: (2) β^=argminβ∈ℝp[12β⊤S^β−δ^⊤β+λn∑j∈M{(1−α)‖βTj‖1+α‖βTj‖2}+λn∑j∈N‖βTj‖1], α∈[0,1],

where Tj is the set of indices corresponding to jth variable’s appearance across all data types for j∈T. Of note, if all the data types share the same variables, the optimization problem in (2) is exactly the same as that in (1). On the other hand, if the data types have no common variables at all, then the penalty term in (2) degenerates to the ℓ1 Lasso penalty.

For a new observation G, given the estimated classification direction in (1) or (2), ICDA classifies it to class 2 if and only if (3) (f^(G)−μ^)⊤β^≥0.

In practice, the tuning parameters λn and α in the optimization problem in (1) or (2) can be selected by V-fold cross-validation (CV). In detail, we use the training samples excluding the vth fold to estimate the classification direction β, denoted by β^(λn,α)(−v). Then, we treat the training samples in the vth fold {Gi(v),i=1,…,n/V} as test samples, and let z^i(λn,α)(v) be the predicted class label of Gi(v), where z^i(λn,α)(v)=I((f^(−v)(Gi(v))−μ^(−v))⊤β^(λn,α)(−v)≥0),

and f^(−v), μ^(−v) are similarly estimated as in Appendix A with the training samples excluding the vth fold. Finally, the optimal value of λn and α is chosen by minimizing the cross-validation error loss, that is, (4) (λn∗,α∗)=argminλn,α1V∑v=1V∑i=1n/VI(z^i(λn,α)(v)≠z^i(v))n/V, λn&gt;0,   α∈[0,1],

where zi(v) is the true class label of Gi(v).

A sketch of the ICDA algorithm is summarized in Algorithm 1. Further algorithm details are put in Appendix B.

Algorithm 1. ICDA algorithm                             _¯Input:STrain, STest, Stepsizesand maximum iterations max.iter.Output:Predicted labels of the test samples in STest. 1: procedure 2:     Estimate the transformation function fand μ with training samples in STrain illustrated in Appendix A. 3:     For the optimization problem (2), adopt the proximal gradient algorithm with a majorization-minimization   scheme, i.e.: 4:     For each pair of (α,λn),initialize β at β(0), let i denotes the ith iteration and initialize i = 0. 5:     while(‖β(i)−β(i−1)‖2≥10−3)and (i≤max.iter)do 6:        i=i+1 7:        for j←1 to |T|do 8:             if j∈Mthen 9:                   βTj(i)=cTj(1−λnαs‖cTj‖2−1)+,10:                   where cTj=Soft[βTj(i−1)−s{S^β(i−1)−δ^}Tj,λn(1−α)s]11:             end if12:             if j∈Nthen13:                   βTj(i)=Soft[βTj(i−1)−s{S^β(i−1)−δ^}Tj,λns]14:             end if15:         end for16:     end while17:     Select the optimal parameters α* and λn* by minimizing CV error loss in (4), and get the corresponding optimal     classification direction β^.18:     Classify the test subjects in STest to class 2 if and only if (3) is satisfied.19:  end procedure                                 _

3 | SIMULATION STUDY

In this section, we conduct simulation studies to compare the finite sample performances of the ICDA method with some existing methods: the iLDA method by Li and Li,20 the ROAD method by Fan et al,4 the LASSO method by Mai,7 the NPN.ROAD method and the NPN.LASSO method proposed by Han et al,9 and the SDA method proposed by Clemmensen.30

For the two classes: (G|D=1)~NPN(μ1,Σ,f) and (G|D=2)~NPN(μ2,Σ,f), we generate n independent samples as the training set and 200 independent samples as the testing set. We set μ1 = 0, μ2 = Σβ∗ so that δ = μ2 − μ1 = Σβ∗. The generation of β∗ and Σ is introduced in the following model settings. For each setting, we let the data types M=3 and each data type contains T variables, then p=MT. Furthermore, we explore the effects of different transformation functions f by considering the following two types of transformation functions: the linear (or identity) transformation flinear =(f0, f0, …, f0)with f0(x)=x; and the Gaussian CDF transformation fCDF =(f1, f1, …, f1) as defined in Liu et al,27 which is given below.

Definition 2 (Gaussian CDF Transformation). Let h(⋅) be a univariate Gaussian CDF with mean μh and the standard deviation σh: h(t) = Φ((t − uh)∕σh). The Gaussian CDF transformation hj =(fj)−1 for the jth dimension is defined as hj(zj)=σj(h(zj)−∫h(t)ϕ(t−ujσj)dt∫(h(y)−∫h(t)ϕ(t−ujσj)dt)2ϕ(y−ujσj)dy)+μj,

where φ(⋅) is the standard Gaussian density function and σj = Σjj.

We also consider two covariance structures. The first is the auto-regression (AR) covariance, where Σij = 0.8|i−j|. The second is the banded covariance (BC), where Σij = 1 if |i−j|=0, Σij = 0.5 if |i−j|=1 and 0 otherwise.

The following two data generating scenarios are considered and the details of the scenarios are shown in Table 1:

Scenario 1 In the scenario all data types share a common set of variables and we set n=100, p=300, T =100. We consider two transformation forms (Gaussian CDF transformation and linear transformation), two Bayes error rates: 10% and 20% and two different correlation structures: AR and BC.

Scenario 2 In this scenario 40% of the unique variables are shared by all data types, and the rest variables only show in one data type. We set n=100, p=300, T =40 and we also consider two different transformation forms (Gaussian CDF transformation and linear transformation), two Bayes error rates: 10% and 20% and two different correlation structures: AR and BC.

We assess the empirical performances of different methods by the misclassification error rate, the false discovery rate (FDR), the sensitivity rate, and the specificity rate. Suppose β∗=(βi∗) has the support S0={i:βi∗≠0}, its estimator β^ has the support set S^ and let |S^| be the cardinality of S^. The sensitivity rate and the specificity rate are defined as follows: FDR=FP|S^|, SPECIFICITY RATE=TNTN+FP, SENSITIVITY RATE=TPTP+FN,

where TP, TN, FP, and FN are the numbers of true positives, true negatives, false positives, and false negatives, which are defined as TP=#{i:i∈S0∩S^},  TN=#{i:i∈S0c∩S^c},

FP=#{i:i∈S0c∩S^},  FN=#{i:i∈S0∩S^c}.

The simulation results for Scenario 1 are shown in Table 2. In terms of misclassification error rate, regardless of the types of covariance structure, ICDA is close to the Bayes error rate and has superior performances over the competitors, especially when the transformation function is a Gaussian CDF transformation. We also find that the integrative methods (ICDA and iLDA) outperform the nonintegrative methods (ROAD, NPN.ROAD, LASSO, NPN.LASSO, and SDA), which means that the integrative methods can extract more information for discriminant analysis and achieve a lower false positive rate. Taking a closer look at the index FDR, we find that our method ICDA performs overwhelmingly well with almost zero false positives. Compared with iLDA, ICDA has a higher specificity rate and comparable sensitivity rate; and compared with ROAD, NPN.ROAD, LASSO, NPN.LASSO, and SDA, our method ICDA has better performances in terms of both the specificity rate and sensitivity rate. These three indicators measure the ability of classifiers to select useful variables in the discriminant analysis. Hence, the superiority of ICDA in these indicators suggests that it is more accurate in identifying important variables than other approaches.

Table 3 shows the results for Scenario 2. Compared with the results in Table 2, our method ICDA has similar performances in terms of all four indicators, which means that ICDA can not only deal with cases where all data types share the common variables, but also can handle cases where data types do have unique variables. The sensitivity of our ICDA method is slightly lower than other methods. The reason lies in that ICDA selects fewer variables than other methods, and from the perspective of the FDR index, most variables selected by ICDA are all truly important. Thus overall our ICDA method is superior to other methods in terms of classification accuracy and the ability to identify important variables. In summary, ICDA achieves excellent prediction results in various scenarios and can be used as a safe replacement of the iLDA method even when the underlying data are truly Gaussian.

Following the comment of one reviewer, we also compared the computational efficiency of different approaches using a single core node with 2.80GHz Intel(R) Core(TM) CPU i7–7700HQ. Our method ICDA is computationally more efficient than ROAD and NPN.ROAD, while is less efficient than iLDA, LASSO, NPN.LASSO, and SDA. To be specific, in Scenario 1, given the tuning parameters α and λ, ICDA takes 0.96 second on average, compared with 0.10 second (iLDA), 1.08 seconds (ROAD), 1.09 seconds (NPN.ROAD), 0.08 second (LASSO), 0.09 second (NPN.LASSO), and 0.02 second (SDA).

4 | REAL DATA ANALYSIS

In this section we apply the proposed method ICDA to analyze a well-known AD research data set from the ROSMAP study. We refer the details of the ROSMAP studies to Bennett et al.31,32 AD is one of the most common senile dementia. It is a complex heterogeneous disease caused by the interaction of environmental factors and genetic factors. As the population ages, the prevalence of dementia is increasing. Therefore, predictive analysis of AD at the genetic level is particularly important. Although many scholars have proposed analytical methods in predicting AD, for example, Allen et al33 and Chen et al,34 little research integrates different types of genetic data for better prediction of AD. Intuitively, different types of genetic data incorporates more information compared with focusing on only one specific genetic data type. For example, Xu et al35 carried out prediction research on cervical cancer, which illuminates the validity of combining different types of genetic data (gene expression data and methylation data) for integrative analysis.

In this study, we integrate three types of omics data: (i) DNA methylation; and two types of gene expression data: (ii) RNA sequence and (iii) microarray. We select 140 subjects labeled as AD (Alzheimer’s disease group) and 122 subjects labeled as NCI (no cognitive impairment group) from the ROSMAP study. We extract 164 genes from the AD pathway (hsa05010) of KEGG containing at least one type of these omics data. In Figure 2 we show the Venn diagram for the three different types of omics data. Out of these 164 genes, 133 genes have all three types of omics data, 17 genes have two types, and 14 genes have only one type. We thus consider a total of 447 omics variables, one for each type of omics data from a gene. The variables corresponding to the same gene are treated as a group. We also take the effects of four confounder variables (age, gender, APOE genotype, and education level) into account. As some confounder variables are discrete variables and the ICDA and iLDA can only deal with continuous variables, we construct a synthetical variable which is a linear combination of the confounder variables. For each subject s, the formula of this synthetical variable is: β1 ages + β2 genders + β3 APOEs + β4 educations and these four coefficients are obtained by building a logistic regression model with disease state as the response variable. Overall, there are a total of 262 subjects and 448 variables in our study.

We employ the Shapiro-Wilk method to test the null hypothesis that each variable follows the normal distribution. Among the 448 variables, the normality hypothesis is rejected for 201 variables in the AD group and 219 variables in the NCI group. For illustration, we select two genes and present the corresponding Q-Q plots in each group, as shown in Figure 3. It can be intuitively seen from Figure 3 that the distributions of the two genes are far away from normal. Hence, our ICDA method which relaxes the normal distribution assumption would be more appropriate for this data set. Then, we randomly split subjects into two sets for each group, one-third of the subjects as the test set, and the remainder as the training set. We train the classifiers with the training set and use the test set to assess the accuracy of classification. We repeat the procedure 100 times. In this real data example, we compared our method with all the competitors in the simulation study.

First we examine the performance of the methods in terms of classification accuracy. The average misclassification error rates (standard deviation) over 100 replications by ICDA, iLDA, ROAD, NPN.ROAD, LASSO, NPN.LASSO, and SDA are 0.29 (0.04), 0.31 (0.04), 0.35 (0.07), 0.35 (0.07), 0.40 (0.08), 0.41 (0.06), and 0.46 (0.06), respectively. The boxplots of the misclassification error rates over 100 replicates are presented in Figure 4, from which it can be clearly seen that our ICDA method is more accurate and more stable in terms of discriminating AD from controls. We then examine the ability of these methods to identify critical genes. To this end, for each gene, we sort by the number of its occurrences in the 100 replicates and show the top five genes selected by each method in Table 4. It can be seen from Table 4 that the critical genes selected by other methods except iLDA are quite different from those selected by our method. We speculate that this may be the reason why these methods performs worse in terms of classification accuracy. As for ICDA and iLDA, three of the critical genes selected by ICDA are the same as those selected by the iLDA method, which are IL1B, CAPN2, and COX6A2. PLCB1 and CALM3 are not selected by iLDA but are identified as critical genes by ICDA. Although there is no gold standard to compare the gene selection performance in the real data example, we find that existing medical literature illustrates the validity of ICDA’s selected critical genes. For example, gene COX6A2 can encode a protein called cytochrome c oxidase subunit VIa polypeptide 2, also known as complex IV, which is the last enzyme in the mitochondrial electron transport chain. Hence, this gene can affect the function of mitochondria. Kriebel et al36 pointed out that age-related impairment of mitochondrial function may negatively impact energy-demanding processes such as synaptic transmission thereby triggering cognitive decline and processes of neurodegeneration, which in turn lead to AD. For another example, Zhuang et al37 identified that the IL1B (interleukin-1 beta) gene was associated with the pathogenesis of AD. Also, Griffin et al38 stated that increased IL-1 expression in reactive microglia surrounding amyloid plaques promotes astrocyte proliferation and development of AD. Besides, Yang et al39 stated that dysregulation of primary PLC signaling is linked to some brain disorders including AD, while the PLC signal imbalance is caused by abnormal expression of genes such as PLCB1. Therefore, the expression of PLCB1 gene has a very important impact on the formation of AD. In summary, our proposed method ICDA has better classification performance and can identify critical genes affecting AD.

5 | CONCLUSION AND DISCUSSION

In this study, we developed a new integrative discriminant analysis method ICDA. This method has two main advantages. First, it integrates multisource data for discriminant analysis, which makes the discrimination results more accurate. Second, it relaxes the normality assumptions, which may be more suitable for real data analysis. We also demonstrated the superiority of our method in terms of classification error rates and identifying important variable through simulation study and real data analysis.

Several extensions of our method can be explored. First, we may consider a more general elliptical copula discriminant analysis model. Second, in the real analysis we only consider gene expression and methylation data, which results in a less satisfactory classification. Other omics data, for example, metabolites (metabolomics) and proteins (proteomics) could be included for better classification power. Finally, a limitation of the ICDA method lies in that it can only deal with the continuous variables. As a future topic, we are motivated to propose a copula discriminant model for mixed data which contains both categorical data and continuous data along the lines of copula graphical model for mixed data, see for example, Fan et al40 and He et al.41

ACKNOWLEDGEMENTS

Research reported in this publication was supported by the National Key R&amp;D Program of China (Grant No. 2018YFA0703900), National Science Foundation of China (11801316, 81803336, 11871309, 11971116), Natural Science Foundation of Shandong Province (ZR2019QA002, ZR2018BH033) and National Statistical Scientific Research Project (2018LY63). Research reported in this publication was also supported by NIH UL1 TR002345 and R21 AG063370. The content is solely the responsibility of the authors and does not necessarily represent the official view of the NIH. The results published here are in whole or in part based on data obtained from the AMP-AD Knowledge Portal (https://adknowledgeportal.synapse.org/). Study data were provided by the Rush Alzheimer’s Disease Center, Rush University Medical Center, Chicago. The authors thank the patients and their families for their selfless donation to further understanding Alzheimer’s disease. The funding body played no role in the design, writing, or decision to publish this manuscript. The authors would like to thank the Editor, the Associate Editor, and the anonymous reviewer for their constructive comments that led to a major improvement of this article.

Funding information

Foundation for the National Institutes of Health, Grant/Award Number: UL1 TR002345; National Key R&amp;D Program of China, Grant/Award Number: 2018YFA0703900; National Natural Science Foundation of China, Grant/Award Numbers: 11801316, 11871309, 11971116, 81803336; Natural Science Foundation of Shandong Province, Grant/Award Numbers: ZR2018BH033, ZR2019QA002

DATA AVAILABILITY STATEMENT

The data sets [ROSMAP] for this study can be found in the [Synapse platform] and the accession number is Synapse: syn3219045 [https://www.synapse.org/\LY1\textbackslash#!Synapse:syn3219045].

APPENDIX A. GAUSSIAN COPULA MODEL AND RANK-BASED ESTIMATORS FOR PARAMETERS

We first introduce the definition of the Gaussian copula, as a promising substitute for Gaussian model in real applications.

Definition 3 (The Gaussian copula). The Gaussian copula is a joint distribution of U1, …, Up over the unit cube [0,1]p with the cumulative density function (cdf) F(u1,…,up)=Φ(Φ−1(u1),…,Φ−1(up)),

where Φ(⋅) is the multivariate Gaussian cdf and Φ(⋅) is the univariate standard Gaussian cdf.

Liu et al27 showed that the nonparanormal distribution is a Gaussian copula if fj’s are monotone and differentiable. In practice, fj’s are estimated from the data. Similar as in Liu et al,27 to make the model identifiable for the classification problem, we add two constraint on f such that f preserves the population means and standard deviations: E(Xj)=E(f(Xj))=μj, Var(Xj)=Var(f(Xj))=Σjj.

Let Fj(t) be the marginal distribution function of Xj, then we have Fj(t)=Pr(Xj≤t)=Pr(fj(Xj)≤fj(t))=Φ(fj(t)−μjΣjj),

which further implies that (A1) fj(t)=μj+ΣjjΦ−1(Fj(t)).

In the following, we introduce the rank-based estimator of Σ. It is known from Fang et al42 that if X=(X1,…,Xp)~NPN(μ,Σ,f) then we have Rij=ΣijΣii1/2Σjj1/2=2sin(π6ρij)=sin(π2τij),

where ρij and τij are separately the Spearman’s rho and Kendall’s tau correlation between Xj and Xk. Denote the sample estimators of ρij and τij as ρ^ij and τ^ij, then we can estimate the unknown correlation Rij by: R^ij={1,i=j,2sin(π6ρ^ij),i≠j; or R^ij={1,i=j,sin(π2τ^ij),i≠j.

We denote S^=[S^ij] with S^ij=Σ^ii1/2Σ^jj1/2R^ij to be the Spearman’s rho or Kendall’s tau covariance matrix estimator, with {Σ^jj,j=1,…,p} defined as the corresponding marginal sample variances. The estimators based on the Spearman’s rho and Kendall’s tau statistics have similar performances, then in the following we focus on the Kendall’s tau covariance matrix estimator. Notice that both the Spearman’s rho or Kendall’s tau are rank statistics and are thus robust to outliers.

In the following we present the estimators of the transformation functions f. To this end, we further define μ^1=1n1∑k=1n1Xk, μ^2=1n2∑k′=1n2Yk′, μ^=(μ^1+μ^2)/2, δ^=μ^2−μ^1,

and the winsorized empirical cumulative distribution function of X and Y as F˜j(t;δn,X)=Tδn(1n1∑k=1n1I(Xkj≤t)), F˜j(t;δn,Y)=Tδn(1n2∑k′=1n2I(Yk′j≤t)), j∈{1,…,p},

where Tδn(x)=δn⋅I(x&lt;δn)+x⋅I(δn&lt;x&lt;1−δn)+(1−δn)⋅(x&gt;1−δn).

Let S^1 and S^2 be the rank-based covariance matrix estimators based on data matrices X and Y, respectively. We further define S^=n1/n⋅S^1+n2/n⋅S^2, where n=n1 +n2. By the result in (A1), the marginal function fj(t),j=1, …, p can be estimated by f^j(t)=n1nf^1j(t)+n2nf^2j(t),

where f^1j and f^2j are defined to be: f^1j(t)=μ^1j+S^jj1/2Φ−1(F˜j(t;δn,X))), f^2j(t)=μ^2j+S^jj1/2Φ−1(F˜j(t;δn,Y))),

where μ^1j and μ^2j are the jth element of μ^1 and μ^2, respectively.

For a new observation G, f^(G)=(f^1(G1),…,f^p(Gp))⊤ with (A2) f^j(Gj)=n1nf^1j(Gj)+n2nf^2j(Gj), for all j∈{1,…,p},

and f^1j and f^2j in (A2) are defined to be: f^1j(t)=μ^1j+S^jj1/2Φ−1(F˜j(t;δn,X))), f^2j(t)=μ^2j+S^jj1/2Φ−1(F˜j(t;δn,Y))),

where μ^1j and μ^2j are the jth element of μ^1 and μ^2, respectively, and the truncation level δn is set to be 1/(2n).

APPENDIX B. DETAILS OF PROXIMAL GRADIENT ALGORITHM IN ALGORITHM 1

In this section, we introduce the notations and present the details of proximal gradient algorithm to estimate β in Algorithm 1. At first, for the optimization (2), Let L(β)=12β⊤S^β−δ^⊤β, P(β)=λn∑j∈M{(1−α)‖βTj‖1+α‖βTj‖2}+λn∑j∈N‖βTj‖1.

Since L(β) is convex and differentiable and P(β) is convex but not differentiable, we use the proximal gradient method similar to that of Li and Li.20 In detail, we construct a surrogate function hs{β,β(i−1)} that majorizes L(β), that is, L(β) ≤ hs{β,β(i−1)}, where hs{β,β(i−1)}=L{β(i−1)}+{β−β(i−1)}T∇L{β(i−1)}+12s‖β−β(i−1)‖22.

Then, we minimize hs{β,β(i−1)} + P(β). Furthermore, we have β(i)=argminβL{β(i−1)}+{β−β(i−1)}T∇L{β(i−1)}+12s‖β−β(i−1)‖22+P(β)    =argminβ12‖β−[β(i−1)−s∇L{β(i−1)}]‖22+sP(β)    =proxsP[β(i−1)−s∇L{β(i−1)}].

We further denote βTj(i)=proxsP[βTj(i−1)−s∇L{βTj(i−1)}]={soft[βTj(i−1)−s{S^β(i−1)−δ^}Tj,λns],j∈NcTj(1−λnαs‖cTj‖2−1)+,j∈M,

where cTj=soft[βTj(i−1)−s{S^β(i−1)−δ^}Tj,λn(1−α)s] and Soft(x,λ) is the elementwise soft-thresholding operator with the value of kth element {Soft(u,λ)}k = sign(uk)(|uk| − λ)+. In this algorithm, we use the fixed step size s=0.8/e, where e is set to be the maximum eigenvalue of S^. For further details, please refer to Li and Li.20

FIGURE 1 Illustration of the omics variables grouped by genes. The top panel illustrates the scenario where all M data types share common variables, while the bottom panel illustrates the scenario where different data types may have its unique variables [Color figure can be viewed at wileyonlinelibrary.com]

FIGURE 2 Venn diagram for different data types in the empirical study [Color figure can be viewed at wileyonlinelibrary.com]

FIGURE 3 Q-Q plots for two genes (COX5A, IL1B) in the AD group and the NCI group, respectively [Color figure can be viewed at wileyonlinelibrary.com]

FIGURE 4 Boxplots of the 100 repeated misclassification error rates for ICDA, iLDA, ROAD, NPN.ROAD, LASSO, NPN.LASSO, and SDA. The triangle symbol of each method represents the mean of misclassification error rate over 100 replicates [Color figure can be viewed at wileyonlinelibrary.com]

TABLE 1 Detailed data generating settings for Scenario 1 and Scenario 2

Scenario	Transformation form	Bayes error	Correlation structure	β∗=(β1⊤,β2⊤,β3⊤)⊤	
1	Gaussian CDF	10%	AR	β1=β2=β3=0.3478∗(15⊤,0T−5⊤)⊤	
			BC	β1=β2=β3=0.4932∗(15⊤,0T−5⊤)⊤	
		20%	AR	β1=β2=β3=0.2284∗(15⊤,0T−5⊤)⊤	
			BC	β1=β2=β3=0.3239∗(15⊤,0T−5⊤)⊤	
	Linear	10%	AR	β1=β2=β3=0.3478∗(15⊤,0T−5⊤)⊤	
			BC	β1=β2=β3=0.4932∗(15⊤,0T−5⊤)⊤	
		20%	AR	β1=β2=β3=0.2284∗(15⊤,0T−5⊤)⊤	
			BC	β1=β2=β3=0.3239∗(15⊤,0T−5⊤)⊤	
2	Gaussian CDF	10%	AR	β1=β2=β3=0.3385∗(15⊤,0T−5⊤,1,0n−T−1⊤)⊤	
			BC	β1=β2=β3=0.4680∗(15⊤,0T−5⊤,1,0n−T−1⊤)⊤	
		20%	AR	β1=β2=β3=0.2223∗(15⊤,0T−5⊤,1,0n−T−1⊤)⊤	
			BC	β1=β2=β3=0.3073∗(15⊤,0T−5⊤,1,0n−T−1⊤)⊤	
	Linear	10%	AR	β1=β2=β3=0.3385∗(15⊤,0T−5⊤,1,0n−T−1⊤)⊤	
			BC	β1=β2=β3=0.4680∗(15⊤,0T−5⊤,1,0n−T−1⊤)⊤	
		20%	AR	β1=β2=β3=0.2223∗(15⊤,0T−5⊤,1,0n−T−1⊤)⊤	
			BC	β1=β2=β3=0.3073∗(15⊤,0T−5⊤,1,0n−T−1⊤)⊤	

TABLE 2 Simulation results for Scenario 1 (the values in the parentheses are the standard errors)

f	Bayes error	Correlation	ICDA	iLDA	ROAD	NPN.ROAD	LASSO	NPN.LASSO	SDA	
Error rate	
f CDF	10%	AR	0.12 (0.06)	0.14 (0.05)	0.20 (0.08)	0.20 (0.09)	0.16 (0.04)	0.14 (0.03)	0.27 (0.03)	
		BC	0.14 (0.08)	0.16 (0.05)	0.24 (0.06)	0.23 (0.07)	0.20 (0.04)	0.20 (0.04)	0.29 (0.03)	
	20%	AR	0.23 (0.05)	0.26 (0.05)	0.34 (0.08)	0.33 (0.07)	0.29 (0.05)	0.29 (0.05)	0.33 (0.04)	
		BC	0.29 (0.07)	0.31 (0.06)	0.37 (0.06)	0.36 (0.06)	0.35 (0.05)	0.34 (0.06)	0.37 (0.05)	
f Linear	10%	AR	0.12 (0.03)	0.13 (0.03)	0.20 (0.09)	0.21 (0.09)	0.14 (0.03)	0.15 (0.03)	0.26 (0.03)	
		BC	0.17 (0.06)	0.16 (0.04)	0.24 (0.08)	0.25 (0.08)	0.21 (0.04)	0.21 (0.04)	0.28 (0.03)	
	20%	AR	0.24 (0.05)	0.26 (0.05)	0.34 (0.08)	0.33 (0.07)	0.29 (0.05)	0.30 (0.05)	0.33 (0.04)	
		BC	0.31 (0.08)	0.30 (0.06)	0.37 (0.07)	0.36 (0.07)	0.35 (0.05)	0.35 (0.05)	0.36 (0.04)	
FDR	
f CDF	10%	AR	0.01 (0.08)	0.33 (0.33)	0.58 (0.24)	0.53 (0.25)	0.54 (0.27)	0.51 (0.26)	0.77 (0.05)	
		BC	0.01 (0.06)	0.32 (0.32)	0.64 (0.21)	0.67 (0.21)	0.52 (0.22)	0.51 (0.22)	0.74 (0.06)	
	20%	AR	0.07 (0.15)	0.38 (0.33)	0.67 (0.24)	0.70 (0.20)	0.55 (0.24)	0.57 (0.22)	0.84 (0.05)	
		BC	0.08 (0.18)	0.49 (0.33)	0.70 (0.21)	0.71 (0.21)	0.65 (0.20)	0.63 (0.19)	0.82 (0.05)	
f Linear	10%	AR	0.03 (0.09)	0.34 (0.31)	0.55 (0.25)	0.55 (0.28)	0.40 (0.23)	0.43 (0.25)	0.77 (0.06)	
		BC	0.01 (0.06)	0.38 (0.32)	0.65 (0.19)	0.64 (0.19)	0.54 (0.20)	0.53 (0.20)	0.75 (0.06)	
	20%	AR	0.08 (0.16)	0.35 (0.33)	0.66 (0.25)	0.65 (0.23)	0.53 (0.24)	0.56 (0.24)	0.83 (0.05)	
		BC	0.08 (0.14)	0.36 (0.36)	0.68 (0.25)	0.69 (0.21)	0.64 (0.21)	0.62 (0.21)	0.82 (0.05)	
Specificity rate	
f CDF	10%	AR	1.00 (0.00)	0.92 (0.18)	0.92 (0.06)	0.92 (0.06)	0.97 (0.03)	0.98 (0.03)	0.90 (0.01)	
		BC	1.00 (0.00)	0.92 (0.16)	0.92 (0.05)	0.91 (0.05)	0.96 (0.03)	0.95 (0.03)	0.92 (0.01)	
	20%	AR	1.00 (0.01)	0.90 (0.19)	0.91 (0.06)	0.91 (0.06)	0.96 (0.03)	0.96 (0.03)	0.91 (0.01)	
		BC	1.00 (0.00)	0.89 (0.18)	0.91 (0.06)	0.90 (0.06)	0.94 (0.03)	0.95 (0.03)	0.90 (0.01)	
f Linear	10%	AR	1.00 (0.00)	0.92 (0.15)	0.94 (0.06)	0.93 (0.06)	0.97 (0.03)	0.97 (0.03)	0.93 (0.01)	
		BC	1.00 (0.00)	0.90 (0.18)	0.92 (0.05)	0.92 (0.05)	0.95 (0.03)	0.95 (0.03)	0.92 (0.01)	
	20%	AR	1.00 (0.05)	0.92 (0.16)	0.91 (0.06)	0.92 (0.07)	0.96 (0.03)	0.96 (0.03)	0.91 (0.01)	
		BC	1.00 (0.00)	0.93 (0.14)	0.91 (0.06)	0.91 (0.06)	0.94 (0.03)	0.95 (0.03)	0.90 (0.01)	
Sensitivity rate	
f CDF	10%	AR	0.66 (0.21)	0.76 (0.23)	0.52 (0.12)	0.53 (0.13)	0.42 (0.09)	0.46 (0.10)	0.40 (0.09)	
		BC	0.67 (0.23)	0.77 (0.23)	0.57 (0.14)	0.60 (0.14)	0.58 (0.11)	0.58 (0.10)	0.53 (0.11)	
	20%	AR	0.56 (0.23)	0.65 (0.27)	0.44 (0.13)	0.44 (0.12)	0.38 (0.10)	0.38 (0.10)	0.33 (0.09)	
		BC	0.50 (0.20)	0.56 (0.26)	0.47 (0.13)	0.48 (0.13)	0.44 (0.11)	0.43 (0.11)	0.40 (0.10)	
f Linear	10%	AR	0.62 (0.23)	0.79 (0.22)	0.50 (0.12)	0.53 (0.11)	0.48 (0.10)	0.49 (0.09)	0.41 (0.10)	
		BC	0.60 (0.24)	0.77 (0.23)	0.58 (0.14)	0.59 (0.15)	0.57 (0.11)	0.58 (0.11)	0.52 (0.11)	
	20%	AR	0.56 (0.24)	0.66 (0.23)	0.44 (0.12)	0.45 (0.12)	0.37 (0.10)	0.39 (0.10)	0.34 (0.11)	
		BC	0.51 (0.20)	0.57 (0.28)	0.47 (0.14)	0.47 (0.13)	0.43 (0.12)	0.43 (0.12)	0.39 (0.10)	

TABLE 3 Simulation results for Scenario 2 (the values in the parentheses are the standard errors)

f	Bayes error	Correlation	ICDA	iLDA	ROAD	NPN.ROAD	LASSO	NPN.LASSO	SDA	
Error rate										
f CDF	10%	AR	0.13 (0.06)	0.15 (0.04)	0.21 (0.09)	0.21 (0.09)	0.17 (0.03)	0.15 (0.03)	0.26 (0.03)	
		BC	0.17 (0.07)	0.20 (0.06)	0.27 (0.08)	0.24 (0.08)	0.23 (0.05)	0.22 (0.05)	0.30 (0.03)	
	20%	AR	0.23 (0.05)	0.27 (0.06)	0.34 (0.08)	0.34 (0.08)	0.30 (0.05)	0.29 (0.05)	0.34 (0.04)	
		BC	0.31 (0.07)	0.33 (0.06)	0.38 (0.06)	0.37 (0.06)	0.36 (0.05)	0.36 (0.05)	0.38 (0.04)	
f Linear	10%	AR	0.14 (0.05)	0.14 (0.04)	0.21 (0.09)	0.22 (0.09)	0.16 (0.04)	0.17 (0.04)	0.27 (0.03)	
		BC	0.18 (0.06)	0.19 (0.05)	0.27 (0.08)	0.27 (0.07)	0.23 (0.04)	0.23 (0.04)	0.29 (0.03)	
	20%	AR	0.24 (0.05)	0.27 (0.06)	0.34 (0.07)	0.33 (0.07)	0.30 (0.05)	0.30 (0.05)	0.34 (0.04)	
		BC	0.31 (0.07)	0.34 (0.06)	0.39 (0.06)	0.40 (0.07)	0.36 (0.05)	0.37 (0.04)	0.38 (0.05)	
FDR										
f CDF	10%	AR	0.06 (0.08)	0.27 (0.29)	0.57 (0.27)	0.59 (0.25)	0.45 (0.25)	0.42 (0.24)	0.76 (0.06)	
		BC	0.03 (0.06)	0.30 (0.33)	0.63 (0.20)	0.60 (0.26)	0.52 (0.19)	0.56 (0.17)	0.73 (0.06)	
	20%	AR	0.06 (0.12)	0.29 (0.32)	0.68 (0.22)	0.69 (0.20)	0.58 (0.23)	0.57 (0.23)	0.82 (0.06)	
		BC	0.08 (0.18)	0.27 (0.34)	0.74 (0.15)	0.73 (0.16)	0.67 (0.17)	0.65 (0.19)	0.81 (0.05)	
f Linear	10%	AR	0.05 (0.06)	0.19 (0.26)	0.58 (0.25)	0.58 (0.25)	0.44 (0.25)	0.45 (0.25)	0.77 (0.06)	
		BC	0.01 (0.06)	0.16 (0.26)	0.66 (0.17)	0.65 (0.17)	0.56 (0.17)	0.56 (0.16)	0.72 (0.06)	
	20%	AR	0.05 (0.10)	0.30 (0.36)	0.70 (0.19)	0.66 (0.22)	0.59 (0.20)	0.55 (0.23)	0.82 (0.06)	
		BC	0.08 (0.19)	0.28 (0.37)	0.75 (0.15)	0.77 (0.13)	0.69 (0.16)	0.68 (0.16)	0.81 (0.05)	
Specificity rate	
f CDF	10%	AR	1.00 (0.01)	0.93 (0.16)	0.93 (0.07)	0.93 (0.06)	0.97 (0.03)	0.97 (0.02)	0.93 (0.01)	
		BC	1.00 (0.01)	0.88 (0.23)	0.92 (0.05)	0.92 (0.06)	0.95 (0.03)	0.94 (0.03)	0.92 (0.01)	
	20%	AR	1.00 (0.00)	0.91 (0.21)	0.91 (0.07)	0.91 (0.07)	0.96 (0.03)	0.96 (0.03)	0.91 (0.01)	
		BC	0.99 (0.03)	0.93 (0.19)	0.90 (0.05)	0.90 (0.06)	0.90 (0.03)	0.91 (0.03)	0.87 (0.01)	
f Linear	10%	AR	1.00 (0.00)	0.96 (0.13)	0.93 (0.05)	0.93 (0.06)	0.97 (0.03)	0.97 (0.03)	0.92 (0.01)	
		BC	1.00 (0.00)	0.96 (0.14)	0.91 (0.06)	0.91 (0.05)	0.95 (0.03)	0.95 (0.03)	0.92 (0.01)	
	20%	AR	1.00 (0.00)	0.90 (0.22)	0.91 (0.06)	0.92 (0.06)	0.96 (0.03)	0.96 (0.03)	0.91 (0.01)	
		BC	1.00 (0.00)	0.91 (0.19)	0.89 (0.05)	0.89 (0.06)	0.93 (0.03)	0.93 (0.03)	0.90 (0.01)	
Sensitivity rate	
f CDF	10%	AR	0.56 (0.22)	0.66 (0.20)	0.45 (0.11)	0.46 (0.11)	0.41 (0.09)	0.41 (0.09)	0.35 (0.08)	
		BC	0.49 (0.21)	0.58 (0.26)	0.47 (0.14)	0.45 (0.14)	0.46 (0.10)	0.47 (0.11)	0.46 (0.10)	
	20%	AR	0.49 (0.20)	0.55 (0.23)	0.40 (0.13)	0.41 (0.13)	0.34 (0.10)	0.35 (0.10)	0.30 (0.09)	
		BC	0.39 (0.16)	0.43 (0.22)	0.36 (0.12)	0.37 (0.12)	0.38 (0.11)	0.39 (0.10)	0.35 (0.10)	
f Linear	10%	AR	0.53 (0.21)	0.64 (0.20)	0.47 (0.12)	0.48 (0.12)	0.42 (0.08)	0.43 (0.09)	0.36 (0.09)	
		BC	0.45 (0.19)	0.51 (0.21)	0.55 (0.16)	0.55 (0.15)	0.53 (0.12)	0.53 (0.10)	0.49 (0.10)	
	20%	AR	0.45 (0.20)	0.51 (0.26)	0.41 (0.13)	0.42 (0.12)	0.34 (0.10)	0.36 (0.10)	0.31 (0.10)	
		BC	0.37 (0.16)	0.41 (0.22)	0.43 (0.11)	0.43 (0.11)	0.36 (0.10)	0.36 (0.11)	0.33 (0.10)	

TABLE 4 Critical genes identified by various methods (the corresponding frequencies in parentheses)

	ICDA	iLDA	ROAD	NPN.ROAD	LASSO	NPN.LASSO	SDA	
1	IL1B (80)	IL1B (68)	GSK3B (76)	GSK3B (71)	SNCA (42)	SNCA (30)	APBB1 (99)	
2	CAPN2 (77)	CAPN2 (55)	COX6A2 (76)	SNCA (71)	RYR3 (41)	COX6A2 (30)	NDUFV3 (93)	
3	PLCB1 (66)	COX7A1 (37)	MAPK3 (75)	COX7A2 (70)	COX6A2 (41)	GSK3B (29)	CALML3 (93)	
4	CALM3 (60)	COX6A2 (31)	RYR3 (74)	MAPK3 (70)	GSK3B (40)	RYR3 (28)	NDUFB7 (92)	
5	COX6A2 (48)	NDUFB9 (31)	NDUFV1 (74)	COX6A2 (72)	ITPR3 (39)/COX4I1 (39)	SDHC (28)/ADAM10 (28)	NDUFA10 (90)	


REFERENCES

1. Bickel PJ , Levina E . Some theory for Fisher’s linear discriminant function,’naive Bayes’, and some alternatives when there are many more variables than observations. Bernoulli 2004;10 (6 ):989–1010.
2. Wang S , Zhu J . Improved centroids estimation for the nearest shrunken centroid classifier. Bioinformatics 2007;23 (8 ):972–979.17384429
3. Fan J , Fan Y . High dimensional classification using features annealed independence rules. Ann Stat 2008;36 (6 ):2605–2637.19169416
4. Fan J , Feng Y , Tong X . A road to classification in high dimensional space: the regularized optimal affine discriminant. J R Stat Soc Ser B Stat Methodol 2012;74 (4 ):745–771.
5. Cai T , Liu W . A direct estimation approach to sparse linear discriminant analysis. J Am Stat Assoc 2011;106 (496 ):1566–1577.
6. Candes E , Tao T . The Dantzig selector: statistical estimation when p is much larger than n. Ann Stat 2007;35 (6 ):2313–2351.
7. Mai Q , Zou H , Yuan M . A direct approach to sparse discriminant analysis in ultra-high dimensions. Biometrika 2012;99 (1 ):29–42.
8. Lin Y , Jeon Y . Discriminant analysis through a semiparametric model. Biometrika 2003;90 (2 ):379–392.
9. Han F , Zhao T , Liu H . Coda: high dimensional copula discriminant analysis. J Mach Learn Res 2013;14 (1 ):629–671.
10. Mai Q , Hui Z . Semiparametric sparse discriminant analysis in ultra-high dimensions. Statistics 2013;99 (1 ):29–42.
11. Jiang B , Leng C . High dimensional discrimination analysis via a semiparametric model. Stat Probab Lett 2016;110 (11 ):103–110.
12. He Y , Zhang X , Wang P . Discriminant analysis on high dimensional Gaussian copula model. Stat Probab Lett 2016;117 (5 ):100–112.
13. Liu J , Huang J , Ma S . Integrative analysis of multiple cancer genomic datasets under the heterogeneity model. Stat Med 2013;32 (20 ):3509–3521.23519988
14. Fan X , Fang K , Ma S , Zhang Q . Integrating approximate single factor graphical models. Stat Med 2020;39 (2 ):146–155.31749227
15. Ma S , Huang J , Song X . Integrative analysis and variable selection with multiple high-dimensional data sets. Biostatistics 2011;12 (4 ):763–775.21415015
16. Mo Q , Wang S , Seshan VE , Pattern discovery and cancer gene identification in integrated cancer genomic data. Proc Natl Acad Sci 2013;110 (11 ):4245–4250.23431203
17. Huang Y , Huang J , Shia BC , Ma S . Identification of cancer genomic markers via integrative sparse boosting. Biostatistics 2012;13 (3 ):509–522.22045909
18. Shen R , Wang S , Mo Q . Sparse integrative clustering of multiple omics data sets. Ann Appl Stat 2013;7 (1 ):269–294.24587839
19. Cai T , Cai T , Anru Z . Structured matrix completion with applications to genomic data integration. JAmStatAssoc 2016;111 (514 ):621–633.
20. Li Q , Li L . Integrative linear discriminant analysis with guaranteed error rate improvement. Biometrika 2018;105 (4 ):917–930.31762476
21. Ning Z , Pan W , Chen Y , Integrative analysis of cross-modal features for the prognosis prediction of clear cell renal cell carcinoma. Bioinformatics 2020;36 (9 ):2888–2895.31985775
22. Kim S , Lin CW , Tseng GC . MetaKTSP: a meta-analytic top scoring pair method for robust cross-study validation of omics prediction analysis. Bioinformatics 2016;32 (13 ):1966–1973.27153719
23. Wang Y , Huang T , Xie L , Liu L . Integrative analysis of methylation and transcriptional profiles to predict aging and construct aging specific cross-tissue networks. BMC Syst Biol 2016;10 (4 ):132.28155676
24. Uludag K , Roebroeck A . General overview on the merits of multimodal neuroimaging data fusion. NeuroImage 2014;102 (5 ):3–10.24845622
25. Zhang D , Wang Y , Zhou L , Yuan H , Shen D . Multimodal classification of Alzheimer’s disease and mild cognitive impairment. NeuroImage 2011;55 (3 ):856–867.21236349
26. Dai Z , Yan C , Wang Z , Discriminative analysis of early Alzheimer’s disease using multi-modal imaging and multi-level characterization with multi-classifier (M3). NeuroImage 2012;59 (3 ):2187–2195.22008370
27. Liu H , Lafferty J , Wasserman L . The nonparanormal: semiparametric estimation of high dimensional undirected graphs. J Mach Learn Res 2009;10 (3 ):2295–2328.
28. Simon N , Friedman J , Hastie T , Tibshirani R . A sparse-group lasso. J Comput Graph Stat 2013;22 (2 ):231–245.
29. Yuan M , Lin Y . Model selection and estimation in regression with grouped variables. J R Stat Soc Ser B Stat Methodol 2006;68 (1 ):49–67.
30. Clemmensen L , Hastie T , Witten D , Ersbøll B . Sparse discriminant analysis. Technometrics 2011;53 (4 ):406–413.
31. Bennett DA , Schneider JA , Arvanitakis Z , Wilson RS . Overview and findings from the religious orders study. Curr Alzheimer Res 2012;9 (6 ):628–645.22471860
32. Bennett DA , Schneider JA , Buchman AS , Barnes LL , Boyle PA , Wilson RS . Overview and findings from the rush memory and aging project. Curr Alzheimer Res 2012;9 (6 ):646–663.22471867
33. Allen M , Zou F , Chai HS , Novel late-onset Alzheimer disease loci variants associate with brain gene expression. Neurology 2012;79 (3 ):221–228.22722634
34. Chen H , He Y , Ji J , Shi Y . A machine learning method for identifying critical interactions between gene pairs in Alzheimer’s disease prediction. Front Neurol 2019;10 (10 ):1162.31736866
35. Xu W , Xu M , Wang L , Integrative analysis of DNA methylation and gene expression identified cervical cancer-specific diagnostic biomarkers. Signal Transduct Target Ther 2019;4 (1 ):1–11.30652029
36. Kriebel M , Ebel J , Battke F , Griesbach S , Volkmer H . Interference with complex IV as a model of age-related decline in synaptic connectivity. Front Mol Neurosci 2020;13 (3 ):43.32265651
37. Zhuang L , Liu X , Xu X , Association of the interleukin 1 beta gene and brain spontaneous activity in amnestic mild cognitive impairment. J Neuroinflammation 2012;9 (1 ):263.23199001
38. Griffin WS , Stanley LC , Ling C , Brain interleukin 1 and S-100 immunoreactivity are elevated in down syndrome and Alzheimer disease. Proc Natl Acad Sci 1989;86 (19 ):7611–7615.2529544
39. Yang YR , Kang DS , Lee C , Primary phospholipase C and brain disorders. Adv Biol Regul 2016;61 (11 ):80–85.26639088
40. Fan J , Liu H , Ning Y , Zou H . High dimensional semiparametric latent graphical model for mixed data. J R Stat Soc Ser B Stat Methodol 2017;79 (2 ):405–421.
41. He Y , Ji J , Xie L , Zhang X , Xue F . A new insight into underlying disease mechanism through semi-parametric latent differential network model. BMC Bioinform 2018;19 (17 ):493.
42. Fang H , Fang K , Kotz S . The meta-elliptical distributions with given marginals. J Multivar Anal 2002;82 (1 ):1–16.
