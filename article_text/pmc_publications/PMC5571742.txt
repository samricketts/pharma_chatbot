LICENSE: This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.


9216871
31978
Inf Process Med Imaging
Inf Process Med Imaging
Information processing in medical imaging : proceedings of the ... conference
1011-2499

28848302
5571742
10.1007/978-3-319-59050-9_16
NIHMS859866
Article
Predicting Interrelated Alzheimer's Disease Outcomes via New Self-Learned Structured Low-Rank Model
Wang Xiaoqian 1
Liu Kefei 23
Yan Jingwen 23
Risacher Shannon L. 2
Saykin Andrew J. 2
Shen Li 2
Huang Heng 1*
ADNI**
1 Computer Science &amp; Engineering, University of Texas at Arlington, TX, 76019, USA
2 Radiology &amp; Imaging Sciences, Indiana University School of Medicine, Indianapolis, IN, 46202, USA
3 BioHealth, Indiana University School of Informatics &amp; Computing, Indianapolis, IN, 46202, USA
* To whom correspondence should be addressed. At UTA, this work was partially supported by NIH R01 AG049371, NSF IIS 1302675, NSF IIS 1344152, NSF DBI 1356628, NSF IIS 1619308, NSF IIS 1633753. At IU, this work was partially supported by NIH R01 EB022574, R01 LM011360, U01 AG024904, P30 AG10133, and R01 AG19771
** Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_AcknowledgementList.pdf.

5 4 2017
23 5 2017
6 2017
01 6 2018
10265 198209
This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.

Alzheimer's disease (AD) is a progressive neurodegenerative disorder. As the prodromal stage of AD, Mild Cognitive Impairment (MCI) maintains a good chance of converting to AD. How to efficaciously detect this conversion from MCI to AD is significant in AD diagnosis. Different from standard classification problems where the distributions of classes are independent, the AD outcomes are usually interrelated (their distributions have certain overlaps). Most of existing methods failed to examine the interrelations among different classes, such as AD, MCI conversion and MCI non-conversion. In this paper, we proposed a novel self-learned low-rank structured learning model to automatically uncover the interrelations among different classes and utilized such interrelated structures to enhance classification. We conducted experiments on the ADNI cohort data. Empirical results demonstrated advantages of our model.

Alzheimer's Disease
MCI Conversion Prediction
Structured Low-Rank Model

1 Introduction

Alzheimer's Disease (AD) usually progresses along a temporal continuum, initially from a preclinical stage, subsequently to mild cognitive impairment (MCI) and ultimately deteriorating to AD [19]. As the transitional step between normal aging and dementia, MCI has attracted high attention since it provides promising opportunities for early detection of AD. MCI is recognized as a clinical state of individuals who are memory impaired but functioning well otherwise, which does not meet the clinical criteria for dementia [13]. According to [11], MCI patients preserve a conversion-to-AD rate of approximately 15% per year, thus it is of great importance to distinguish MCI patients with high potential of AD conversion from those not years before dementia.

Recent advances in neuroimaging have offered a helping hand for exploring associations between brain structure and behavior, which have provided effective features for early detection of AD[7,8]. In the past few years, several machine learning techniques have been applied to predict MCI conversion by means of neuroimaging data [12]. Researches utilized various classification models to identify MCI converters from other classes, e.g., health control samples and MCI non-converters by adopting neuroimaging data only in baseline time, which indicated a promising approach of ‚Äúforecasting‚Äù stage changes of MCI patients several years before the conversion happens. As successful early detection of MCI conversion can boost therapeutic intervention of AD to a large extent, studies on this topic have attracted high attention in recent time.

However, most existing models hold a simple and common assumption that the neuroimaging data is drawn from an unimodal distribution [11,12,18,17,16], which is not applicable for all occasions. In AD research, since MCI converters and AD eventually evolve to AD with certain common biological mechanism, it is reasonable to assume that these subjects share similar distribution patterns, but their distributions are distinct from that of health control samples. That is to say, the brain data may come from multimodal distribution, e.g., mixture of Gaussian. Thus, it is natural to assume latent group structure exists among different classes. Discovery of such subspace structure can enhance MCI conversion prediction and improve image biomarker discovery.

The most straightforward way to discover such groupwise interrelations is to cluster different data into groups before classification. However, since the clustering step is detached with the classification model, the learned interrelation structures are not associated to the prediction results. Such separated steps usually lead to suboptimal result. Here, we propose a novel structured low-rank learning model to simultaneously uncover the interrelations among different diagnostic stages and employ such interrelated structures to enhance the prediction of MCI conversion. We adopt Schatten p-norm to identify the shared low-rank subspace. Our new model is applied to the ADNI cohort for MCI conversion prediction. All empirical results show that the proposed classification model is capable of predicting MCI conversion with better performance.

2 Self-Learned Low-Rank Structured Classification Model

Multi-class classification problem with c classes can be seen as a multi-task learning problem with c tasks, where each task is to classify one class from all others via the one-vs-rest technique. Suppose these c tasks come from g groups, where tasks in each group are mutually related. We introduce and optimize a group index matrix set Q = {Q1, Q2, ‚Ä¶ Qg} to discover this group structure. Each Qi is a diagonal matrix with Qi ‚àà {0, 1}c√óc showing the assignment of tasks to the i-th group. For the (k, k)-th element of Qi, (Qi)kk = 1 means the k-th task belongs to the i-th group while (Qi)kk = 0 means not. To avoid overlap of groups, we have ‚àëi=1gQi=I.

Since each group of tasks share correlative dependence, we reasonably assume the latent subspace of each group maintains a low-rank structure. Schatten-p norm [10] can be used as a low-rank regularization for uncovering common subspaces shared by tasks.

For a matrix A ‚àà ‚Ñùd√ón, suppose œÉi is its i-th singular value, then the rank of A can be written as rank(A)=‚àëi=1min{d,n}œÉi0, where 00 = 0. The definition of p-th power Schatten p-norm (0 &lt; p &lt; ‚àû) is:

‚ÄñA‚ÄñSpp=Tr((ATA)p2)=‚àëi=1min{d,n}œÉip.

The well-known trace norm (a.k.a. nuclear norm) is a special case of Schatten p-norm with p = 1: ‚ÄñA‚Äñ‚àó=‚ÄñA‚ÄñS1=Tr((ATA)12)=‚àëi=1min{d,n}œÉi.

Obviously, when 0 &lt; p &lt; 1, Schatten p-norm makes a better approximation of rank(A) thus a more strict low-rank constraint than trace norm. The more closer p is 0, the more strict low-rank constraint the regularization term ‚ÄñA‚ÄñSpp imposes.

According to the above analysis, we can formulate our novel self-learned structured low-rank classification model as follows:

(1) minW,b,Qi|i=1g‚àà{0,1}c√óc,‚àëi=1gQi=IL(Y;X,W,b)+Œ≥‚àëi=1g(‚ÄñWQi‚ÄñSpp)k

In Problem (1), we use a general classification loss ‚Ñí(Y; X, W, b), which can be any loss function, e.g., logistic regression, hinge loss, etc. W ‚àà ‚Ñùd√óc is the weight matrix for classification, b ‚àà ‚ààc√ó1 is the bias, and Y ‚àà ‚Ñùn√óc is the label matrix. Moreover, we add a power parameter k to the Schatten p-norm regularization term for robustness of Problem (1), whose influence will be elaborately discussed in Section 4.

When 0 &lt; p &lt; 1, it is apparent that the new objective is non-convex thus difficult for optimization. In the next section, we adopt an efficient re-weighted optimization algorithm.

3 Optimization Algorithm

Here, we first introduce a re-weighted algorithm to solve a general problem where Problem (1) is a special case, and then talk about the detailed optimization of (1).

3.1 Optimization Algorithm for A General Problem

Lemma 1. Let gi(x) denote a general function over x, where x can be a scalar, vector or matrix, ùíû denotes the constraints on x, then we can claim:

When Œ¥ ‚Üí 0, The optimization problem

minx‚ààCf(x)+‚àëiTr((giT(x)gi(x))p2),

is equivalent to

minx‚ààCf(x)+‚àëiTr(giT(x)gi(x)Di),whereDi=p2(giT(x)gi(x)+Œ¥I)p‚àí22.

Proof: When Œ¥ ‚Üí 0, it's apparent that the optimization problem

(2) minx‚ààCf(x)+‚àëiTr((giT(x)gi(x)+Œ¥I)p2),

will reduce to

(3) minx‚ààCf(x)+‚àëiTr((giT(x)gi(x))p2).

So with a fairly small parameter Œ¥, we turn the non-smooth Problem (3) to the smooth Problem (2).

The Lagrangian function of Problem (2) is:

f(x)+‚àëiTr((giT(x)gi(x)+Œ¥I)p2)‚àíŒªI»ºC(x),

where ƒ®C(x) equals 0 if x ‚àà C and ‚àû otherwise [4]. Take derivative w.r.t. x and set it to zero. Based on the chain rule [2], we have:

(4) ‚àëiTr(2p2(giT(x)gi(x)+Œ¥I)p‚àí22giT(x)‚àÇgi(x))‚àÇx+f‚Ä≤(x)‚àíŒª‚àÇI»ºC(x)‚àÇx=0.

According to the Karush-Kuhn-Tucker conditions [4], if we can find a solution x that satisfies Eq. (4), then we usually find a local/global optimal solution to Problem (2). However, it is intractable to directly find the solution x that satisfies Eq. (4). Here we came up with a strategy as follows:

If Di=p2(giT(x)gi(x)+Œ¥I)p‚àí22 is a given constant, Eq. (4) can be reduced to

(5) f‚Ä≤(x)+‚àëiTr(2DgiT(x)‚àÇgi(x))‚àÇx‚àíŒª‚àÇI»ºC(x)‚àÇx=0.

Based on the chain rule [2], the optimal solution x* of Eq. (5) is also an optimal solution to the following problem:

(6) minx‚ààCf(x)+‚àëiTr(giT(x)gi(x)Di).

Based on this observation, we can first guess a solution x, next calculate Di based on the current solution x, and then update the current solution x by the optimal solution of Problem (6) on the basis of the calculated Di. We can iteratively perform this procedure until it converges.

3.2 Optimization of Problem (1)

It is obvious that Problem (1) can be optimized via Lemma 1. Noticing that QiQiT=Qi, our objective becomes:

(7) minWb,Qi|i=1g‚àà{0,1}c√óc,‚àëi=1gQi=IL(Y;X,W,b)+Œ≥‚àëi=1gTr(WQiWTDi),

where Di is defined as:

(8) Di=kp2(‚ÄñWQi‚ÄñSpp)k‚àí1(WQiWT+Œ¥I)p‚àí22,

with Œ¥ being a fairly small parameter close to zero.

We can solve Problem (7) by means of the alternating optimization method.

The first step is fixing W and solving Q, then Problem (7) becomes:

(9) minQi|i=1g‚àà{0,1}c√óc,‚àëi=1gQi=I‚àëi=1gTr((WTDiW)Qi),

Let Ai = WTDiW, then the solution of each Qi is evident as follows:

(10) Qi(l,l)={1,i=argminjAj(l,l)0,otherwise

The second step is fixing Q and solving W, b, then Problem (7) becomes:

(11) minW,bL(Y;X,W,b)+Œ≥‚àëi=1gTr(WQiWTDi).

Problem (11) can be solved according to the choice of the classification loss ‚Ñí(Y; X, W, b).

Here, we take an example to illustrate the optimization steps of Problem (11) when we adopt hinge loss for ‚Ñí(Y; X, W, b). Problem (11) can be written as:

(12) minW,bC‚àëi=1n‚àëj=1chij(1‚àíyij(wjTxi+bj))+12‚ÄñW‚ÄñF2+Œ≥‚àëi=1gTr(WQiWTDi).

where H ‚àà ‚Ñùn√óc is a slack variable defined as follow:

(13) hij={1,yij(wjTxi+bj)‚â§10,otherwise

Take derivative of Problem (12) w.r.t. b and set it to zero, then we get: ‚àëi=1nhijyij=0. which indicates that b can be updated according to the support vectors.

Take derivative of Problem (12) w.r.t. wj and set it to zero, then we get:

wj=C(I+2Œ≥(‚àëi=1gQi(j,j)Di))‚àí1‚àëi=1nhijyijxi.

We can iteratively update D, Q, W and b with the alternating steps mentioned above and the algorithm of Problem (7) is summarized in Algorithm 1.

Convergence and time analysis

Our algorithm as a whole employs the alternating optimization method to update variables, whose convergence has already been proved in [3]. Our model usually converges in 15 iterations. In our experiments on the ADNI data, the runtime for five-fold cross validation is around 3 seconds.

	
Algorithm 1 Algorithm to solve problem (7).	
	
Input:	
‚ÄÉImaging feature data X ‚àà ‚Ñùd√ón, label matrix Y ‚àà ‚Ñùn√óc, parameter p, k, Œ≥, group number g.	
Output:	
‚ÄÉWeight matrix W ‚àà ‚Ñùd√óc and g different group matrices Qi|i=1g‚àà‚Ñùc√óc which groups the c classes into g subspaces.	
‚ÄÉInitialize W by the optimal solution to the ridge regression problem.	
‚ÄÉInitialize Q randomly.	
‚ÄÉwhile not converge do	
‚ÄÉ‚ÄÉ1. Update D according to the definition in Eq. (8)	
‚ÄÉ‚ÄÉ2. Update Q according to the solution in Eq. (10)	
‚ÄÉ‚ÄÉ3. Update W and b by solving Problem (11). The solution differs w.r.t. the choice of loss function ‚Ñí(Y; X, W, b).	
‚ÄÉend while	
	

4 Discussion of Parameters

We introduced several hyper-parameters to make our model more general and adaptive to various circumstances. Here, we analyze the functionality of each parameter in detail.

In Problem (1), the parameter p is the norm parameter for the low-rank regularization term. It adjusts the stringency of the low-rank penalty. As is analyzed in previous section, Schatten p-norm makes a more strict low-rank constraint than trace norm when 0 &lt; p &lt; 1. The closer p is to 0, the more rigorous low-rank constraint the regularization term ‚ÄñM‚ÄñSpp imposes. But empirically we don't set p to a too small value since it makes the model contain too many local-minima thus is sensitive to noise and outliers.

The parameter k in Problem (1) is proposed to guarantee the robustness of our model. When p is small, the number of local solutions becomes more thus lead the model to be more sensitive to outliers. Under this condition, a larger k value will render the model more robust to outliers.

Let's take an intuitive example for understanding p and k: Suppose p ‚Üí 0, then in Problem (1) our regularization term approximates to ‚àëi=1g(rank(WQi))k. Assume rank(W) = 10 and there exist two latent low-rank subspaces that rank(WQ1) = 5 and rank(WQ2) = 5.

If k = 1, the real latent low-rank subspaces WQ1 and WQ2 give the minimum regularization value as 5 + 5 = 10. However, if we find some non-low-rank subspaces WQ3 and WQ4 instead, the regularization value is at most 10 + 10 = 20, which maintains no big difference from 10.

If k = 2, the optimal regularization value is 52 + 52 = 50, while WQ3 and WQ4 cause a value of 102 + 102 = 200, which is much larger than 50. In this case, the algorithm will favor the real low-rank subspaces. This is how a larger k value makes our model robust when p is small. According to our pre-experiments, we usually set k value in the range of [2, 3].

Parameter Œ≥ is use to balance the role of the low-rank penalty, which can be adjusted to accommodate different cases. Œ≥ can be set to any positive value.

When conducting the experiments, we did not spend too much time tuning the parameters. On the contrary, in order to fairly compare all methods, we simply set each parameter to a reasonable value, which is discussed in the next section. While these parameters introduced significant challenges in optimizing our objective, they make our model more flexible and adapt to different situations.

5 Experimental Results

5.1 Experimental Settings

In the classification experiment, we employed hinge loss in Problem (1). We compared with the following methods: Support Vector Machine with ‚Ñì1-norm loss (L1SVM) as baseline, k-Nearest Neighbors algorithm (KNN), Least Square SVM (LSSVM) [15] and SVM with with ‚Ñì2-norm loss (L2SVM). To apply SVM model to the multi-class classification problem, we adopted 1-vs-all mechanism. Besides, we compared with one state-of-art method conducting structured multi-task learning via trace norm regularization (TMTL) [9]. In TMTL model, we also used hinge loss to conduct classification. It is notable that TMTL makes a special case of our model (1) with p = 1 and k = 1.

In our experiments, we exploited the toolbox of LIBSVM [6] to implement both L1SVM and L2SVM. All participating data sets were normalized to the range of [0, 1] and randomly divided using 5-fold cross validation. We excavated the classification result in each fold and recorded the average in these 5 times repetition.

The evaluation of different methods was based on the percentage of correctly classified samples, i.e., classification accuracy. For KNN, we set k = 1. For all other methods using the hinge loss, we tuned the C parameter in the range of {10‚àí3, 10‚àí2, ‚Ä¶, 103} on training and validation data and recorded the performance on testing data using the best parameter w.r.t. each method.

Our model consists of several other parameters such as p, Œ≥, k and Œ¥. In our pre-experiments, we use cross-validation to find a reasonable range for each parameter. We found the performance of our model relatively stable within the reasonable range of parameters (data not shown). Indeed, we can further improve the performance with fine-tuning the parameters. Instead, we simply fix p = 0.25, Œ≥ = 1, k = 3 and Œ¥ = 10‚àí12 in the experiments. Unless specified otherwise, we set the number of groups as g = 2. The values of these parameters were determined according to the theoretical reasonable range discussed in Section 4 and empirical convention.

5.2 Description of ADNI Data

Data used in the preparation of this paper was obtained from the ADNI database (http://adni.loni.usc.edu). One goal of ADNI has been to test whether serial MRI, PET, other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of MCI and early AD. For up-to-date information, we refer interested readers to visit www.adni-info.org. We downloaded baseline 1.5 T MRI scans and demographic information for 818 ADNI-1 participants. For each baseline MRI scan, FreeSurfer [14] was employed for brain segmentation and cortical parcellation, and extracted 90 thickness and volume measures, which were pre-adjusted by intracranial volume (ICV) using the regression weights derived from the healthy control (HC) participants. Besides, we performed voxel-based morphometry (VBM) [14] on the MRI data, and extracted mean gray matter (GM) density measures for 90 target regions of interest (ROIs). The time points examined in this study for both imaging markers and diagnostic status included baseline (BL) and Month 36 (M36). All participants with no missing BL/M36 MRI measurements and diagnostic status were involved in this study. All in all, we include 516 sample subjects in our study, including 105 AD samples, and 237 MCI samples and 174 health control (HC) samples. Among the 237 MCI samples, 9 of them become HC in M36, 95 become AD in M36 while the rest 133 remain as MCI along this three-year continuum.

5.3 Performance Comparison on ADNI Cohort

We labeled the ADNI data according to a three-year clinical observation to five different classes, which are are: 1. health control (HC), 2. MCI(baseline)-HC(M36), 3. MCI(baseline)-MCI(M36), 4. MCI(baseline)-AD(M36) and 5. AD. Classification experiments were performed only on the baseline neuroimaging data so as to compare the ‚Äúforecasting‚Äù ability of different methods. Our goal is to classify these different classes using baseline data, i.e., detect MCI stage changes three years before the clinical diagnosis, which will make a contribution to therapeutic intervention of AD in the most effective stage. The comparison results are summarized in Table 1.

From Table 1, we found that our new method performs better than the counterparts in classifying the different classes using merely baseline data. Besides, we get two other interesting observations: 1) SVM methods outperforms KNN on the ADNI data; 2) L1SVM and L2SVM perform equal or better than LSSVM. The reason may go as follows: For KNN, it is a method focused more on the local data structure, while SVM model is meant to effectively find the separating hyperplanes, which is more suitable for classification. The unilateral loss is more interpretable and robust than bilateral loss for classification, thus we notice that L1SVM and L2SVM perform equal or better than LSSVM method. As for our proposed method, we utilized the unilateral hinge loss to be adaptive for classification and also automatically discovered the groupwise structure among different classes, which strengthened the classification performance. To compare our method with TMTL, even though both methods attempted to detect the groupwise structure among different tasks, our model is more general and robust. The use of Schatten p-norm and the power parameter k make our model better approximate the low-rank structure of the latent subspaces thus perform better.

It is also worth mentioning that in this classification, we only use neuroimaging data but not cognitive test information as previous papers do, e.g., [12]. In [12], the classification accuracy is over 70% by adding the cognitive test information to prediction. However, cognitive assessment is a direct diagnostic criterion of MCI and AD [1]. Predicting MCI with cognitive scores is like classifying with label information, which will definitely boost the performance. But using the cognitive test scores as features, the classification is no longer ‚Äúforecasing‚Äù but just a classification of existing information.

Moreover, we present the detected groupwise structure from TMTL and our method on VBM analysis in Fig. 1. It seems that TMTL fails to detect the appropriate group structure among the five classes, but put them all together in one group. On the contrary, our method successfully finds the intrinsic group information among different classes. Fig. 1 shows an interesting phenomenon that no matter what the group number g we set, our model always groups the five classes into two clusters. This illustrates that our model is able to find the intrinsic group structure regardless of g parameter settings. Also, according to the detected structure, we know that even though three different types of MCI patients i.e., class 2, 3 and 4, end up with 3 different stages in month 36, they adopt a similar pattern in the baseline. As a subdivision, MCI-AD shows a potential similarity with AD while the other two types of MCI obtain patterns like HC. Such detected group information may help with the diagnosis of MCI and AD.

5.4 Discussion on Top Ranked Features

In this section, let's take an insight into the results. We use heat maps and brain maps to intuitively indicate the degree of influence imposed by each imaging feature, such that important features in classification can be determined.

Shown in Fig. 3 and Fig. 2 are the heat maps of sorted neuroimaging feature weights and corresponding brain maps. The figures demonstrate the capture of a small set of features that are predominant for classification. Among the selected features, we found LHippoCampus and LPostCentral on the top, whose impact on AD have already been proved in the previous papers [5,20]. These identified imaging disease associations warrant further investigation in independent cohorts. If replicated, these findings can potentially contribute to biomarker discovery for diagnosis and drug design.

5.5 Experiments of Convergence Analysis

In this subsection, we empirically analyze the convergence of our algorithm with respect to the two parameters p and k in Eq. (1). We apply our method to the entire data with two different p values (i.e., 0.25 and 0.75) and two different k values (i.e., 2 and 3), then record the objective value of our model in each iteration.

We use the results on FreeSurfer as an example. The convergence plots are shown in Fig. 4. We notice that the number of iterations need before convergence is fairly stable with respect to the settings of p and k parameters. No matter what p and k values are, our model usually converges within 15 iterations.

6 Conclusions

In this paper, we proposed a novel low-rank structured classification model to predict MCI conversion using neuroimaging data in the baseline time. Our model simultaneously uncovered the interrelation structures existing in different classes and employed such structure to enhance the classification model. Moreover, we utilized Schatten p-norm to extract the common low-rank subspace shared by different patient classes. We conducted experiments on ADNI cohort. Empirical results validated the effectiveness of our model by demonstrating improved classification performance compared with competing methods. In addition, the top ranked biomarkers in our method were verified by previous literature, which indicated the potential contribution of our model to AD diagnosis and drug design.

Fig. 1 Illustration of the detected group structure among different classes in our method ((a) and (c)) and TMTL ((b) and (d)) in the VBM analysis. We set the number of groups to be 2 and 3, respectively. White blocks denote that a class belongs to a certain group while black block denote otherwise. The five classes are: 1. health control (HC), 2. MCI(baseline)-HC(M36), 3. MCI(baseline)-MCI(M36), 4. MCI(baseline)-AD(M36) and 5. AD.

Fig. 2 Neuroimaging features mapped on the brain for the FreeSurfer analysis.

Fig. 3 Heat maps of sorted neuroimaging feature weights in our method in descending order from left to right. The feature weight matrix is learned on the entire data.

Fig. 4 Objective function value of Eq. (1) with different k and p parameters in each iteration on the FreeSurfer data.

Table 1 Classification accuracy (%) comparison using ‚ÄúFreeSurfer‚Äù and ‚ÄúVBM‚Äù data

	KNN	L1SVM	LSSVM	L2SVM	TMTL	OURS	
VBM	33.21¬±8.00	43.66¬±1.73	44.14¬±5.64	44.15¬±5.61	42.55¬±3.37	44.79¬±4.46	
FreeSurfer	37.77¬±3.74	45.78¬±3.71	44.23¬±3.60	45.02¬±4.97	44.83¬±4.79	48.48¬±3.25	


1 Albert MS DeKosky ST Dickson D Dubois B Feldman HH Fox NC Gamst A Holtzman DM Jagust WJ Petersen RC The diagnosis of mild cognitive impairment due to alzheimer's disease: Recommendations from the national institute on aging-alzheimer's association workgroups on diagnostic guidelines for alzheimer's disease Alzheimer's &amp; Dementia 7 3 270 279 2011
2 Bentler P Lee SY Matrix derivatives with chain rule and rules for simple, hadamard, and kronecker products Journal of Mathematical Psychology 17 3 255 262 1978
3 Bezdek JC Hathaway RJ Convergence of alternating optimization Neural, Parallel &amp; Scientific Computations 11 4 351 368 2003
4 Boyd S Vandenberghe L Convex optimization Cambridge university press 2004
5 Cacabelos R Yamatodani A Niigawa H Hariguchi S Tada K Nishimura T Wada H Brandeis L Pearson J Brain histamine in alzheimer's disease Methods and findings in experimental and clinical pharmacology 11 5 353 360 1989 2755282
6 Chang CC Lin CJ LIBSVM: A library for support vector machines ACM Transactions on Intelligent Systems and Technology 2 27:1 27:27 2011 software available at http://www.csie.ntu.edu.tw/‚àºcjlin/libsvm
7 Devanand D Pradhaban G Liu X Khandji A De Santi S Segal S Rusinek H Pelton G Honig L Mayeux R Hippocampal and entorhinal atrophy in mild cognitive impairment prediction of alzheimer disease Neurology 68 11 828 836 2007 17353470
8 Hua X Leow AD Parikshak N Lee S Chiang MC Toga AW Jack CR Weiner MW Thompson PM Initiative ADN Tensor-based morphometry as a neuroimaging biomarker for alzheimer's disease: an mri study of 676 ad, mci, and normal subjects Neuroimage 43 3 458 469 2008 18691658
9 Kang Z Grauman K Sha F Learning with whom to share in multi-task feature learning Proceedings of the 28th International Conference on Machine Learning (ICML-11) 521 528 2011
10 Kittaneh F Inequalities for the schatten p-norm Glasgow Mathematical Journal 26 02 141 143 1985
11 Misra C Fan Y Davatzikos C Baseline and longitudinal patterns of brain atrophy in mci patients, and their use in prediction of short-term conversion to ad: results from adni Neuroimage 44 4 1415 1422 2009 19027862
12 Moradi E Pepe A Gaser C Huttunen H Tohka J Initiative ADN Machine learning framework for early mri-based alzheimer's conversion prediction in mci subjects NeuroImage 104 398 412 2015 25312773
13 Petersen R Stevens J Ganguli M Tangalos E Cummings J DeKosky S Practice parameter: Early detection of dementia: Mild cognitive impairment (an evidence-based review) report of the quality standards subcommittee of the american academy of neurology Neurology 56 9 1133 1142 2001 11342677
14 Shen L Kim S Risacher SL Nho K Swaminathan S West JD Foroud T Pankratz N Moore JH Sloan CD Whole genome association study of brain-wide imaging phenotypes for identifying quantitative trait loci in mci and ad: A study of the adni cohort Neuroimage 53 3 1051 1063 2010 20100581
15 Suykens JA Van Gestel T De Brabanter J De Moor B Vandewalle J Suykens J Van Gestel T Least squares support vector machines 4 World Scientific 2002
16 Wang H Nie F Huang H Risacher S Ding C Saykin AJ Shen L ADNI: Sparse multi-task regression and feature selection to identify brain imaging predictors for memory performance IEEE Conference on Computer Vision 557 562 2011
17 Wang H Nie F Huang H Risacher S Saykin AJ Shen L ADNI: Joint classification and regression for identifying ad-sensitive and cognition-relevant imaging biomarkers The 14th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI) 115 123 2011
18 Wang H Nie F Huang H Risacher SL Saykin AJ Shen L ADNI Identifying disease sensitive and quantitative trait relevant biomarkers from multi-dimensional heterogeneous imaging genetics data via sparse multi-modal multi-task learning Bioinformatics 28 12 i127 i136 2012 22689752
19 Wenk GL Neuropathologic changes in alzheimer's disease Journal of Clinical Psychiatry 64 7 10 2003
20 West MJ Coleman PD Flood DG Troncoso JC Differences in the pattern of hippocampal neuronal loss in normal ageing and alzheimer's disease The Lancet 344 8925 769 772 1994
