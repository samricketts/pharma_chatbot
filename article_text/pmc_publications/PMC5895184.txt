LICENSE: This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.


9607483
22834
Adv Neural Inf Process Syst
Adv Neural Inf Process Syst
Advances in neural information processing systems
1049-5258

29657513
5895184
NIHMS939429
Article
Regularized Modal Regression with Applications in Cognitive Impairment Prediction
Wang Xiaoqian 1
Chen Hong 1
Cai Weidong 2
Shen Dinggang 3
Huang Heng 1
1 Department of Electrical and Computer Engineering, University of Pittsburgh, USA
2 School of Information Technologies, University of Sydney, Australia
3 Department of Radiology and BRIC, University of North Carolina at Chapel Hill, USA
X. Wang and H. Chen made equal contributions to this paper.

2 2 2018
12 2017
11 4 2018
30 14481458
This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.
Linear regression models have been successfully used to function estimation and model selection in high-dimensional data analysis. However, most existing methods are built on least squares with the mean square error (MSE) criterion, which are sensitive to outliers and their performance may be degraded for heavy-tailed noise. In this paper, we go beyond this criterion by investigating the regularized modal regression from a statistical learning viewpoint. A new regularized modal regression model is proposed for estimation and variable selection, which is robust to outliers, heavy-tailed noise, and skewed noise. On the theoretical side, we establish the approximation estimate for learning the conditional mode function, the sparsity analysis for variable selection, and the robustness characterization. On the application side, we applied our model to successfully improve the cognitive impairment prediction using the Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI) cohort data.


1 Introduction

Modal regression [21, 5] has gained increasing attention recently due to its effectiveness on function estimation and robustness to outliers and heavy-tailed noise. Unlike the traditional least-square estimator pursuing the conditional mean, modal regression aims to estimate the conditional mode of output Y given the input X = x. It is well known that the conditional modes can reveal the structure of outputs and the trends of observation, which is missed by the conditional mean [29, 4]. Thus, modal regression often achieves better performance than the traditional least square regression in practical applications.

There are some studies for modal regression with (semi-)parametric or nonparametric methods, such as [29, 28, 4, 6]. For parametric approaches, a parametric form is required for the global conditional mode function. Recent works in [29, 28] belong to this category, where the method in [28] is based on linear mode function assumption and the algorithm in [29] is associated with the local polynomial regression. For non-parametric approaches, the conditional mode is usually derived by maximizing a conditional density or a joint density. Typical work for this setting is established in [4], where a local modal regression is proposed based on kernel density estimation and theoretical analysis is provided to characterize asymptotic error bounds.

Most of the above mentioned works consider the asymptotic theory on the conditional mode function estimation. Recently, several studies on variable selection under modal regression were also conducted in [30, 27]. These approaches addressed the problem from statistical theory viewpoint (e.g., asymptotic normality) and were implemented by modified EM algorithm. Although these studies provide us good understanding for modal regression, the following problems still remain unclear in theory and applications. Can we design new modal regression following the line of structural risk minimization? Can we provide its statistical guarantees and computing algorithm for designed model? This paper focuses on answering the above questions.

To illustrate the effectiveness of our model, we looked into a practical problem, i.e., cognitive impairment prediction via neuroimaging data. As the most common cause of dementia, Alzheimerâ€™s Disease (AD) imposes extensive and complex impact on human thinking and behavior. Accurate and automatic study of the relationship between brain structural changes and cognitive impairment plays a crucial role in early diagnosis of AD. In order to increase the diagnostic capabilities, neuroimaging provides an effective approach for clinical detection and treatment response monitoring of AD [13]. Several cognitive tests were presented to assess the individualâ€™s cognitive level, such as Mini-Mental State Examination (MMSE) [8] and Trail Making Test (TMT) [1]. With the development of these techniques, a wide range of work employed regression models to study the correlations between neuroimaging data and cognitive measures [23, 16, 26, 25, 24].

However, existing methods use mean regression models based on the least-square estimator to predict the relationship between neuroimaging features and cognitive assessment, which may fail when the noise in the data is heavy-tailed or skewed. According to the complex data collection process [13], the assumption of symmetric noise may not be guaranteed in biomedical data. Under such a circumstance, modal regression model proves to be more appropriate due to its robustness to outliers, heavy-tailed noise, and skewed noise. We applied our method to the ADNI cohort for the association study between neuroimaging features and cognitive assessment. Experimental results illustrated the effectiveness of our model. Moreover, with sparse constraints, our model found several imaging features that have been reported to be crucial to the onset and progression of AD. The replication of these results further support the validity of our model.

Our main works can be summarized as below:

Following the Tikhonov regularization and kernel density estimation, we develop a new Regularized Modal Regression (RMR) for estimating the conditional mode function and selecting informative variables, which can be considered as a natural extension of Lasso [22] and can be implemented efficiently by half-quadratic minimization methods.

Learning theory analysis is established for RMR from three aspects: approximation ability, sparsity, and robustness, which provide the theoretical foundations of the proposed approach.

By applying our RMR model to the ADNI cohort, we reveal interesting findings in cognitive impairment prediction of Alzheimerâ€™s disease.

2 Regularized Modal Regression

2.1 Modal regression

We consider learning problem with input space ğ’³ âŠ‚ â„p and output space ğ’´ âŠ‚ â„. Let pY|X=x be the conditional density of Y âˆˆ ğ’´ for given X = x âˆˆ ğ’³. In the prediction of cognitive assessment, we denote the neuroimaging data for the i-th sample as xi and the cognitive measure for the i-th sample as yi. Suppose that training samples z={(xi,yi)}i=1nâŠ‚XÃ—Y are generated independently by: (1) Y=fâˆ—(X)+Îµ,

where mode(Îµâˆ£X=x)=argmaxtpÎµâˆ£X(tâˆ£X=x)=0 for any x âˆˆ ğ’³. Here, pÎµ|X, as the conditional density of Îµ conditioned on X, is well defined. Then, the target function of modal regression can be written as: (2) fâˆ—(x)=mode(Yâˆ£X=x)=argmaxtpYâˆ£X(tâˆ£X=x),âˆ€xâˆˆX.

To assure f* is well defined on ğ’³, we require that the existence and uniqueness of pY|X(t|X = x) for any given x âˆˆ ğ’³. The relationship (2) means f* is the maximum of the conditional density pY|X, and also equals to maximize the joint density pX,Y [4, 29, 28]. Here, we formulate the modal regression following the dimension-insensitive statistical learning framework [7].

For feasibility, we denote Ï on ğ’³ Ã— ğ’´ as the intrinsic distribution for data generated by (1), and denote Ïğ’³ as the corresponding marginal distribution on ğ’³. It has been proved in Theorem 3 [6] that f* is the maximizer of

(3) R(f)=âˆ«XpYâˆ£X(f(x)âˆ£X=x)dÏX(x)

over all measurable function. Hence, we can adopt â„›(f) as the evaluation measure of modal regression estimator f : ğ’³ â†’ â„. However, we can not get the estimator directly by maximizing this criterion since pY|X and Ïğ’³ are unknown. Recently, Theorem 5.1 in [6] shows â„›(f) = pÎµf(0), where pÎµf is the density function of random variable Îµf = Y âˆ’ f(X). Then, the problem of maximizing â„›(f) over some hypothesis spaces can be transformed to maximize the density of Îµf at 0. This density pÎµf can be estimated by nonparametric kernel density estimation.

For a kernel KÏƒ : â„ Ã— â„ â†’ â„+, we denote its representing function Ï•(u-uâ€²Ïƒ)=KÏƒ(u,uâ€²), which usually satisfies Ï•(u) = Ï•(âˆ’u), Ï•(u) â‰¤ Ï•(0) for any u âˆˆ â„ and âˆ«â„ Ï•(u)du = 1. Typical examples of kernel include Gaussian kernel, Epanechnikov kernel, quadratic kernel, triwight kernel, and sigmoid function. The empirical estimation of â„›(f) (also pÎµf (0)) can be obtained by kernel density estimation, which is defined as: RzÏƒ(f)=1nÏƒâˆ‘i=1nKÏƒ(yi-f(xi),0)=1nÏƒâˆ‘i=1nÏ•(yi-f(xi)Ïƒ),

Hence, the approximation of f* can be found by learning algorithms associated with RzÏƒ(f). In theory, for any f : ğ’³ â†’ â„, the expectation version of RzÏƒ(f) is: RÏƒ(f)=1Ïƒâˆ«XÃ—YÏ•(y-f(x)Ïƒ)dÏ(x,y).

In particular, there holds â„›(f) âˆ’ â„›Ïƒ(f) â†’ 0 as Ïƒ â†’ 0 [6].

2.2 Modal regression with coefficient-based regularization

In this paper, we assume that fâˆ—(x)=mode(Yâˆ£X=x)=wâˆ—Tx for some w* âˆˆ â„p. Following the ideas of ridge regression and Lasso [22], we consider the robust linear estimator for learning the conditional mode function.

Let â„± be a linear hypothesis space defined by: F={f(x)=wTx:w=(w1,â€¦,wp)âˆˆâ„p,xâˆˆX}.

For any given positive tuning parameters {Ï„j}j=1p, we denote: Î©(f)=inf{âˆ‘j=1pÏ„jâˆ£wjâˆ£q:f(x)=wTx,qâˆˆ[1,2]}.

Given training set z, the regularized modal regression (RMR) can be formulated as below: (4) fz=argmaxfâˆˆF{RzÏƒ(f)-Î»Î©(f)},

where regularization parameter Î» &gt; 0 is used to balance the modal regression measure and hypothesis space complexity. It is easy to deduce that fz(x)=wzTx with

(5) wz=argmaxwâˆˆâ„p{1nÏƒâˆ‘i=1nÏ•(yi-wTxiÏƒ)-Î»âˆ‘j=1pÏ„jâˆ£wjâˆ£q}.

When Ï„j â‰¡ 1 for 1 â‰¤ j â‰¤ p and q = 1, (5) can be considered as an natural extension of Lasso in [22] from learning the conditional mean function to estimating the conditional mode function. When Ï„j â‰¡ 1 for 1 â‰¤ j â‰¤ p and q = 2, (5) also can be regarded as the corresponding version of ridge regression by replacing the MSE criterion with modal regression criterion. In particular, when KÏƒ is Gaussian kernel and Ï„j â‰¡ 1 for 1 â‰¤ j â‰¤ p, (5) can be rewritten as: wz=argmaxwâˆˆâ„p{1nÏƒâˆ‘i=1nexp{(yi-wTxi)2Ïƒ2}-Î»â€–wâ€–qq},

which is equivalent to correntropy regression under maximum correntropy criterion [19, 9, 7].

2.3 Optimization algorithm

We employ the half-quadratic (HQ) theory [18] in the optimization. For a convex problem minsu(s), it is equivalent to solve the following half-quadratic reformulation: mins,tQ(s,t)+v(t),

where Q(s, t) is quadratic for any t âˆˆ â„ and v : â„ â†’ â„ satisfies: u(s)=mintQ(s,t)+v(t),âˆ€sâˆˆâ„.

Such a dual potential function v can be determined via convex conjugacy as shown below.

According to the convex optimization theory [20], for a closed convex function f(a), there exists a convex function g(b), such that: f(a)=maxb(ab-g(b)),

where g is the conjugate of f, i.e., g = fâ˜…. Symmetrically, it is easy to prove f = gâ˜….

Theorem 1

For a closed convex function f(a)=maxb(ab-g(b)), we have argmaxb(ab-g(b))=fâ€²(a) for any a âˆˆ â„.

When KÏƒ is Gaussian kernel, the optimization steps can be found in [9]. Here we take Epanechnikov kernel (a.k.a., parabolic kernel) as an example to show the optimization of Problem (5) via HQ theory. The kernel-induced representing function of Epanechnikov kernel is Ï•(e)=34(1-e2)ğŸ™[âˆ£eâˆ£â‰¤1].

Define a closed convex function f as: f(a)={34(1-a),0â‰¤aâ‰¤10,aâ‰¥1.

There exists a convex function g such that f(a)=maxb(ab-g(b)) and Ï•(e)=f(e2)=maxb(e2b-g(b)). Thus, when Ï„j â‰¡ 1 for 1 â‰¤ j â‰¤ p, the optimization problem (5) can be rewritten as: (6) maxwâˆˆâ„p,bâˆˆâ„n{1nÏƒâˆ‘i=1n(bi(yi-wTxiÏƒ)2-g(bi))-Î»âˆ‘j=1pÏ„jâˆ£wjâˆ£q}.

Problem (6) can be easily optimized via alternating optimization algorithm. Note that according to Theorem 1, when w is fixed, b can be updated as bi=fâ€²((yi-wTxiÏƒ)2)=-34ğŸ™[âˆ£yi-wTxiÏƒâˆ£â‰¤1] for i = 1, 2, . . . , n. For the space limitation, we provide the proof of Theorem 1 and the optimization steps of RMR in the supplementary material.

3 Learning Theory Analysis

This section presents the theoretical foundations of RMR from approximation ability, variable sparsity, and algorithmic robustness. Detail proofs of these results can be found in the supplementary material.

3.1 Approximation ability analysis

Besides the linear requirement for the conditional mode function, we also need some basic conditions on the kernel-induced representing function Ï• [6, 28].

Assumption 1

The representing function Ï• satisfies the following conditions: 1) âˆ€u âˆˆ â„, Ï•(u) â‰¤ Ï•(0) &lt; âˆ, 2) Ï• is Lipschitz continuous with constant LÏ•, 3) âˆ«â„ Ï•(u)du = 1 and âˆ«â„u2 Ï•(u)du &lt; âˆ.

It is easy to verify that most of kernels used for density estimation satisfy the above conditions, e.g., Gaussian kernel, Epanechnikov kernel, quadratic kernel, etc. Since RMR is associated with RzÏƒ(f), we need to establish quantitative relationship between â„›Ïƒ(f) and â„›(f). Recently, the modal regression calibration has been illustrated in Theorem 10 [6] under the following restrictions on the conditional density pÎµ|X.

Assumption 2

The conditional density pÎµ|X is second-order continuously differentiable and uniform bounded.

Now, we present the approximation bound on â„›(f*) âˆ’ â„›(fz).

Theorem 2

Let â€–xâ€–qq-1â‰¤a for q âˆˆ (1, 2] for any x âˆˆ ğ’³ and f* âˆˆ â„±. Under Assumptions 1â€“2, for q âˆˆ (1, 2], by taking Î»=Ïƒ2=O(n-q4q+3), we have: R(fâˆ—)-R(fz)â‰¤Clog(4/Î´)n-q4q+3

with confidence at least 1 âˆ’ Î´ In particular, for q = 1 and ||x||âˆ â‰¤ a, choosing Î»=Ïƒ2=(lnpn)17, we have: R(fâˆ—)-R(fz)â‰¤Clog(4/Î´)(lnpn)17

with confidence at least 1 âˆ’ Î´, Here C1, C2 is a constant independent of n, Î´.

Theorem 2 shows that the excess risk of â„›(f*) âˆ’ â„›(fz) â†’ 0 with the polynomial decay and the estimation consistency is guaranteed as n â†’ âˆ. Moreover, under Assumption 3 in [6], we can derive fz tends to f* with approximation order O(n-q4q+3) for q âˆˆ (1, 2] and O(lnpn)17) for q = 1. Although approximation analysis has been provided for modal regression in [6, 28], both of them are limited to the empirical risk minimization. This is different from our result for regularized modal regression under structural risk minimization.

3.2 Sparsity analysis

To characterize the variable selection ability of RMR, we first present the properties for nonzero component of wz.

Theorem 3

Assume that Ï• is differentiable for any t âˆˆ â„. For j âˆˆ {1, 2, ..., p} satisfying wzj â‰  0, there holds: |1nÏƒ2âˆ‘i=1nÏ•â€²(yi-fz(xi)Ïƒ)xij|=pÎ»Ï„jâˆ£wzjâˆ£p-12.

Observe that the condition on Ï• holds true for Gaussian kernel, sigmoid function, and logistic function. Theorem 3 demonstrates the necessary condition for the non-zero wzj. Without loss of generality, we set S0 = {1, 2, ..., p0} as the index set of truly informative variables and denote Sz = {j : wzj â‰  0} as the set of identified informative variables by RMR in (4).

Theorem 4

Assume that ||x||âˆ â‰¤ a for any x âˆˆ ğ’³ and Î»Ï„j â‰¥ ||Ï•â€²||âˆÏƒ for any j &gt; p0. Then, for RMR (4) with q = 1, there holds Sz âŠ‚ S0 for all z âˆˆ (ğ’³ Ã— ğ’´)n.

Theorem 4 assures that RMR has the capacity to identify the truly informative variable in theory. Combining Theorem 4 and Theorem 2, we provide the asymptotic theory of RMR on estimation and model selection.

3.3 Robustness analysis

To quantify the robustness of RMR, we calculate its finite sample breakdown point, which reflects the largest amount of contamination points that an estimator can tolerate before returning arbitrary values [11, 12]. Recently, this index has been used to investigate the robustness of modal linear regression [28] and kernel-based modal regression [6].

Recall that the derived weight wz defined in (5) is dependent on any given sampling set z={(xi,yi)}i=1n. By adding m arbitrary points zâ€²={(xn+j,yn+j)}j=1mâŠ‚XÃ—Y, we obtain the corrupted sample set z âˆª zâ€². For given Î», Ïƒ, {Ï„j}j=1p, we denote wz âˆª zâ€² be the maximizer of (5). Then, the finite sample breakdown point of wz is defined as: Îµ(wz)=min1â‰¤mâ‰¤n{mn+m:supzâ€²â€–wzâˆªzâ€²â€–2=âˆ}.

Theorem 5

Assume that Ï•(u) = Ï•(âˆ’u) and Ï•(t) â†’ 0 as t â†’ âˆ. For given Î», Ïƒ, {Ï„j}j=1p, we denote: M=1Ï•(0)âˆ‘i=1nÏ•(yâˆ¼i-fz(xi))Ïƒ)-Î»Ïƒ(Ï•(0))-1Î©(fz).

Then the finite sample breakdown point of wz in (5) is Îµ(wz)=mâˆ—n+mâˆ—, where m* â‰¥ âŒˆMâŒ‰ and âŒˆMâŒ‰ is the smallest integer not less than M.

From Theorem 5, we know that the finite breakdown point of RMR depends on Ï•, Ïƒ, and the sample configuration, which is similar with re-descending M-estimator and recent analysis for modal linear regression in [28]. As illustrated in [11, 12], the finite sample breakdown point is high when the bandwidth Ïƒ only depends on the training samples. Hence, RMR can achieve satisfactory robustness when Î», Ï„j are chosen properly and Ïƒ is determined by data-driven techniques.

4 Experimental Analysis

In this section, we conduct experiments on both toy data, benchmark data as well as the ADNI cohort data to evaluate our RMR model. We compare several regression methods in the experiments, including: LSR (traditional mean regression based on the least square estimator), LSR-L2 (LSR with squared â„“2-norm regularization, i.e., ridge regression) LSR-L1 (LSR with â„“1-norm regularization), MedianR (median regression), HuberR (regression with huber loss), RMR-L2 (RMR with squared â„“2-norm regularization), and RMR-L1 (RMR with â„“1-norm regularization).

For evaluation, we calculate root mean square error (RMSE) between the predicted value and ground truth in out-of-sample prediction. The RMSE value is normalized via Frobenius norm of the ground truth matrix. We employ 2-fold cross validation and report the average performance for each method. For each method, we set the hyper-parameter of the regularization term in the range of {10âˆ’4, 10âˆ’3.5, . . . , 104}. We tune the hyper-parameters via 2-fold cross validation on the training data and report the best parameter w.r.t. RMSE of each method. For RMR methods, we adopt the Epanechnikov kernel and set the bandwidth as Ïƒ = max(|y âˆ’ wTx|).

4.1 Performance comparison on toy data

Following the design in [28], we generate the toy data by sampling i.i.d. from the model: Y = âˆ’2 + 3X + Ï„(X)Îµ, where X ~ ğ’°(0, 1), Ïƒ(X) = 1 + 2X and Îµ ~ 0.5ğ’©(âˆ’2, 32) + 0.5ğ’© (2, 12). We can derive that E(Îµ) = 0, Mode(Îµ) = 1.94 and Median(Îµ) = 1, hence the conditional mean regression function of the toy data is E(Y|X) = âˆ’2 + 3X, the conditional median function is Median(Y|X) = 1 + 5X, while the conditional mode is Mode(Y|X) = âˆ’0.06 + 6.88X.

We consider three different number of samples: 100,200,500, and repeat the experiments 100 times for each setting. We present the RMSE in Table 1, which shows that RMR models get lower RMSE values than all comparing methods. It indicates that RMR models make better estimation of the output when the noise in data is skewed and relatively heavy-tailed. Moreover, we compare the coverage probabilities for prediction intervals centered around the predicted value from each method. We set the length of coverage intervals to be {0.1Î½, 0.2Î½, 0.3Î½} respectively with Î½ = 3 being the approximate standard error of Îµ. From Table 2 we can find that RMR models provide larger coverage probabilities than the counterparts.

4.2 Performance comparison on benchmark data

Here we present the comparison results on six benchmark datasets from UCI repository [15] and StatLib2, which include: slumptest, forestfire, bolts, cloud, kidney, and lupus. We summarize the results in Table 3. From the comparison we notice that RMR models tend to perform better on all datasets. Also, RMR-L1 obtains lower RMSE value since the RMR-L1 model is more robust with the â„“1-norm regularization term.

4.3 Performance comparison on the ADNI cohort data

Now we look into a practical problem in Alzheimerâ€™s disease, i.e., prediction of cognitive scores via neuroimaging features. Data used in this article were obtained from the ADNI database (adni. loni.usc.edu). We extract 93 regions of interest (ROIs) as neuroimaging features and use cognitive scores from three tests: Fluency Test, Alzheimerâ€™s Disease Assessment Scale (ADAS) and Trail making test (TRAILS). 795 sample subjects were involved in our study, including 180 AD samples, 390 MCI samples and 225 normal control (NC) samples. Detailed data description can be found in the supplementary material.

Our goal is to construct an appropriate model to predict cognitive performance given neuroimaging data. Meanwhile, we expect the model to illustrate the importance of different features in the prediction, which is fundamental to understanding the role of each imaging marker in the study of AD. From Table 4, we find that RMR models always perform equal or better than the comparing methods, which verifies that RMR is more appropriate to learn the association between neuroimaging markers and cognitive performance. We can notice that RMR-L2 always performs better than LSR-L2, and RMR-L1 outperforms LSR-L1. This is because the symmetric noise assumption in least square models may not be guaranteed on the ADNI cohort. Compared with HuberR, our RMR model is shown to be less sensitive to outliers. Moreover, from the comparison between MedianR and RMR models, we can infer that conditional mode is more suitable than conditional median for the prediction of cognitive scores.

RMR-L1 imposes sparse constraints on the learnt weight matrix, which naturally achieves the goal of feature selection in the association study. Here we take TRAILS cognitive assessment as an example and look into the important neuroimaging features in the prediction. From the heat map and brain map in Fig. 1 and 2, we obtain several interesting findings. In the prediction, temporal lobe white matter has been picked out as a predominant feature. [10, 2] reported decreased fractional anisotropy (FA) and increased radial diffusivity (DR) in the white matter of the temporal lobe among AD and Mild Cognitive Impairment (MCI) subjects. [10] also revealed the correlation between temporal lobe FA and episodic memory, which may account for the influence of temporal lobe to TMT results. Besides, there is evidence in [17] supporting the association between left temporal lobe and the working memory component involving letters and numbers in TMT. Moreover, angular gyrus indicates high correlation with TRAILS scores in our analysis. Previous research has revealed that angular gyrus share many clinical features with AD. [14] presented structural MRI findings showing more left anular gyrus in MCI converters than non-converters, which pointed out the role of atrophy of structures like angular gyrus in the progression of dementia. [3] showed evidence for the role of angular gyrus in orienting spatial attention, which serves as a key factor in TMT results. The replication of these results supports the effectiveness of our model.

5 Conclusion

This paper proposes a new regularized modal regression and establishes its theoretical foundations on approximation ability, sparsity, and robustness. These characterizations fill in the theoretical gaps for modal regression under Tikhonov regularization. Empirical results verify the competitive performance of the proposed approach on simulated data, benchmark data and real biomedical data. With the sparsity property of our model, we identified several biological meaningful neuroimaging markers, showing the potential to enhance the understanding of onset and progression of AD.

Supplementary Material

Sup

This work was partially supported by U.S. NSF-IIS 1302675, NSF-IIS 1344152, NSF-DBI 1356628, NSF-IIS 1619308, NSF-IIS 1633753, NIH AG049371. Hong Chen was partially supported by National Natural Science Foundation of China (NSFC) 11671161. We are grateful to the anonymous NIPS reviewers for the insightful comments.

Figure 1 Heatmap showing the weights of each neuroimaging feature via RMR-L1 model for the prediction of TRAILS cognitive measures. We draw two matrices, where the upper figure is for the left hemisphere and the lower figure for the right hemisphere. Imaging markers (columns) with larger weights indicate higher correlation with corresponding cognitive measure in the prediction.

Figure 2 Cortical maps of ROIs identified in RMR-L1 model for the prediction of TRAILS cognitive measures. The brain maps show one slice of multi-view. The three maps correspond to three different cognitive measures in TRAILS cognitive test, respectively.

Table 1 Average RMSE and standard deviation with different number (n) of toy samples.

	n=100	n=200	n=500	
LSR	0.9687Â±0.0699	0.9477Â±0.0294	0.9495Â±0.0114	
LSR-L2	0.9671Â±0.0685	0.9469Â±0.0284	0.9495Â±0.0114	
LSR-L1	0.9672Â±0.0685	0.9473Â±0.0288	0.9495Â±0.0114	
MedianR	0.9944Â±0.0806	0.9568Â±0.0350	0.9542Â±0.0120	
HuberR	0.9725Â±0.0681	0.9485Â±0.0296	0.9502Â±0.0116	
RMR-L2	0.9663Â±0.0683	0.9466Â±0.0282	0.9493Â±0.0114	
RMR-L1	0.9662Â±0.0679	0.9465Â±0.0281	0.9492Â±0.0114	

Table 2 Average coverage possibilities and standard deviation on toy data.

		n=100	n=200	n=500	
0.1Î½	LSR	0.0730Â±0.0247	0.0702Â±0.0166	0.0702Â±0.0106	
LSR-L2	0.0753Â±0.0247	0.0731Â±0.0155	0.0709Â±0.0108	
LSR-L1	0.0747Â±0.0246	0.0719Â±0.0161	0.0706Â±0.0106	
MedianR	0.0563Â±0.0255	0.0626Â±0.0124	0.0654Â±0.0097	
HuberR	0.0710Â±0.0258	0.0698Â±0.0160	0.0694Â±0.0101	
RMR-L2	0.0760Â±0.0254	0.0740Â±0.0161	0.0719Â±0.0111	
RMR-L1	0.0760Â±0.0255	0.0742Â±0.0156	0.0720Â±0.0111	
0.2Î½	LSR	0.1313Â±0.0338	0.1450Â±0.0255	0.1430Â±0.0193	
LSR-L2	0.1337Â±0.0334	0.1461Â±0.0251	0.1429Â±0.0196	
LSR-L1	0.1337Â±0.0337	0.1458Â±0.0258	0.1430Â±0.0193	
MedianR	0.1087Â±0.0351	0.1331Â±0.0239	0.1377Â±0.0182	
HuberR	0.1237Â±0.0347	0.1442Â±0.0257	0.1421Â±0.0188	
RMR-L2	0.1340Â±0.0336	0.1477Â±0.0256	0.1441Â±0.0199	
RMR-L1	0.1343Â±0.0340	0.1481Â±0.0247	0.1441Â±0.0198	
0.3Î½	LSR	0.1923Â±0.0402	0.2142Â±0.0342	0.2150Â±0.0229	
LSR-L2	0.1940Â±0.0415	0.2165Â±0.0331	0.2156Â±0.0222	
LSR-L1	0.1940Â±0.0415	0.2153Â±0.0334	0.2153Â±0.0226	
MedianR	0.1750Â±0.0414	0.2031Â±0.0299	0.2095Â±0.0233	
HuberR	0.1873Â±0.0389	0.2132Â±0.0333	0.2144Â±0.0224	
RMR-L2	0.1943Â±0.0420	0.2179Â±0.0327	0.2168Â±0.0220	
RMR-L1	0.1950Â±0.0406	0.2177Â±0.0323	0.2167Â±0.0219	

Table 3 Average RMSE and standard deviation on benchmark data.

	slumptest	forestfire	bolts	cloud	kidney	lupus	
LSR	0.2689Â±0.0295	0.9986Â±0.0874	0.4865Â±0.0607	0.6178Â±0.0190	0.5077Â±0.0264	0.8646Â±0.3703	
LSR-L2	0.2616Â±0.0266	0.9822Â±0.0064	0.4687Â±0.0137	0.5782Â±0.0029	0.5106Â±0.0219	0.8338Â±0.3282	
LSR-L1	0.2571Â±0.0277	0.9822Â±0.0079	0.4713Â±0.0172	0.5802Â±0.0043	0.5196Â±0.0089	0.8408Â±0.3366	
MedianR	0.2810Â±0.0024	0.9964Â±0.0050	0.4436Â±0.0232	0.6457Â±0.0301	0.5432Â±0.0160	1.2274Â±0.6979	
HuberR	0.2669Â±0.0268	0.9874Â±0.0299	0.4841Â±0.0661	0.6178Â±0.0190	0.5447Â±0.0270	0.9198Â±0.4226	
RMR-L2	0.2538Â±0.0185	0.9817Â±0.0093	0.4782Â±0.0107	0.5702Â±0.0131	0.4871Â±0.0578	0.8071Â±0.3053	
RMR-L1	0.2517Â±0.0240	0.9802Â±0.0198	0.3298Â±0.1313	0.5663Â±0.0305	0.4989Â±0.0398	0.7885Â±0.2910	

Table 4 Average RMSE and standard deviation on the ADNI data.

	Fluency	ADAS	TRAILS	
LSR	0.3856Â±0.0034	0.4397Â±0.0112	0.6798Â±0.0538	
LSR-L2	0.3269Â±0.0069	0.4116Â±0.0208	0.5443Â±0.0127	
LSR-L1	0.3295Â±0.0035	0.4121Â±0.0100	0.5476Â±0.0115	
MedianR	0.4164Â±0.0291	0.4700Â±0.0151	0.6702Â±0.1184	
HuberR	0.3856Â±0.0034	0.4383Â±0.0133	0.6621Â±0.0789	
RMR-L2	0.3256Â±0.0049	0.4105Â±0.0216	0.5342Â±0.0186	
RMR-L1	0.3269Â±0.0057	0.4029Â±0.0234	0.5423Â±0.0123	

2 http://lib.stat.cmu.edu/datasets/

31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.


1 Armitage SG An analysis of certain psychological tests used for the evaluation of brain injury Psychol Monogr 60 1 1 48 1946
2 Bozzali M Falini A Franceschi M Cercignani M Zuffi M Scotti G Comi G Filippi M White matter damage in alzheimerâ€™s disease assessed in vivo using diffusion tensor magnetic resonance imaging J Neurol Neurosurg Psychiatry 72 6 742 746 2002 12023417
3 Chambers CD Payne JM Stokes MG Mattingley JB Fast and slow parietal pathways mediate spatial attention Nat Neurosci 7 3 217 218 2004 14983182
4 Chen YC Genovese CR Tibshirani RJ Wasserman L Nonparametric modal regression Ann Statist 44 2 489 514 2016
5 Collomb G HÃ¤rdle W Hassani S A note on prediction via estimation of the conditional mode function J Stat Plan Infer 15 227 236 1987
6 Feng Y Fan J Suykens JA A statistical learning approach to modal regression arXiv:1702.05960 2017
7 Feng Y Huang X Shi L Yang Y Suykens JA Learning with the maximum correntropy criterion induced losses for regression J Mach Learn Res 16 993 1034 2015
8 Folstein MF Folstein SE McHugh PR â€œmini-mental stateâ€: a practical method for grading the cognitive state of patients for the clinician J Psychiatr Res 12 3 189 198 1975 1202204
9 He R Zheng WS Hu BG Maximum correntropy criterion for robust face recognition IEEE Trans Pattern Anal Mach Intell 33 8 1561 1576 2011 21135440
10 Huang J Friedland R Auchus A Diffusion tensor imaging of normal-appearing white matter in mild cognitive impairment and early alzheimer disease: preliminary evidence of axonal degeneration in the temporal lobe AJNR Am J Neuroradiol 28 10 1943 1948 2007 17905894
11 Huber PJ Robust statistics John Wiley &amp; Sons 1981
12 Huber PJ Finite sample breakdown of m-and p-estimators Ann Statist 12 1 119 126 1984
13 Jack CR Bernstein MA Fox NC Thompson P Alexander G Harvey D Borowski B Britson PJ Whitwell JL Ward C The alzheimerâ€™s disease neuroimaging initiative (adni): Mri methods J Magn Reson Imaging 27 4 685 691 2008 18302232
14 Karas G Sluimer J Goekoop R Van Der Flier W Rombouts S Vrenken H Scheltens P Fox N Barkhof F Amnestic mild cognitive impairment: structural mr imaging findings predictive of conversion to alzheimer disease AJNR Am J Neuroradiol 29 5 944 949 2008 18296551
15 Lichman M UCI machine learning repository 2013
16 Moradi E Hallikainen I HÃ¤nninen T Tohka J Initiative ADN Reyâ€™s auditory verbal learning test scores can be predicted from whole brainmri in alzheimerâ€™s disease Neuroimage Clin 13 415 427 2017 28116234
17 Nickel J Jokeit H Wunderlich G Ebner A Witte OW Seitz RJ Gender-specific differences of hypometabolism in mtle: Implication for cognitive impairments Epilepsia 44 12 1551 1561 2003 14636327
18 Nikolova M Ng MK Analysis of half-quadratic minimization methods for signal and image recovery SIAM J Sci Comput 27 3 937 966 2005
19 Principe JC Information theoretic learning: Renyiâ€™s entropy and kernel perspectives Springer New York 2010
20 Rockafellar RT Convex Analysis Princeton, NJ, USA Princeton Univ. Press 1970
21 Sager TW Thisted RA Maximum likelihood estimation of isotonic modal regression Ann Statist 10 3 690 707 1982
22 Tibshirani R Regression shrinkage and selection via the lasso J Royal Statist Soc B 58 1 267 288 1996
23 Wang H Nie F Huang H Risacher S Ding C Saykin AJ Shen L Sparse multi-task regression and feature selection to identify brain imaging predictors for memory performance Computer Vision (ICCV), 2011 IEEE International Conference on 557 562 IEEE 2011
24 Wang H Nie F Huang H Risacher S Saykin AJ Shen L Joint classification and regression for identifying ad-sensitive and cognition-relevant imaging biomarkers The 14th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2011) 115 123
25 Wang H Nie F Huang H Yan J Kim S Risacher S Saykin A Shen L High-order multi-task feature learning to identify longitudinal phenotypic markers for alzheimer disease progression prediction Neural Information Processing Systems Conference (NIPS 2012) 1286 1294
26 Wang X Shen D Huang H Prediction of memory impairment with mri data: A longitudinal study of alzheimerâ€™s disease 19th International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI 2016) 273 281
27 Yang H Yang J A robust and efficient estimation and variable selection method for partially linear single-index models J Multivariate Anal 129 227 242 2014
28 Yao W Li L A new regression model: modal linear regression Scandinavian J Statistics 41 3 656 671 2014
29 Yao W Lindsay BG Li R Local modal regression J Nonparametric Stat 24 3 647 663 2012
30 Zhao W Zhang R Liu J Lv Y Robust and efficient variable selection for semiparametric partially linear varying coefficient model based on modal regression Ann I Stat Math 66 1 165 191 2014
