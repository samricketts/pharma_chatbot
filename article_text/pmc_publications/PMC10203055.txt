LICENSE: This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.


101584082
40276
Appl Neuropsychol Adult
Appl Neuropsychol Adult
Applied neuropsychology. Adult
2327-9095
2327-9109

36416227
10203055
10.1080/23279095.2022.2146504
NIHMS1878228
Article
Racial Differences in Positive Findings on Embedded Performance Validity Tests
Hromas Gabrielle Dr The University of Texas Health Science Center at San Antonio, Neurology, San Antonio, 78284 United States

Rolin Summer Dr The University of Texas Health Science Center at San Antonio, Rehabilitation Medicine, San Antonio, 78284 United States

Davis Jeremy J. Dr The University of Texas Health Science Center at San Antonio, Neurology, San Antonio, 78284 United States

Corresponding Author: davisj20@uthscsa.edu
30 3 2023
23 11 2022
23 5 2024
19
This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.
Introduction:

Embedded performance validity tests (PVTs) may show increased positive findings in racially diverse examinees. This study examined positive findings in an older adult sample of African American (AA) and European American (EA) individuals recruited as part of a study on aging and cognition.

Method:

The project involved secondary analysis of deidentified National Alzheimer’s Coordinating Center data (N = 22,688). Exclusion criteria included diagnosis of dementia (n = 5,550), mild cognitive impairment (MCI; n = 5,160), impaired but not MCI (n = 1,126), other race (n = 864), and abnormal Mini Mental State Examination (MMSE &lt; 25; n = 135). The initial sample included 9,853 participants (16.4% AA). Propensity score matching matched AA and EA participants on age, education, sex, and MMSE score. The final sample included 3,024 individuals with 50% of participants identifying as AA. Premorbid ability estimates were calculated based on demographics. Failure rates on five raw score and six age-adjusted scaled score PVTs were examined by race.

Results:

Age, education, sex, MMSE, and premorbid ability estimate were not significantly different by race. Thirteen percent of AA and 3.8% of EA participants failed two or more raw score PVTs (p &lt; .0001). On age-adjusted PVTs, 20.6% of AA and 5.9% of EA participants failed two or more (p &lt; .0001).

Conclusions:

PVT failure rates were significantly higher among AA participants. Findings indicate a need for cautious interpretation of embedded PVTs with underrepresented groups. Adjustments to embedded PVT cutoffs may need to be considered to improve diagnostic accuracy.

neuropsychology
performance validity
embedded validity
racial groups
false positive rate

pmcPerformance validity tests (PVTs) were developed to address whether an individual’s performance on neuropsychological testing was representative of their actual cognitive abilities by flagging atypical patterns and/or levels of performance on specific measures (Larrabee, 2012). While much of this research initially focused on performance and symptom validity in the forensic context, over the past several decades there has been an expansion in research and incorporation of PVTs into clinical settings (Suchy, 2019). This corresponds with the American Academy of Clinical Neuropsychology’s (AACN) updated consensus statement in 2021 which distinguished between detecting test invalidity and diagnosing malingering. The AACN emphasized using PVTs to demonstrate the accuracy and interpretability of test results rather than as a measure of examinee effort (Sweet et al., 2021). Relevant consideration of factors associated with failed PVTs, other than deliberate symptom feigning (malingering), include non-deliberate symptom distortion (e.g., conversion disorder and somatic symptom disorder), non-neurologic features (e.g., fatigue and pain), attitude towards testing, inattention, medication side-effects, and psychological symptoms. Below-expectation PVT results in isolation do not imply that examinees are intentionally performing poorly (Marcopulos et al., 2014; Greher, &amp; Wodushek, 2017; Martin &amp; Schroeder, 2020) except in cases involving below-chance level scores on forced choice recognition PVTs (Binder &amp; Chafetz, 2018).

The increased research and use of PVTs in the field is occurring in the context of a larger focus on the effects of culture and acculturation on neuropsychological assessment (Razani, Burciaga, Madore, &amp; Wong, 2007; Tan, Burgess, &amp; Green, 2021). Several challenges have been identified including differences in how measures perform in individuals across cultures, a lack of representative normative samples, and biases these issues create within the measures themselves (Cory, 2021). Regarding PVTs specifically, there is limited research on the influence of sociodemographic factors like race, ethnicity, and culture on PVT performance (Faust, Gaudet, Ahern, &amp; Bridges, 2021). A review of research examining performance and symptom validity measures in ethnically and culturally diverse populations identified only 45 studies published between 2002 and 2015 (Nidam-Jones &amp; Rosenfeld, 2017), which is a striking contrast to the 30 to 80 publications per year on validity issues in general in the same interval (Suchy, 2019). Based on the limited available findings, there is evidence that PVTs may show increased false positive rates in racially diverse samples. A recent article indicated that the odds of failing the Test of Memory Malingering (TOMM; Tombaugh, 1996), a commonly used standalone PVT, were significantly higher in a group of African American or Black (AA) examinees when compared to European American or White (EA) test-takers (Braun et al., 2021). A multitude of reasons could explain these differences, including cultural biases in test development, a lack of representative sampling during test validation and normative data collection (Braun et al., 2021), and examiner-examinee interaction effects within the test taking environment itself (e.g., stereotype threat or perceived discrimination; Barnes et al., 2012; Marx &amp; Goff, 2005;).

An additional hypothesis is that the racial differences in performance on PVTs is actually an artifact due to differences in measured IQ. When a group of AA patients were compared to EA participants, the EA participants scored higher on multiple embedded and standalone PVT measures including the following measures: Dot Counting Test (DCT; Boon, et al., 2002b), b Test (Boone, et al., 2002a), WRMT-W (Kim et al., 2010b), Rey Word Recognition Test (Rey, 1941), embedded indicators on Digit Span (Babikian et al., 2006), Digit Symbol subtest (Kim, et al., 2010a), the Rey Complex Figure Test (Lu et al., 2003), the Auditory Verbal Learning Test (Boone et al., 2005; Salazar et al., 2007; Sherman et al., 2002; Suhr et al., 1997), WMS-III Logical Memory (Bortnik et al., 2010), Comalli Stroop (Arentsen, et al., 2013), Rey-15 (Boone, Salazar, et al., 2002), and WCST (Greve, Heinly, Bianchini, &amp; Love, 2009). However, after excluding individuals with measured IQ in the borderline range (Standard Scores between 70–79) the racial differences in PVT performance resolved for all aforementioned measures except for the DCT. These results imply that when assessing individuals with higher than low average measured IQ, racial differences on PVT performance may be less of a clinical concern (Hood et al., 2015; Hood et al., 2022). However, this hypothesis in itself is affected by known cultural influences on IQ measurement, as traditional IQ tests do not perform equally across cultures. Differences in measured IQ across cultures can be due to environmental factors (e.g., educational quality), cultural ideas, and exposure to stimuli related to test questions (Onwuegbuzie &amp; Daley, 2001).

Lastly, a study utilizing a racially diverse sample to examine performance on embedded and standalone PVTs found that adjustments to traditionally used cut scores were necessary to maintain a specificity rate of 90% or higher on several measures, including a freestanding measure (i.e., Rey Fifteen-Item Test plus recognition; Boone et al., 2002) and embedded indicators on Digit Span (Babikian et al., 2006), the Rey Complex Figure Test (Lu et al., 2003) and the Auditory Verbal Learning Test (Boone et al., 2005; Salazar et al., 2007). In contrast, assessment of a group of college athletes showed that the Rey Word Recognition Test (Rey, 1941) traditional cutoff was robust to racial differences (Goworowski et al., 2020).

The current study examined positive rates by race in an older adult sample via secondary data analysis of a dataset from the National Alzheimer’s Coordinating Center (NACC). We hypothesized that, because of the aforementioned biases in measures and effects within the testing environment, embedded PVTs would show increased positive findings in racially diverse examinees, requiring additional consideration when interpreting clinical results.

Method

Participants

This study involved secondary analysis of a dataset from the NACC. This is a longitudinal study of aging in which data on cognitive status is collected yearly (for as long as the participant is medically able to participate) through participating Alzheimer’s Disease Research Centers since 2005. Details on the study design and consensus test battery are described by Weintraub et al. (2009). Updates on the current status of the project including proposed and complete secondary analytic projects and data request procedures are available at the NACC web site (https://naccdata.org/). NACC data are de-identified and the Institutional Review Board determined the project to be non-human subjects research. NACC data were provided from the March 2021 data freeze. The dataset included cases evaluated with Uniform Data Set (UDS) version 1 and 2 and was restricted to the initial visit and cases with complete data on outcome measures resulting in 22,688 cases. Cases were excluded for race other than AA or EA (n = 864), diagnosis of mild cognitive impairment (MCI; n = 5,550), diagnosis of dementia (n = 5,160), and diagnosis of impaired but not MCI (n = 1,126) resulting in an intermediate sample of cases identified as showing normal cognition (n = 9,988). Cases were also excluded if the Mini-Mental State Examination (MMSE; Folstein, Folstein, &amp; McHugh, 1975) score was abnormal (&lt; 25; n = 135) resulting in the to-be-matched sample (n = 9,853) in which slightly more than 15% of participants identified as AA (n = 1,512). AA and EA participants were matched resulting in a final sample of 3,024. Details on the sample derivation are in Figure 1.

The sample was 22.0% male with an average age of 70.0 years (SD = 9.5) and average education of 14.7 years (SD = 2.9). The sample was evenly split between AA and EA participants. Groups were matched on gender, age, education, and MMSE score. Details on demographics by group are in Table 1.

Measures

UDS Battery.

Participants underwent evaluation that included cognitive measures in the UDS (Weintraub et al., 2009). The UDS version 1 and 2 cognitive test battery includes the MMSE (Folstein et al., 1975), Boston Naming Test (Kaplan et al., 1983), Semantic Word Generation (animal naming and vegetable naming; Gladsjo et al., 1999), Trail Making Test (Reitan, 1958), Wechsler Adult Intelligence Scale-Revised Digit Symbol Coding (Wechsler, 1987a), and Wechsler Memory Scale-Revised subtests (Digit Span and Logical Memory Story A; Wechsler, 1987b). Given the aim of this study, MMSE scores and measures from the UDS test battery that have been previously examined as embedded PVTs with older adults were of interest. MMSE was included among the variables used to match participants. PVT batteries of five and six embedded PVTs were examined using raw and age-adjusted scores, respectively from the following UDS measures: Digit Span (DS; Wechsler, 1987a, 1987b), Digit Symbol Coding (COD; Wechsler, 1987a), semantic word generation (SWG; animal naming; Gladsjo et al., 1999), and the Trail Making Test (TMT; Reitan, 1958).

PVT Batteries.

Positive findings (e.g., PVT failure) were defined by scores below or above the following cutoffs. The DS raw score cutoff was based on the longest forward trial (DS-LDF; ≤ 4) that has shown acceptable specificity among older adults (Heinly, Greve, Bianchini, Love, &amp; Brennan, 2005; Kiewel, Wisdom, Bradshaw, Pastorek, &amp; Strutt, 2012). The SWG raw score cutoff was based on animal naming (&lt; 13), which has been examined in older adult populations (Davis, 2018; Sugarman &amp; Axelrod, 2015). Three cutoffs were derived from TMT: TMT-A time (≥ 62), TMT-B time (≥ 200), and the TMT ratio (TMT-R), which was calculated by dividing raw scores on TMT-B by raw scores on TMT-A (&lt; 1.5; Bortnik, Horner, &amp; Bachman, 2013; Davis, 2018; Iverson et al., 2002; Merten, Bossink, &amp; Schmand, 2007). TMT-R cutoffs have been supported by multiple studies of older adults (Bortnik et al., 2013; Davis, 2018; Merten et al., 2007). Although not the focus of the study, descriptive data reported by Davis (2018) demonstrate the potential utility of TMT-A and -B raw score cutoffs among cognitively intact older adults.

DS, COD, SWG, and TMT raw scores were converted to age-adjusted scores using equations from Shirk et al. (2011) based on normative data reported by Weintraub et al. (2009). Two scores were included from DS: DS Forward (DSF) and DS Backward (DSB). Two scores were included from TMT: TMT-A and TMT-B. The age-adjusted score cutoff (≤ −1.7 z) was based on consistency across multiple studies of different measures, test versions, and normative data (Ashendorf et al., 2017; Babikian et al., 2006, Erdodi et al., 2017; Etherton et al., 2006; Heinly et al., 2005; Sugarman &amp; Axelrod, 2015; Trueblood, 1994).

Premorbid Ability Estimate.

FSIQ estimates based on demographics were calculated from the regression equation reported by McCarthy et al. (2003), which was derived from the Mayo Older Adult Normative sample.

Analysis

AA participants were matched to EA participants using one-to-one propensity score matching without replacement using the psmatch2 command in Stata 13.2 (StataCorp LP, 2013). Matching covariates included dichotomous gender, age, education, and MMSE score. Non-parametric correlations (Spearman’s rho) were calculated to examine associations among demographic variables and PVTs. PVT failure rates were examined by race using descriptive statistics and non-parametric analyses (chi-square tests). The effect size r was calculated for chi-square tests: r = √(Χ2 / N). Effect sizes and associations were classified as small (.10 to .29), medium (.30 to .49), or large (.50 or higher; Cohen, 1992).

Results

Participant groups were not significantly different in gender, age, education, or MMSE score. Premorbid ability estimates did not significantly differ by race, and the sample did not include any cases with estimated premorbid ability below a previously identified threshold (FSIQ &lt; 80) associated with PVT failure (Hood et al., 2015).

Age and education showed small to medium associations with raw scores on the measures used to derive raw score PVTs. The measures showed small to medium intercorrelations except for a large association observed between TMT-A and -B and between TMT-B and -R. Age was not associated with age-adjusted scaled scores on the measures used to derive PVTs. Education demonstrated small associations with age-adjusted scaled score. Intercorrelations among age-adjusted scores were small to medium except for large associations between COD and TMT, DSF and DSB, and TMT-A and -B. None of the intercorrelations were high enough to raise concern for non-redundancy of measures. Details are in Table 2.

In the whole sample, 75.3% of participants performed above cutoffs on all five of the raw score PVTs, and 69.2% performed above cutoffs on all six of the age-adjusted PVTs. One embedded raw score PVT was below expectation in 16.3% of the sample, and 17.6% scored below cutoff on a single age-adjusted PVT. Two or more PVTs were below expectation in 8.4% and 13.3% of participants on raw score and age-adjusted PVTs, respectively.

The proportion of participants performing below expectation on two or more raw score PVTs was significantly different between AA (13.0%) and EA (3.8%) participants, Χ2 (1) = 84.2, p &lt; .0001, r = .17. The proportion of participants performing below expectation on two or more age-adjusted PVTs was significantly different between AA (20.6%) and EA (5.9%) participants, Χ2 (1) = 143.0, p &lt; .0001, r = .22. The frequency and cumulative percentage of positive findings on different numbers of raw score and age-adjusted PVTs are shown in Table 3.

Positive findings on raw score PVTs showed a wide range from 3.4% on DS-LDF to 11% on SWG. No significant differences were observed in positive rates by race on TMT-R or DS-LDF. AA participants showed significantly higher positive rates on TMT-A, Χ2 (1) = 79.8, p &lt; .001, r = .16, TMT-B, Χ2 (1) = 64.1, p &lt; .001, r = .15, and SWG, Χ2 (1) = 84.0, p &lt; .001, r = .17.

Positive findings on age-adjusted PVTs also showed a wide range from 6.1% on SWG to 13% on TMT-B. Significant differences were observed between AA and EA participants on all measures: COD, Χ2 (1) = 68.5, p &lt; .001, r = .15; DSF, Χ2 (1) = 8.4, p = .004, r = .05; DSB, Χ2 (1) = 25.8, p &lt; .001, r = .09; TMT-A, Χ2 (1) = 122.9, p &lt; .001, r = .20; TMT-B, Χ2 (1) = 115.8, p &lt; .001, r = .20; and SWG, Χ2 (1) = 54.2, p &lt; .001 r = .13. Details on PVT positive findings by test are in Table 4 and Figure 2.

Discussion

This study examined PVT positive findings (i.e., scores below or above cutoffs, or PVT “failures”) by race in participants matched on gender, age, education, and MMSE score. Consistent with some prior research (Proto et al., 2014; Victor et al., 2009), these findings support that it is not uncommon to see below-expectation performance on a single PVT. The interpretation or weight given to such observations depends on the evaluation context (Sweet et al., 2021). In the whole sample, positive findings on two or more raw score PVTs remained below a commonly used threshold of 10% or less (Larrabee, Rohling, &amp; Meyers, 2019; Sweet et al., 2021). Positive findings on two or more age-adjusted score PVTs were slightly over the threshold at 13.3%. AA participants were observed to show a significantly higher rate of below-expectation performance on two or more PVTs based on raw scores and age-adjusted scores with small effect sizes in both comparisons.

The implications of these observations are mixed. From the perspective of evaluating older adults, it is promising that the sample-wide proportion of participants that performed below expectation on two or more PVTs was below or near the 10% threshold. This finding lends additional support to other research indicating that neurologic status (e.g., dementia) likely accounts for PVT positive findings in older adults rather than age alone (Davis, 2018; Duff et al., 2011; Loring et al., 2016). Age showed a medium association with three raw score PVTs (i.e., SWG, TMT-A, and TMT-B), but age alone does not account for these findings because it would have exerted a similar effect on both groups given matching on age in addition to other variables. Furthermore, in the oldest sample subset of participants 80 years and older, there were slightly more EA participants than AA participants (54% versus 46%). This aspect of the sample would have been expected to exert an opposite influence than what is observed in these findings if age alone were explaining PVT positives.

It is concerning, however, that AA participants showed significantly higher rates of performing below expectation on two or more raw score and age-adjusted score PVTs than EA participants despite being matched on gender, age, education, and MMSE. Given prior research (Hood et al., 2015), differences in premorbid ability might be considered to possibly contribute to differential false positive rates, but participant groups were not significantly different in premorbid ability estimates and no participants in either group were below the low average range in premorbid ability estimate. On the raw score PVTs, the rate below expectation on two or more PVTs among AA participants was close to the 10% threshold, which may mitigate this concern. On age-adjusted score PVTs, however, the rate below expectation on two or more PVTs among AA participants was double the common threshold and almost four times higher than the rate among EA participants. Effect sizes in both comparisons were small, but the absolute differences may warrant further consideration.

Turning to positive findings on individual raw score PVTs, TMT-R and DS-LDF positive rates remained low in both participant groups and were not significantly different by group consistent with other research in samples including older adults (Heinly et al., 2005; Merten et al. 2007). TMT-A and TMT-B raw score positive rates were significantly higher among AA participants but were not substantially elevated above 10%. SWG raw score false positive rates were significantly higher among AA participants with a small effect size. The rate was above a liberal threshold (&gt;15%) and 2.8 times higher than the positive rate among EA participants. Given incremental contributions of race to prediction of word generation performance beyond age and education, this difference is not unexpected (Gladsjo et al., 1999). These findings are similar to the recent study by Hood and colleagues (2022) that identified embedded PVTs as more likely to show racial differences than freestanding PVTs. In contrast to Hood et al., the current findings did not demonstrate elimination of differences when measured IQ was equated.

Although the MMSE was used to match participants in global cognitive status and groups did not differ in premorbid ability estimate, there likely remain additional group differences for which race is a proxy (e.g., educational quality; Gasquoine, 2009). Matching participants on educational level does not address potential differences in educational quality, which is but one of multiple environmental factors that have been identified as likely underlying performance variability by race. Additional considerations include socioeconomic factors, access to healthcare, nutrition, exposure to pollutants and toxins, and the psychosocial consequences of racism (American Academy of Clinical Neuropsychology, 2021; Manly &amp; Glymour, 2021). Further research efforts are needed to clarify these factors and quantify their effects on neuropsychological test data to improve diagnostic accuracy.

Even though positive findings on individual age-adjusted score PVTs were significantly higher among AA participants, the observed positive rates remained below the 10% threshold for both participant groups on three measures: DSF, DSB, and SWG. COD age-adjusted score positive rates were significantly higher in AA participants but remained below a liberal threshold. TMT-A and TMT-B were significantly higher with a small effect size and exceeded a liberal false positive threshold. These rates were 3.1 to 3.9 times higher among AA participants than EA participants.

The findings of positive rates on specific PVTs lead to some key take away points. First, given the lack of significant differences by race, raw score cutoffs on TMT-R and DS-LDF are supported. Raw scores cutoffs on TMT-A and TMT-B may warrant further examination or adjustment to achieve optimal specificity among older adult AA examinees. These findings indicate a need for additional attention to raw score cutoffs on SWG and are concerning for elevated false positive rates among AA examinees. It appears prudent to consider alternative cutoffs based on norm-referenced scores or use other embedded PVTs until further research can establish adjusted raw scores cutoffs. Consistent with this possibility, current findings demonstrate the potential utility of age-adjusted score cutoffs on SWG, DSF, and DSB. COD, TMT-A, and TMT-B age-adjusted score cutoffs require modification for use among older adult AA examinees to lower the false positive rate.

Limitations of the study include the retrospective design that limits ability to adjust the test battery to tests more widely used currently. A recent survey, however, shows that several measures (namely PVTs derived from TMT and DS scores) are still utilized regularly by clinicians (Martin, Schroeder, &amp; Odland, 2015). While we are not aware of usage data among clinicians of other measures (e.g., COD, SWG), prior studies examining their potential as embedded PVTs attests to interest in them at least among researchers (Ashendorf et al., 2017; Babikian et al., 2006; Davis, 2018; Erdodi et al., 2017; Etherton et al., 2006; Heinly et al., 2005; Sugarman &amp; Axelrod, 2015; Trueblood, 1994). The NACC dataset by design uses older versions of tests to maintain consistency over time (Weintraub et al., 2009). The embedded PVTs on older versions of tests may have less relevance given changes in revised editions, but some consistency is observed in scaled score cutoffs on COD (Etherton et al., 2006; Inman &amp; Berry, 2002) and DS (Babikian et al., 2006; Iverson &amp; Franzen, 1994) across versions of the parent measure. Another limit to the design is the lack of criterion measure of validity status. Given the nature of the evaluations from which NACC data are derived, there is a low expectation of secondary gain, raising concern that the scores below cutoff represent false positive findings. Treating all positive PVT findings as false positives might overestimate the rate, but such inflation would occur across the whole study. Comparisons by race would, therefore, likely remain valid. The sample was well-educated and predominantly female, which may restrict generalizability of the results. Additionally, this study was limited to AA and EA individuals. Future studies should examine findings across lower education levels, with a wider range of premorbid ability estimates, as well as other underrepresented minority groups.

In summary, we present findings on positive rates on five raw score PVTs and six age-adjusted score PVTs in AA and EA older adult participants matched on demographics and global cognitive status. Overall positive rates remained low, but positive rates on certain measures were elevated with a pattern of greater positive rates among AA participants. Even in consideration of potential psychometric weaknesses of the examined embedded PVT’s, they would be expected to perform equally poorly across races. Though we do not know the cause of this discrepancy, the fact that AA examinees performed below cutoff more frequently than EA examinees raises concern of cultural biases in neuropsychological tests or other social factors that would affect performance in the minority group. The context of the NACC database and sample raise significant concern that these are false positive failures. Findings will hopefully contribute to a small but growing body of literature that examines racial and ethnic effects on PVTs potentially due to test development occurring predominantly in the majority culture, non-representative normative samples, and/or test-taking interaction effects. Future research might extend the findings to a sample with wider educational and age range using updated PVTs. It might also be helpful to explore more refined methods of comparing participants on socioeconomic or educational quality variables to better estimate the psychosocial factors for which race appears to serve as a proxy. Other research might explore the feasibility of adjusted cutoffs on PVTs and contrast a race or ethnicity-based adjustment with adjustment for other psychosocial factors.

Acknowledgment:

The NACC database is funded by NIA/NIH Grant U24 AG072122. NACC data are contributed by the NIA-funded ADCs: P50 AG005131 (PI James Brewer, MD, PhD), P50 AG005133 (PI Oscar Lopez, MD), P50 AG005134 (PI Bradley Hyman, MD, PhD), P50 AG005136 (PI Thomas Grabowski, MD), P50 AG005138 (PI Mary Sano, PhD), P50 AG005142 (PI Helena Chui, MD), P50 AG005146 (PI Marilyn Albert, PhD), P50 AG005681 (PI John Morris, MD), P30 AG008017 (PI Jeffrey Kaye, MD), P30 AG008051 (PI Thomas Wisniewski, MD), P50 AG008702 (PI Scott Small, MD), P30 AG010124 (PI John Trojanowski, MD, PhD), P30 AG010129 (PI Charles DeCarli, MD), P30 AG010133 (PI Andrew Saykin, PsyD), P30 AG010161 (PI David Bennett, MD), P30 AG012300 (PI Roger Rosenberg, MD), P30 AG013846 (PI Neil Kowall, MD), P30 AG013854 (PI Robert Vassar, PhD), P50 AG016573 (PI Frank LaFerla, PhD), P50 AG016574 (PI Ronald Petersen, MD, PhD), P30 AG019610 (PI Eric Reiman, MD), P50 AG023501 (PI Bruce Miller, MD), P50 AG025688 (PI Allan Levey, MD, PhD), P30 AG028383 (PI Linda Van Eldik, PhD), P50 AG033514 (PI Sanjay Asthana, MD, FRCP), P30 AG035982 (PI Russell Swerdlow, MD), P50 AG047266 (PI Todd Golde, MD, PhD), P50 AG047270 (PI Stephen Strittmatter, MD, PhD), P50 AG047366 (PI Victor Henderson, MD, MS), P30 AG049638 (PI Suzanne Craft, PhD), P30 AG053760 (PI Henry Paulson, MD, PhD), P30 AG066546 (PI Sudha Seshadri, MD), P20 AG068024 (PI Erik Roberson, MD, PhD), P20 AG068053 (PI Marwan Sabbagh, MD), P20 AG068077 (PI Gary Rosenberg, MD), P20 AG068082 (PI Angela Jefferson, PhD), P30 AG072958 (PI Heather Whitson, MD), P30 AG072959 (PI James Leverenz, MD).

Figure 1. Details on derivation of the sample. AA = African American or Black; EA = European American or White; MCI = mild cognitive impairment; MMSE = Mini-mental State Examination.

Figure 2. Positive rates (%) on individual performance validity test cutoffs based on raw and age-adjusted scores. Raw = raw score cutoff; TMT = Trail Making Test; TMT-R = Trail Making Test-B to - A ratio; DS-LDF = Digit Span Longest Digit Forward; SWG = semantic word generation (animals); Age-adjusted = age-adjusted scaled score cutoff; COD = Digit Symbol Coding; DSF = Digit Span Forward; DSB = Digit Span Backward; AA = African American or Black; EA = European American or White.

Table 1. Sample demographics, matching variables, and estimated premorbid ability

Variable	AA	EA	Total	
% Male	21.9	22.0	22.0	
Age	70.0 (8.5)	70.0 (10.5)	70.0 (9.5)	
Education	14.6 (2.9)	14.7 (2.9)	14.7 (2.9)	
MMSE	28.5 (1.4)	28.4 (1.4)	28.4 (1.4)	
Pre-FSIQ	107.9 (5.7)	108.0 (5.8)	108.0 (5.8)	
Note. Data are in the format M (SD), except where noted. AA = African American or Black; EA = European American or White; MCI = mild cognitive impairment; MMSE = Mini-mental State Examination; Pre-FSIQ = estimated of FSIQ based on McCarthy et al. (2003).

Table 2. Demographic and performance validity test correlations

Raw Scores	
	Age	Ed	TMT-A	TMT-B	TMT-R	DS-LDF	SWG		
	
Age	-								
Ed	−0.11*	-							
TMT-A	0.39*	−0.21*	-						
TMT-B	0.40*	−0.30*	0.68*	-					
TMT-R	0.11*	−0.19*	−0.17*	0.56*	-				
DS-LDF	−0.09*	0.20*	−0.20*	−0.28*	−0.17*	-			
SWG	−0.28*	0.28*	−0.41*	−0.44*	−0.14*	0.19*	-		
Age Adjusted Scores	
	Age	Ed	COD	DSF	DSB	TMT-A	TMT-B	SWG	
Age	-								
Ed	−0.11*	-							
COD	−0.02	0.28*	-						
DSF	0.03	0.19*	0.20*	-					
DSB	0.01	0.21*	0.29*	0.51*	-				
TMT-A	0.02	0.18*	0.55*	0.21*	0.27*	-			
TMT-B	0.00	0.29*	0.59*	0.30*	0.39*	0.61*	-		
SWG	−0.04	0.27*	0.34*	0.20*	0.23*	0.34*	0.38*	-	
Note. Values are non-parametric correlation coefficients (Spearman’s rho). Ed = years of education; TMT = Trail Making Test; TMT-R = Trail Making Test-B to - A ratio; DS-LDF = Digit Span Longest Digit Forward; SWG = semantic word generation (animals); Adjusted = age-adjusted scaled score cutoff; COD = Digit Symbol Coding; DSF = Digit Span Forward; DSB = Digit Span Backward.

* p &lt; .0001

Table 3. Frequency and cumulative percentage of performance validity positive findings

Pos Rate	Raw Score PVTs	Age Adjusted PVTs	
AA
(n = 1512)	EA
(n = 1512)	Total
(N = 3024)	AA
(n = 1512)	EA
(n = 1512)	Total
(N = 3024)	
0	1003 (66.3)	1273 (84.2)	2276 (75.3)	865 (57.2)	1226 (81.1)	2091 (69.2)	
1	312 (87.0)	182 (96.2)	494 (91.6)	335 (79.4)	197 (94.1)	532 (86.7)	
2	136 (96.0)	34 (98.5)	170 (97.2)	165 (90.3)	43 (97.0)	208 (93.6)	
3	54 (99.5)	22 (99.9)	76 (99.7)	98 (96.8)	30 (98.9)	128 (97.9)	
4	7 (100)	1 (100)	8 (100)	35 (99.1)	8 (99.5)	43 (99.3)	
5				12 (99.9)	8 (100)	20 (99.9)	
6				2 (100)		2 (100)	
Note. Data are in the format f (cumulative %). PVT = performance validity test; AA = African American or Black; EA = European American or White; Pos Rate = number of positive findings on performance validity tests.

Table 4. Positive rates on individual performance validity tests

PVT	AA
(n = 1512)	EA
(n = 1512)	Total
(N = 3024)	
Raw				
 TMT-A	13.0**	4.0	8.5	
 TMT-B	13.2**	4.8	9.0	
 TMT-R	4.8	3.8	4.3	
 DS-LDF	4.0	2.8	3.4	
 SWG	16.3**	5.8	11.0	
Adjusted				
 COD	12.9**	4.4	8.7	
 DSF	9.3*	6.5	7.4	
 DSB	9.8**	5.0	7.9	
 TMT-A	16.5**	4.2	10.4	
 TMT-B	19.6**	6.4	13.0	
 SWG	9.3**	2.9	6.1	
Note. Values are percentages. Positive rate comparisons by race within diagnostic group. PVT = performance validity test; AA = African American or Black; EA = European American or White; Raw = raw score cutoff; TMT = Trail Making Test; TMT-R = Trail Making Test-B to - A ratio; DS-LDF = Digit Span Longest Digit Forward; SWG = semantic word generation (animals); Adjusted = age-adjusted scaled score cutoff; COD = Digit Symbol Coding; DSF = Digit Span Forward; DSB = Digit Span Backward.

* p = .004

** p &lt; .001

Disclosure Statement: Two authors have provided forensic consultation (SNR, JJD). Otherwise, the authors have no competing interests, funding sources, or financial disclosures to report.

Data Disclosure Statement: The data that support the findings of this study are available from the National Alzheimer’s Coordinating Center (NACC) Quick-Access data set as of March 2021 data freeze at https://naccdata.org/


References

American Academy of Clinical Neuropsychology (2021). Position statement on use of race as a factor in neuropsychological test norming and performance prediction Retrieved on August 24, 2022 from https://theaacn.org/position-papers-and-policies/
Arensten TJ , Boone KB , Lo TTY , Goldberg HE , Cottingham ME , Victor TL , … Zeller MA (2013), Effectiveness of the Comalli Stroop Test as a measure of negative response bias. The Clinical Neuropsychologist, 27 (6 ), 1060–1076. doi: 10.1080/13854046.2013.803603 23742292
Ashendorf L , Clark EL , &amp; Sugarman MA (2017). Performance validity and processing speed in a VA polytrauma sample. The Clinical Neuropsychologist, 31 (5 ), 857–866. 10.1080/13854046.2017.1285961 28276866
Babikian T , Boone KB , Lu P , &amp; Arnold G (2006). Sensitivity and specificity of various digit span scores in the detection of suspect effort. The Clinical Neuropsychologist, 20 , 145–159.16393925
Barnes LL , Lewis TT , Begeny CT , Yu L , Bennett DA , &amp; Wilson RS (2012). Perceived discrimination and cognition in older African Americans. Journal of the International Neuropsychological Society, 18 (5 ), 856–865. doi: 10.1017/S1355617712000628 22595035
Binder LM &amp; Chafetz MD (2018). Determination of the smoking gun of intent: Significance testing of forced choice results in social security claimants. The Clinical Neuropsychologist, 32 , 132–144.28617092
Boone KB , Lu P , &amp; Herzberg D (2002a). The b Test Los Angeles: Western Psychological Services.
Boone KB , Lu P , &amp; Herzberg D (2002b). Rey Dot Counting Test Los Angeles: Western Psychological Services.
Boone KB , Salazar X , Lu P , Warner-Chacon K , &amp; Razani J (2002). The Rey 15-item Recognition Trial: A technique to enhance sensitivity of the Rey 15-Item Memorization Test. Journal of Clinical and Experimental Neuropsychology, 24 , 561–573.12187441
Boone KB , Lu P , &amp; Wen J (2005). Comparison of various RAVLT scores in the detection of non-credible memory performance. Archives of Clinical Neuropsychology, 20 , 301–319.15797167
Bortnik KE , Boone KB , Marion SD , Amano S , Ziegler E , Cottingham ME , Victor TL , &amp; Zeller MA (2010). Examination of various WMS-III logical memory scores in the assessment of response bias. The Clinical neuropsychologist, 24 (2 ), 344–357. 10.1080/13854040903307268 19921593
Bortnik KE , Horner MD , &amp; Bachman DL (2013). Performance on standard indexes of effort among patients with dementia. Applied Neuropsychology: Adult, 20 (4 ), 233–242. 10.1080/09084282.2012.695757 23537314
Braun SE , Fountain-Zaragoza S , Halliday CA , &amp; Horner MD (2021). Demographic differences in performance validity test failure. Applied Neuropsychology: Adult, doi: 10.1080/23279095.2021.1958814
Cohen J (1992). A power primer. Psychological Bulletin, 112 , 155–159.19565683
Cory JM (2021). White privilege in neuropsychology: An ‘invisible knapsack’ in need of unpacking. The Clinical Neuropsychologist, 35 (2 ), 206–218. doi: 10.1080/13854046.2020.1801845 32749202
Davis JJ (2018). Performance validity in older adults: Observed versus predicted false positive rates in relation to number of tests administered. Journal of Clinical and Experimental Neuropsychology, 40 (10 ), 1013–1021.29779432
Duff K , Spering CC , O’Bryant SE , Beglinger LJ , Moser DJ , Bayless JD , Culp KR , Mold JW , Adams RL , &amp; Scott JG (2011). The RBANS Effort Index: base rates in geriatric samples. Applied neuropsychology, 18 (1 ), 11–17. 10.1080/09084282.2010.523354 21390895
Erdodi LA , Abeare CA , Lichtenstein JD , Tyson BT , Kucharski B , Zuccato BG , &amp; Roth RM (2017). Wechsler adult intelligence scale-fourth edition (WAIS-IV) processing speed scores as measures of noncredible responding: The third generation of embedded performance validity indicators. Psychological Assessment, 29 (2 ), 148–157. 10.1037/pas0000319 27124099
Etherton JL , Bianchini KJ , Heinly MT , &amp; Greve KW (2006). Pain, malingering, and performance on the WAIS-III Processing Speed Index. Journal of Clinical and Experimental Neuropsychology, 28 (7 ), 1218–1237. 10.1080/13803390500346595 16840247
Faust DF , Gaudet CE , Ahern DC , &amp; Bridges AJ (2021). Assessment of malingering and falsification: Continuing to push the boundaries of knowledge in research and clinical practice. In Horton AM &amp; Reynolds CR (Eds.), Detection of malingering during head injury litigation (3rd ed., vol. 1 , pp. 1–156). Cham, Switzerland: Springer.
Folstein MF , Folstein SE , &amp; McHugh PR (1975). Mini-mental state. A practical method for grading the cognitive state of patients for the clinician. Journal of Psychiatric Research, 12 , 189–198.1202204
Gasquoine PG (2009). Race-norming of neuropsychological tests. Neuropsychology Review, 19 (2 ), 250–262. 10.1007/s11065-009-9090-5 19294515
Gladsjo JA , Schuman CC , Evans JD , Peavy GM , Miller SW , &amp; Heaton RK (1999). Norms for letter and category fluency: Demographic corrections for age, education, and ethnicity. Assessment, 6 , 147–160.10335019
Greher MR , &amp; Wodushek TR (2017). Performance validity testing in neuropsychology: Scientific basis and clinical application – a brief review. Journal of Psychiatric Practice, 23 (2 ), 134–140. doi: 10.1097/PRA.0000000000000218 28291039
Greve KW , Heinly MT , Bianchini KJ , &amp; Love JM (2009). Malingering detection with the Wisconsin Card Sorting Test in mild traumatic brain injury. The Clinical Neuropsychologist, 23 , 343–362. 10.1080/13854040802054169 18609328
Goworowski L , Vagt D , Salazar C , Mulligan K , &amp; Webbe F (2020). Normative values of the Rey Word Recognition Test in college athletes. Applied Neuropsychology: Adult, 21 (1 ), 94–100. doi: 10.1080/23279095.2018.1488716
Heinly MT , Greve KW , Bianchini KJ , Love JM , &amp; Brennan A (2005). WAIS digit span-based indicators of malingered neurocognitive dysfunction: classification accuracy in traumatic brain injury. Assessment, 12 (4 ), 429–444. 10.1177/1073191105281099 16244123
Hood E , Boone K , &amp; Moira C (2015). Ethnic differences in the detection of feigned cognitive symptoms: A comparison of the neuropsychological assessment performance validity tests between African-Americans and Caucasians Poster presentation at the annual meeting of the National Academy of Neuropsychology, Austin, TX.
Hood ED , Boone KB , Miora DS , Cottingham ME , Victor TL , Zeilgler MA , &amp; Write MJ (2022). Are there differences in performance validity tests scores between African American and White American neuropsychology clinic patients? Journal of Clinical and Experimental Neuropsychology, 44 (1 ), 31–41. doi: 10.1080/13803395.2022.2069230 35670549
Inman TH , &amp; Berry DT (2002). Cross-validation of indicators of malingering: A comparison of nine neuropsychological tests, four tests of malingering, and behavioral observations. Archives of Clinical Neuropsychology, 17 , 1–23.14589749
Iverson GL , &amp; Franzen MD (1994). The Recognition Memory Test, Digit Span, and Knox Cube Test as markers of malingered memory impairment. Assessment, 1 , 323–334.
Iverson GL , Lange RT , Green P , &amp; Franzen MD (2002). Detecting exaggeration and malingering with the Trail Making Test. The Clinical Neuropsychologist, 16 , 398–406.12607151
Kaplan E , Goodglass H , &amp; Weintraub S (1983). The Boston Naming Test Philadelphia: Lea and Febiger.
Kiewel NA , Wisdom NM , Bradshaw MR , Pastorek NJ , &amp; Strutt AM (2012). A retrospective review of digit span-related effort indicators in probable Alzheimer’s disease patients. The Clinical neuropsychologist, 26 (6 ), 965–974. 10.1080/13854046.2012.694478 22703555
Kim N , Boone KB , Victor T , Po L , Keatinge C , &amp; Mitchell C (2010a). Sensitivity and specificity of a Digit Symbol recognition trial in the identification of response bias. Archives of Clinical Neuropsychology, 25 , 420–428.20562116
Kim MS , Boone KB , Victor T , Marion SD , Amano S , Cottingham ME , … Zeller MA (2010b). The Warrington Recognition Memory Test for Words as a measure of response bias: Total score and response time cutoffs developed on “real world” credible and noncredible subjects. Archives of Clinical Neuropsychology, 25 (1 ), 60–70.19906738
Larrabee GJ (2012). Performance validity and symptom validity in neuropsychological assessment. Journal of the International Neuropsychological Society, 18 , 625–631. doi: 10.1017/S1355617712000240 23057079
Larrabee GJ , Rohling ML , &amp; Meyers JE (2019). Use of multiple performance and symptom validity measures: Determining the optimal per test cutoff for determination of invalidity, analysis of skew, and inter-test correlations in valid and invalid performance groups. The Clinical Neuropsychologist, 33 (8 ), 1354–1372. doi: 10.1080/13854046.2019.1614227 31111775
Loring DW , Goldstein FC , Chen C , Drane DL , Lah JJ , Zhao L , Larrabee GJ , &amp; Alzheimer’s Disease Neuroimaging Initiative (2016). False-positive error rates for reliable digit span and Auditory Verbal Learning Test performance validity measures in amnestic mild cognitive impairment and early Alzheimer Disease. Archives of Clinical Neuropsychology, 31 (4 ), 313–331. 10.1093/arclin/acw014 27084732
Lu PH , Boone KB , Cozolino L , &amp; Mitchell C (2003). Effectiveness of the Rey Osterreith Complex Figure Test and the Meyers and Meyers Recognition Trial in the detection of suspect effort. The Clinical Neuropsychologist, 17 , 426–440.14704893
Manly JJ , &amp; Glymour MM (2021). What the Aducanumab Approval Reveals About Alzheimer Disease Research. JAMA Neurology, 78 (11 ), 1305–1306. 10.1001/jamaneurol.2021.3404 34605885
Marcopulos BA , Caillouet BA , Bailey CM Tussey C , Kent J &amp; Frederick R (2014). Clinical decision making in response to performance validity test failure in a psychiatric setting. The Clinical Neuropsychologist, 28 (4 ), 633–652. doi: 10.1080/12854046.2014.896416 24678658
Martin PK , &amp; Schroeder RW (2020). Base rates of invalid test performance across clinical non-forensic contexts and settings. Archives of Clinical Neuropsychology: The Official Journal of the National Academy of Neuropsychologists, 35 (6 ), 717–725. doi: 10.1093/arclin/acaa017 32318712
Martin PK , Schroeder RW , &amp; Odland AP (2015). Neuropsychologists’ validity testing beliefs and practices: A survey of North American professionals. The Clinical Neuropsychologist, 29 (6 ), 741–776. doi: 10.1080/13854046.2015.1087597 26390099
Marx DM &amp; Goff PA (2005). Clearing the air: The effect of experimenter race on target’s test performance and subjective experience. British Journal of Social Psychology, 44 , 645–657. doi: 10.1348/014466604X17948 16368024
McCarthy FM , Sellers AH , Burns WJ , Smith G , Ivnik RJ , &amp; Malec JF (2003). Prediction of IQ in the Mayo Older Adult Normative sample using multiple methods. Journal of clinical psychology, 59 (4 ), 457–463. 10.1002/jclp.10071 12652637
Merten T , Bossink L , &amp; Schmand B (2007). On the limits of effort testing: symptom validity tests and severity of neurocognitive symptoms in nonlitigant patients. Journal of clinical and experimental neuropsychology, 29 (3 ), 308–318. 10.1080/13803390600693607 17454351
NACC (2015). NACC Uniform Data Set: Researchers data dictionary University of Washington. https://files.alz.washington.edu/documentation/uds3-rdd.pdf
Onwuegbuzie AJ , &amp; Daley CE (2001). Racial differences in IQ revisited: A synthesis of nearly a century of research. Journal of Black Psychology, 27 (2 ), 209–220.
Proto DA , Pastorek NJ , Miller BI , Romesser JM , Sim AH , &amp; Linck JF (2014). The dangers of failing one or more performance validity tests in individuals claiming mild traumatic brain injury-related postconcussive symptoms. Archives of Clinical Neuropsychology, 29 (7 ), 614–624.25252598
Razani J , Burciaga J , Madore M , &amp; Wong J (2007). Effects of acculturation on tests of attention and information processing in an ethnically diverse group. Archives of Clinical Neuropsychology, 22 (3 ), 333–341. 10.1016/j.acn.2007.01.008 17298874
Reitan RM (1958). Validity of the Trail Making test as an indicator of organic brain damage. Perceptual and Motor Skills, 8 , 271–276.
Rey A (1941). L’examen psychologique dans les cas d’encephalopathie traumatique. (Les problems). Archives de Psychologie 28 : 215–285.
Salazar XF , Lu PH , Wen J , &amp; Boone KB (2007). The use of effort tests in ethnic minorities and non-English-speaking and English as a second language populations. In Boone KB (Ed.). Assessment of feigned cognitive impairment: A neuropsychological perspective (pp. 405–427). New York: The Guilford Press.
Sherman D,S , Boone KB , Lu P , &amp; Razani J (2002). Re-examination of a Rey Auditory Verbal Learning Test/Rey Complex Figure Discriminant Function to detect suspect effort. The Clinical Neuropsychologist, 16 , 242–250.12607137
Shirk SD , Mitchell MB , Shaughnessy LW , Sherman JC , Locascio JJ , Weintraub S , &amp; Atri A (2011). A web-based normative calculator for the uniform data set (UDS) neuropsychological test battery. Alzheimer’s Research &amp; Therapy, 3 (6 ), 32.
Suchy Y (2019). Introduction to special issue: Current trends in empirical examinations of performance and symptom validity. The Clinical Neuropsychologist, 33 (8 ), 1349–1343. 10.1080/13854046.2019.1672334 31595824
Sugarman MA , &amp; Axelrod BN (2015). Embedded measures of performance validity using verbal fluency tests in a clinical sample. Applied Neuropsychology: Adult, 22 , 141–146 25153155
Suhr J , Tranel D , Wefel J , Barrash J (1997). Memory performance after head injury: Contributions of malingering, litigation status, psychological factors, and medication use. Journal of Clinical and Experimental Neuropsychology, 19 (4 ), 500–514.9342686
Sweet JJ , Heilbronner RL , Morgan JE , Larrabee GJ , Rohling ML , Boone KB … Conference Participants. (2021). American Academy of Clinical Neuropsychology (AACN) 2021 consensus statement on validity assessment: Update of the 2009 AACN consensus conference statement on neuropsychological assessment of effort, response bias, and malingering. The Clinical Neuropsychologist, 35 (6 ),1053–1106. doi:10.1080/13854046.2021.1896036 33823750
Tan YW , Burgess GH , &amp; Green RJ (2021). The effects of acculturation on neuropsychological test performance: A systematic literature review. The Clinical Neuropsychologist, 35 (3 ), 541–571. 10.1080/13854046.2020.1714740 31996089
Tombaugh TN (1996). Test of memory malingering (TOMM) North Tonowanda, NY: Multi-Health Systems.
Trueblood W (1994). Qualitative and quantitative characteristics of malingered and other invalid WAIS-R and clinical memory data. Journal of Clinical and Experimental Neuropsychology, 16 , 597–607, DOI: 10.1080/01688639408402671 7962361
Victor TL , Boone KB , Serpa JG , Buehler J , &amp; Ziegler EA (2009). Interpreting the meaning of multiple symptom validity test failure. The Clinical Neuropsychologist, 23 (2 ), 297–313.18821138
Wechsler D (1987a). Wechsler Adult Intelligence Scale-Revised San Antonio, Texas: Psychological Corporation.
Wechsler D (1987b). Wechsler Memory Scale-Revised Manual San Antonio, TX: Psychological Corporation.
Weintraub S , Salmon D , Mercaldo N , Ferris S , Graff-Radford NR , Chui H , Cummings J , DeCarli C , Foster NL , Galasko D , Peskind E , Dietrich W , Beekly DL , Kukull WA , &amp; Morris JC (2009). The Alzheimer’s Disease Centers’ Uniform Data Set (UDS): The neuropsychologic test battery. Alzheimer disease and Associated Disorders, 23 (2 ), 91–101.19474567
