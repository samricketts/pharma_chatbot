LICENSE: This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.


101480209
34848
Electron J Stat
Electron J Stat
Electronic journal of statistics
1935-7524

33294093
7720899
10.1214/20-ejs1767
NIHMS1647288
Article
Reconstruction of a directed acyclic graph with intervention
Peng Si School of Statistics, University of Minnesota, 313 Ford Hall, 224 Church St SE, Minneapolis, MN 55455

Shen Xiaotong School of Statistics, University of Minnesota, 313 Ford Hall, 224 Church St SE, Minneapolis, MN 55455

Pan Wei Division of Biostatistics, University of Minnesota, 420 Delaware St. S.E., Minneapolis, MN 55455

pengx179@umn.edu
19 11 2020
17 11 2020
2020
07 12 2020
14 2 41334164
This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.
Identification of causal relations among variables is central to many scientific investigations, as in regulatory network analysis of gene interactions and brain network analysis of effective connectivity of causal relations between regions of interest. Statistically, causal relations are often modeled by a directed acyclic graph (DAG), and hence that reconstruction of a DAG’s structure leads to the discovery of causal relations. Yet, reconstruction of a DAG’s structure from observational data is impossible because a DAG Gaussian model is usually not identifiable with unequal error variances. In this article, we reconstruct a DAG’s structure with the help of interventional data. Particularly, we construct a constrained likelihood to regularize intervention in addition to adjacency matrices to identify a DAG’s structure, subject to an error variance constraint to further reinforce the model identifiability. Theoretically, we show that the proposed constrained likelihood leads to identifiable models, thus correct reconstruction of a DAG’s structure through parameter estimation even with unequal error variances. Computationally, we design efficient algorithms for the proposed method. In simulations, we show that the proposed method enables to produce a higher accuracy of reconstruction with the help of interventional observations.

MSC2020 subject classifications:

Primary 62–09
Causal relations
constrained likelihood
intervention
reconstruction identifiability

1. Introduction

Directed acyclic graph (DAG) models are useful to describe pairwise causal relations between random variables, defined by a certain Markov property [5], with each node representing one variable and each directed edge representing the corresponding pairwise causal relation. DAG models have been widely used in gene and social networks [7, 19]. To identify causal relations, intervention observations are usually collected in addition to observational attributes [16]. The central topic this article addresses is the reconstruction of a DAG model based on interventional data and pertinent issues with respect to the effect of the intervention on the reconstruction of a DAG’s structure.

In the literature, it is generally believed that interventions may help the reconstruction of a DAG’s structure, particularly when a DAG model is not identifiable from data, that is, DAGs in a Markov equivalence class are not distinguishable based on observational data alone [16]. In biological experiments, for instance, intervention occurs in a form of randomized treatments in a clinical trial or a form of gene knockdown or knockout experiments in systems biology. In such a situation, some or all system variables are controlled, permitting direction estimation of ambiguous edges connecting to these controlled variables. Yet exactly how intervention impacts reconstruction of a DAG’s structure remains unknown. Consequently, it is practically important to design a reconstruction method for interventional data, permitting the identification of a DAG’s structure. Most existing methods for intervention [6, 10, 8] assume known intervention, that is, affected variables of the intervention are known a priori before data collection; see [9] for references therein. However, assuming known intervention is impractical, as in system biology, where the effect of various chemicals intervening a system cannot be precisely known. To our knowledge, one exception is a Bayesian method of [4], which is designed for a low-dimensional problem without theoretical guarantee, due to the super-exponential complexity in the number of nodes.

In this article, we propose a novel variance constraint on interventions to fully identify the DAG structure in the framework of unknown intervention. Theoretically, we show in Theorem 1 that a DAG structure is fully identifiable under the constraint, which is otherwise only possible when all the error variances are not the same [18]. Moreover, we propose a constrained maximum likelihood to seek the most efficient interventions by sparsity pursuit, leading to an identifiable reconstruction of a DAG’s structure. Computationally, we develop an efficient algorithm to solve nonconvex minimization subject to the quadratic variance constraint based on the alternating direction method of multipliers (ADMM) [2]. In simulations, we investigate the impact of the intervention on reconstruction and compare the proposed method with its counterpart without intervention. Overall, the proposed method performs well.

This article is organized into seven sections. Section 2 introduces the proposed method and discusses the issue of identifiability due to intervention, followed by the computational development in Section 3. Section 4 establishes the consistency theorems of the proposed method. Section 5 performs some simulations to study the intervention effect, and two real datasets are analyzed. Section 6 summarizes the results. Finally, the Appendix A contains technical details and proofs.

2. Method

Consider a causal model consisting of p random variables Y = (Y1, … , Yp)⊤ described by a DAG, with each node representing one variable and directed edges encoding causal relations between any two variables, where ⊤ denotes the transpose. This model factorizes the joint distribution of Y , P(Y) into a product of conditional distributions of each variable given its parents: P(Y)=∏j=1pp(Yj∣paj), where paj denotes the parent set of Yj and is defined to be empty if Yj has no parents. This factorization is known as the local Markov property [5].

A DAG is modeled by structural equations as (1) Y=AY+ϵ,     ϵ~N(0,D),

where ϵ = (ϵ1, ϵ2, … , ϵp)⊤ represents the latent or unexplained error, D=Diag(σ12,⋯,σp2) is the error covariance matrix and A = (Aij)p×p is an adjacency matrix that uniquely determines a DAG. Here Aij ≠ 0 encodes an edge from node j to node i. In (1), the inverse covariance matrix of Y is Ω = (I – A)⊤D−1(I − A), where I is the identity matrix.

When σ1 = ⋯ = σp, (1) is identifiable [18].Then a DAG’s structure can be reconstructed by estimating A. However, when σ1 = ⋯ = σp breaks down, (1) is usually not identifiable, which means that A is not estimable.

2.1. Intervention or covariate models and variance constraint

To treat non-identifiability in an observational study, consider a model consisting of W intervention variables X = (X1, X2, … , XW)⊤, where the outcome of Y is observed with intervention variables X that may be non-informative.

After incorporating the intervention variables into (1), we obtain that (2) Y=AY+BX+ϵ,     ϵ~N(0,D),

where A, ϵ and D are defined as in (1), B = (Bjw)p×W is an unknown intervention coefficient matrix, whose jwth entry Bjw indicates the directional strength of the intervention of Xw on Yj. When Bjw = 0; j = 1, ⋯ , p, there is no intervention of Xw on Yi, and thus Xw is non-informative. Note that (2) becomes a causal model with covariates X.

In the situation of unequal error variances, with the help of the intervention, we may impose constraints to achieve model identifiability, which otherwise is impossible [18]. Assume that X ~ N(0, ΣX), which is independent of ϵ. Without loss of generality, assume that ΣX = I subsequently because we can reparametrize X as ΣX−1/2X. Then under (2), Y ~ N(0, Ω−1), where Ω = (I − A)⊤ (BB⊤ + D)−1(I − A). In (2), we impose the variance constraint: (3) Var(BX+ϵ)=θI, or BB⊤+D=θI,

where θ &gt; 0 is a parameter to be estimated.

More details are deferred to Sections 3 and 4.

Theorem 1 (Identifiability). Assume that X ~ N(0, ΣX) is independent of ϵ in (2), and Ajk ≠ 0 for all k which is a parent of j; j = 1, 2, ⋯ , p. Under (3), A is identifiable from the distribution of (Y , X).

As suggested by Theorem 1, A in (2) becomes identifiable when (3) is imposed on B, which is otherwise impossible. Note, however, that interventions B leading to identifiable A may not be unique. In what is to follow, we impose a sparsity constraint to identify a most sparse B in terms of the number of nonzero elements of B.

2.2. Constrained maximum likelihood

This section estimates A subject to the DAG requirement while seeking a most sparse B subject to (3). Consequently, the smallest set of informative intervention variables can be identified through B.

Under (2), two data matrices (yij)n×p and (xiw)n×W are observed, with n representing the sample size. Then the negative log likelihood is (4) l(A,B,D)=∑j=1p[−n2log σj2+12σj2∑i=1n(yij−∑k≠jAjkyik−∑w=1WBjwxiw)2].

To identify nonzero entries of A and B, we impose sparsity constraints to regularize: (5) ∑1≤j≠l≤pI(Ajl≠0)≤K1,     ∑1≤j≤p,1≤l≤WI(Bjl≠0)≤K2,

where K1 and K2 are nonnegative integer-valued tuning parameters. Note that the constraint on B removes zero entries thus zero-columns of B, which can be regarded as selection of intervention variables. To reinforce the DAG requirement, we impose acyclic constraints [27] to reinforce the DAG requirement to ensure no loops to occur: (6) λik+I(j≠k)−λjk≥I(Aij≠0);i,j,k=1,⋯,p,i≠j,

where λ = {λjl}p×p is a dual variable matrix.

For computation, we replace the indicator functions in (5) and (6) by its computational surrogate Jτ(z)=min(|z|τ,1) [20] to circumvent the difficulty of non-discontinuity in optimization. This yields (7) ∑1≤j&lt;l≤p Jτ(Ajl)≤K1,     ∑1≤j≤p,1≤l≤W Jτ(Bjl)≤K2,

(8) λik+I(j≠k)−λjk≥Jτ(Aij);i,j,k=1,⋯,p,i≠j,

where Jτ (z) approximates the indicator function as τ → 0+.

Minimizing (4) in (A, B, D) subject to (7), (8), and (3) yields the constrained maximum likelihood estimate (CMLE): minA,B,D l(A,B,D)=∑j=1p[−n2log σj2+12σj2∑i=1n(yij−∑k≠jAjkyik−∑w=1WBjwxiw)2],

subj to ∑1≤j&lt;l≤p Jτ(Ajl)≤K1,     ∑1≤j≤p,1≤l≤W Jτ(Bjl)≤K2,

λik+I(j≠k)−λjk≥Jτ(Aij);i,j,k=1,⋯,p,i≠j,

(9) BB⊤+D=θI,

where (K1, K2, τ) are tuning parameters.

3. Computation

This section develops a computational strategy to solve (9). First, θ is estimated by θ^ through an estimate A^ of A from the method in [27], that is, (10) θ^=1n∑i=1n∑j=1p(yij−∑k≠j,k=1pA^jkyik)2,

where {A^jk}j,k=1,⋯,p are obtained by a constrained maximum likelihood estimate with the sparsity constraint and acyclic constraint, based on the structural equation model (1) with observational data Y alone. Then we solve (9) with θ replaced by θ^ using a blockwise coordinate descent alternating between two blocks (A, B) and D until convergence. In particular, the (A, B)-block is solved via a difference convex (DC) programming followed by an alternating direction method of multipliers (ADMM), while the D-block is updated by gradient descent. More details are further discussed subsequently.

3.1. Optimization subject to the variance constraint

To deal with the variance constraint in (9), we first consider a general constrained minimization subject to the variance constraint as follows: (11) minB f(B),     subj to      BB⊤=Λ,

where f(B) is a cost function and Λ is a diagonal matrix.

For (11), we work with its equivalent form by introducing a dual matrix C to decouple B and the constraint: min(B, C) f(B), subject to CC⊤ = Λ and C − B = 0. Then we apply the alternating direction method of multipliers (ADMM) [2] to obtain its augmented Lagrangian: Lρ(B,C,y)=f(B)+y⊤ vec(C−B)+ρ2‖C−B‖F2, where y∈ℝpW is the Lagrangian multiplier for constraint C−B = 0, and ρ &gt; 0 is the augmented Lagrangian parameter. Then it is further simplified by introducing a dual variable matrix V = {Vjl}p×W to incorporate y⊤ vec(C − B) into the quadratic form (12) min(B,C,V) Lρ(B,C,V)=f(B)+ρ2‖C−B+V‖F2,      subj to      CC⊤=Λ.

Now we apply ADMM to iterate through three steps to solve (12) until convergence. In the kth iteration, (13) C(k+1)=argminC ρ2‖C−B(k)+V(k)‖F2,      subj to CC⊤=Λ,

(14) B(k+1)=argminB f(B)+ρ2‖C(k+1)−B+V(k)‖F2,

(15) V(k+1)=V(k)+C(k+1)−B(k+1).

In (13)–(15), the variance constraint CC⊤ = Λ enters only in (13). Next we provide a closed-form solution of (13) in Lemma 1.

Lemma 1. The solution of (13) can be written as C(k+1) = Λ1/2PO⊤, where a singular value decomposition of (B(k) − V(k))⊤Λ1/2 gives OEP⊤, O∈ℝW×p, E, P∈ℝp×p are the left, diagonal, and right matrices in the decomposition.

3.2. Algorithm for solving (9)

After plugging θ^ into (9), we begin with the update of the (A, B)-block by fixing D at an initial value D0 and optimize (9) with regard to (A, B). A good estimate of D0 can be obtained by solving (9) without the variance constraint, details are given in the Appendix.

To solve (9) with a fixed D, we follow [27] to convert (9) to its equivalent dual form. The procedure contains two steps. First, we apply a DC programming method and decompose the nonconvex constraint function of the nonconvex constraints (7) and (8) into a difference of two convex functions, based on which we construct a sequence of convex approximation of nonconvex constrained sets iteratively, the details are given in the Appendix. Then at the mth iteration, we solve a relaxed subproblem (16). The iteration process continues until a termination criterion is met.

The mth subproblem amounts to min(A,B,λ,ξ)l(A,B)+μ1τ∑1≤j≠l≤p|Ajl|wjl(m−1)+μ2τ∑1≤j≤p,1≤l≤W|Bjl|vjl(m−1),

subj toλjs+τI(l≠s)−λls=|Ajl|1wjl(m−1)+τ(1−wjl(m−1))+ξjls;

j,l,s=1,…,p,j≠l,ξjls≥0,

(16) BB⊤+D=θ^I,

where ξ = {ξjls}p×p×p, ξjls ≥ 0 is a slack variable tensor, and wjl(m−1)=I(|A^jl(m−1)|≤τ) and vjl(m−1)=I(|B^jl(m−1)|≤τ) are obtained from the (m−1)th iteration.

For (16), we apply ADMM method by decoupling (A, B) in the likelihood from the rest part of the cost function and the acyclic constraint. As in Section 3.1, we introduce dual variable tensor y = {yjls}p×p×p, dual variable matrix U = {Ujl}p×(p+W) and V = {Vjl}p×W. Then we minimize the augmented Lagrangian under the variance constraint: min(A,C,B,F,λ,ξ,y,U,V)Lρ(A,C,B,F,λ,ξ,y,U,V)=l(A,B)+μ1τ∑1≤j≠l≤p|Fjl|wjl(m−1)+μ2τ∑1≤j≤p,1≤l≤W|Bjl|vjl(m−1)+∑1≤s≤p∑1≤j≠l≤pρ2(|Fjl|wjl(m−1)+τ(1−wjl(m−1))+ξjls−λjl−τI(l≠s)+λls+yjls)2+ρ2∑1≤j,l≤p(Ajl−Fjl+Ujl)2+ρ2∑1≤j≤p,1≤l≤W(Bjl−Fj,l+p+Uj,l+p)2+ρ2∑1≤j≤p,1≤l≤W(Cjl−Bjl+Vjl)2,

(17) subj to CC⊤+D=θ^I.

Again, we solve (17) over blocks (A, C, B, F, λ, ξ, y, U, V) iteratively until convergence, where analytic updating formulas are given in the Appendix. After ADMM iterations converge, we continue the DC iterations until converge, then the current (A, B)-block is updated.

When updating the D-block, we conduct a gradient descent update for each of (σ12,⋯,σp2) based on the current values of (A^,B^). The gradients for the jth dimension is lj′=−n2σj2−12σj4∑i=1n(yij−∑k≠jA^jkyik−∑w=1WB^jwxiw)2. Then σj2 is updated by (18) σj2=σj2−αlj′,

where α &gt; 0 is the step size.

The computational strategy is summarized in Algorithm 1. Algorithm 1: Constrained maximum likelihood

Step 1. Obtain an estimate θ^ of θ by (10), then plug θ^ into (9).	
Step 2. Fix D at an initial value D0. Set pre-specified error tolerance ϵ0 and the maximum number of iterations M0 for blockwise coordinate descent.	
Step 3. ((A, B)-block) Initialize A and B. Set pre-specified error tolerance ϵ1 and the maximum number of DC iterations M1.	
 Step 3.1. At the mth DC iteration, compute A^(m) and B^(m) cycling through the ADMM updating steps until convergence.	
 Step 3.2. When |l(A^(m+1),B^(m+1),D^)−l(A^(m),B^(m),D^)|≤ϵ1 or m = M1, stop the DC loop and output (A^,B^)=(A^(m),B^(m)).	
Step 4. (D-block) Update D according to (18).	
Step 5. Iterate through steps 3 through 4 until ∑j=1p|lj′|≤ϵ0 or the number of iterations equals to M0, stop and output (A^,B^).	

The computation complexity of Algorithm 1 is of order O(M0M1M2p2(p + W) + n(p + W)2 + p(p + W)3), where M0, M1 and M2 are the maximum number of iterations for (A, B)-step, DC and ADMM, respectively. For each ADMM iteration, the computation complexity is O(p2(p + W)). The preparation phase has O(n(p + W)2 + p(p + W)3) complexity. In practice, the DC loop usually converges in a few iterations, which has finite termination property, c.f., Lemma 2 in [21].

4. Theory

This section develops a theory quantifying the reconstruction error of the CMLE defined in (9). In particular, we first show the estimated θ^ recovers the optimal parameter estimation of the oracle estimator θ^O, which is defined as the maximum likelihood estimate in [27] provided that the true non-zero pattern of the DAG is given. Then we establish reconstruction consistency of the true DAG’s structure defined by adjacency matrix A in the presence of intervention variables X in (2).

Let A, B, and D represent model parameters under (2). Let E = {(i, j) : Aij ≠ 0} be the set of non-zero edges in the graph G, and |E| denote the size of the set. Let Ω = (I − A)⊤ (BB⊤ + D)−1(I − A) = (I − A)⊤ (I − A)/θ be the precision matrix of Y. In what follows, we will assume the variance constraint (3), and by Theorem 1, A is identifiable from the distribution of (Y, X), and thus the graph structure of G is identifiable. Let G0, A0, B0, R0, E0, Ω0, θ0 denote the truth. Let A^O and θ^O denote the oracle estimator, or the maximum likelihood estimate provided that the true set E0 of non-zero edges is given. Let Ω^O=(I−A^O)⊤(I−A^O)/θ^O be the oracle estimator for Ω.

For the observational model, let Aobs be the model parameter, then the precision matrix is Ωobs = (I − Aobs)⊤ (I − Aobs)/θ, whose oracle estimator is Ω^Oobs=(I−A^Oobs)⊤(I−A^Oobs)/θ^O where A^Oobs is the oracle estimator of Aobs.

The degree of reconstructability is defined as Cmin(Ω0)=inf{A≠A0,|E|≤|E0|,A  induces a DAG}−log (1−h2(Ω,Ω0))max(|E0\E|,1),

where h2(Ω, Ω0) is the Hellinger distance between Ω and Ω0 under (2) and the variance constraint (3), and E1 \ E2 denote the set difference between E1 and E2. The degree of reconstructability measures the difficulty of reconstructing the graph, and we require it to be larger than a certain level in order for our proposed method to be consistent in reconstructing the graph structure. For a detailed discussion about the degree of reconstructability of a graph, c.f., [27].

Assumption A.1 (Boundedness). For some positive constants M1 and M2, infΩ cmin(Ω) ≥ M1, sup1≤k≤p |Ωkk| ≤ M2, where cmin(Ω) is the smallest eigenvalue of Ω and Ωkk is the kth diagonal element of Ω.

Assumption A.2 (Boundedness). For some positive constants M3 and M4, infΓ cmin(Γ) ≥ M3 and sup1≤k≤p+W, |Γkk)| ≤ M4, where Π is the covariance matrix of the joint distribution of (Y, X) and Γ = Π−1.

Assumption B (Degree of reconstructability). Cmin(Ω0)≥4d0−1n−1×max(log p,|E0|), for some positive constant d0 &gt; 0, say d0=2271963.

Assumption C. For some positive constants d1, d2 and d3, h2(Ω,Ω0)≥d1h2(Ωτ,Ω0)−d3pτd2,

where Ωτ = (I − Aτ)⊤ (I − Aτ)/θ and Aτ is a truncated version of A with its ijth element AijI(|Aij| ≥ τ).

Assumptions A.1 and A.2 concern the smallest eigenvalues and the maximum diagonal element of Ω. Under Assumption A.1, the likelihood function becomes bounded. Assumption B is a key condition for the consistency of reconstructed graph structure, we require the degree of reconstructability to be no less than a lower bound, which is related to the size of p or |E0|. Assumption C requires the Hellinger distance to be smooth so that we approximate L0 function with its computational surrogate, the TLP function [21] to the desired level by tuning τ.

First, we show that θ is uniquely defined regardless of the value of B.

Lemma 2. Under (2), θ uniquely satisfies (3).

Then we show the optimal parameter estimation of θ achieved by utilizing the observational data in the Theorem 2.

Theorem 2 (Optimal parameter estimation). Under Assumptions A.1 and C, if K1 = |E0| and τ ≤ Cmin(Ω0)M1/4p, then there exists a constant c2 &gt; 0, say c2=42711926, such that for any (n, |E0|, p), P(θ^≠θ^O)≤P(Ω^obs≠Ω^Oobs)≤exp(−c2nCmin(Ω0)+2 log(p(p−1)+1)+3).

Under Assumption B, P(θ^≠θ^O)→0 as n, p, |E0| → ∞.

The next theorem gives a reconstruction error bound, under which we obtain reconstruction consistency of the CMLE as well as its optimal parameter estimation.

Theorem 3 (Error bound and oracle properties). Under Assumptions A.1, A.2 and C, if K1 = |E0|, τ ≤ Cmin(Ω0)M1/4p, then there exists a constant c2 &gt; 0, say c2=42711926, such that for any (n, |E0|, p), P(G^≠G0)≤P(Ω^≠Ω^O)≤exp(−c2nCmin(Ω0)+2 log(p(p−1)+1)+3).

Under Assumption B, P(G^≠G0)→0, Eh2(Ω^,Ω0)Eh2(Ω^O,Ω0)→1 as n, p, |E0| → ∞.

5. Numerical study

5.1. Simulations

This section examines the performance of the proposed method and demonstrates how intervention, as well as the variance constraint, improves the reconstructability of a DAG’s structure. Seven methods are compared, including the proposed method with intervention, that without the variance constraint (3), the observational method without intervention [27], the constraint-based PC algorithm [22], the score-and-search method GES [3], and two hybrid methods, Max-Min Hill-Climbing (MMHC) [24] and ARGES [14]. For PC algorithm and GES, we use R package pcalg, while for MMHC we use R package bnlearn. For ARGES, we use the ARGES-CIG version [14] by first conducting a neighborhood selection using R package huge and then apply the greedy search using pcalg. For the other three methods, we implement in R with the main algorithm written in C, which is also available in the R package intdag [17] https://cran.r-project.org/web/packages/intdag/index.html.

Several performance metrics are used to measure the accuracy of reconstruction of a graph’s skeleton as well as directionality. With respect to the skeleton of a graph, we use the false discovery rate (FDR) and false negative rate (FNR), defined as FDR = FP/(TP + FP) and FNR = FN/(TP + FN), where TP, FP, TN, and FN denote the true positives, false positives, true negatives and false negatives, respectively. These two metrics together measure the abilities to control false discoveries, as well as false negatives. With regard to directionality, we employ the Structural Hamming Distance (SHD), defined as the minimal number of operations required to transform one DAG to the other, including edge insertions, deletions or flips, c.f., [24]. Note that a smaller SHD value indicates two DAGs are closer to each other. To compute the SHD, we use the R-package pcalg. All the metrics are used on the estimation of adjacency matrix A, since we focus on the reconstructability of DAG.

For tuning, PC algorithm and MMHC require one tuning parameter α controlling the significance level for independence tests, yet there is no practical tuning way via a separate tuning set. In this simulation, the significance level is fixed at 0.05. This choice of α seems sensible as the number of estimated edges is roughly the same as the number of edges in the true graph, as shown in the simulation. For ARGES, the first stage of neighborhood selection needs one tuning parameter corresponding to the LASSO penalization, we use the functions huge.path() in the R package huge to select the tuning range based on the data, and the function huge.fit() to select the tuning parameter. For the observational DAG method [27], τ is chosen from a set {0.1, 0.05, 0.01}, and the sparsity regularization parameter μ1 is chosen so that the number of estimated edges roughly ranges from 0 to 100. For our methods, τ and μ1 are selected similarly, and μ2 = r × μ1 with the ratio r selected from {1, 2, 4, 8}. For each method, the optimal tuning parameters are obtained by maximizing the predicted log-likelihood (4) based on an independent tuning set of size 1000 over a set pre-specified grid points.

In simulations, we examine a sparse neighborhood graph and a sparse graph with non-sparse neighborhoods in Examples 1 and 2, respectively. A sparse neighborhood requires each node to have sparse links, but a sparse graph does not necessarily have sparse neighborhoods. To further investigate the operating characteristics of the methods, we consider two additional situations in Examples 3 and 4, in which the true graphs satisfy the variance constraint while other settings resemble Examples 1 and 2, respectively. Finally, to compare the performances of the methods in non-sparse situations, Examples 5 and 6 are added by increasing the sampling probabilities when generating the edges. More details are given in the example settings.

Example 1 (Sparse neighborhood). A DAG with 50 nodes is generated with a random generation mechanism as described in [11]. First, the partial ordering of these nodes is randomly generated. Second, we sample edges according to a binomial distribution with probability 0.02, where the edges are assigned to a weight 0.5. The intervention matrix B is a diagonal matrix with diagonal values being 0.5s, describing a situation that each node in DAG is intervened by exactly one intervention covariate. Third, we set the error variances (σ12,⋯σp2)⊤ to be a sequence from 1.5 to 1 with equally spaced points. Finally, we sample X from a p-dimensional normal distribution N(0, I) and generate Y is generated according to (2).

Example 2 (Non-sparse neighborhood). This example is modified from the previous example to generate a DAG of 50 nodes with a special structure of so-called “one-control-all”, where all directed edges are connected from the first node to the other nodes with connection strength of 0.5. Evidently, the neighborhood of the first node is not sparse, but the overall graph remains sparse. The intervention matrix B and the error variances are the same as in Example 1.

Example 3 (Sparse neighborhood with a causal model satisfying (3)). This example is modified from Example 1, in which the intervention matrix is diagonal, and the squared diagonals are generated from a sequence from 0.5 to 1 with equally spaced points so that the variance constraint is satisfied. The true θ is 2 in (3). Other settings remain the same as Example 1.

Example 4 (Non-sparse neighborhood with a causal model satisfying (3)). This example is modified from Example 2, in which the intervention matrix and θ = 2 are set in the same fashion as in Example 3.

Example 5 (Non-sparse case). This example is modified from Example 1, in which the sampling probability of the binomial distribution is increased to 0.1 when generating the edges, the rest are in the same fashion as in Example 1.

Example 6 (Non-sparse case with a causal model satisfying (3)). This example is modified from Example 3, in which the sampling probability of the binomial distribution is increased to 0.1 when generating the edges, the rest are in the same fashion as in Example 3.

As indicated in Tables 1–4, the proposed method with the variance constraint (2) performs favorably against the other methods. In Examples 1 and 3, it performs the best in all the cases in terms of all the three evaluation metrics. In Example 2, it achieves the top performances in terms of FDR and FNR when n = 100, in terms of FNR when n = 200 and performs the best in terms of all the three metrics when n = 500. In Example 4, it also achieves the top performances in terms of FDR when n = 100, in terms of FDR and SHD when n = 200 and in terms of FNR when n = 500. Interestingly, in Examples 2 and 4, PC algorithm and MMHC do not handle “non-sparse neighborhood” structures well as they fail to return results within 24 hours in the case of n = 500. Also, when n is small, ARGES performs poorly, almost identifying no directed edges, such as in the cases of n = 100 of Examples 1–4 and n = 200 of Examples 3 and 4. One explanation is that when n is small the tuning process of the neighborhood selection step tends to select a large penalty, resulting in a very sparse conditional independent graph (CIG).

Overall, both the variance constraint (3) and intervention lead to improvements of the reconstruction accuracy across all the situations. Specifically, the proposed method has an average amount of improvement (10.00, 10.50, 8.20, 6.80) in terms of SHD over the intervention method without the variance constraint when n = 100 in all the four examples, respectively. The amount of improvement becomes (5.70, 4.20, 8.30, 6.70) when n increases to 200 and (2.00, 14.50, 2.90, 19.00) when n = 500. The improvement remains noticeable in Examples 2 and 4 even when n is large. Interestingly, a DAG’s structure can be well-reconstructed even without the variance constraint, particularly when n is large, for example, n = 500 in Examples 1 and 3. Moreover, the proposed method has an average improvement of (28.50, 33.80, 29.10, 40.20) in SHD over the observational method when n = 100, (27.30, 1.40, 14.50, 60.60) when n = 200 and (11.20, 16.10, 9.60, 21.70) when n = 500. The amount of improvement is significant across all the cases except the case of n = 200 in Example 2.

In Examples 2 and 4, with a non-sparse neighborhood structure, the estimation becomes more challenging. In such situations, we added the simulations with n = 1000, as shown in Tables 5–6, the proposed method with the variance constraint (2) outperforms all the competitors, with SHD very close to 0 in Example 2 and exactly 0 in Example 4. As a comparison, PC and MMHC fail to return results within 24 hours, GES and ARGES perform even worse than the cases with n = 500. For large sample sizes, GES and ARGES tend to have more false positives, with estimated graph structure failing to determine the directions of the edges between the first node and the rest.

As demonstrated in Tables 7–8, the benefits of the proposed methods are evident against the competitors in the non-sparse cases of Examples 5 and 6, where each node in the graph has 10 edges instead of 2 edges on average. The proposed method with the variance constraint continues to perform the best in all cases in the three metrics, except the case of n = 200 in which PC performs better in FDR. When the sample size increases from 200 to 500 and then to 1000, our methods exhibit a larger amount of improvement, as the SHD reduces from 135.80 to 61.70 and to 10.60 in Example 5 while from 164.20 to 41.50 and to 8.60 in Example 6. By comparison, ARGES fails to produce a meaningful graph in all cases, GES yields very large values of FDR, FNR, and SHD. Whereas PC and MMHC do perform better than GES, their performances do not improve much at all when the sample size increases.

In conclusion, intervention clearly has a positive impact on the reconstructability of a DAG through interventional covariates. With the variance constraint, the accuracy of reconstruction can be further enhanced.

5.2. Analysis of Alzheimer’s disease dataset

This section applies our proposed method to analyze the Alzheimer’s disease dataset [25], where 8560 gene expressions were collected for 176 Alzheimer’s disease patients and 187 healthy participants. Our primary goal is to reconstruct a causal network of Alzheimer’s disease-related genes with the help of intervention, and compare the DAG structures for the patient and control groups, for identifying regulatory gene-gene interactions that differentiate these two patient groups.

Biologically, transcription factors (TFs) are proteins controlling the transcription process. By regulating genes, TFs ensure that target genes have right expressions in a cell and during a biological organism. In fact, a TF binds to its target DNA sequence and thus can be mapped to specific genes. For our purpose, we use the database [23] to extract a list of human DNA binding TFs and map them to genes in the dataset, resulting in 1031 mapped TF genes. Then we treat TF genes as intervention covariates to facilitate reconstruction of causal relations encoded by the gene networks.

To identify TF gene expressions associated with Alzheimer’s disease in our dataset, we examine the KEGG database [12]. There, 168 genes in the Alzheimer’s disease pathway, among which the expressions of 99 genes are mapped to the pathway. Among the mapped genes, we perform a two-sample t-test to obtain the significant ones between the patient and control groups, resulting in 43 selected genes at the significance level 0.05.

After pre-processing, we apply the proposed method to both the groups to reconstruct two DAG networks for the disease and control groups with 176 and 187 subjects, involving p = 43 genes as causal variables and W = 1031 TFs as intervention variables, where the tuning parameters of the method are estimated by a five-fold cross validation. As shown in Figure 1, there are 29 and 33 estimated directed connections in the patient and control groups, respectively, with 11 shared common directed connections. Moreover, the two networks share some common sub-structures, particularly, we can find common directed connections from NDUFAB1 to ATP5A1, ATP5G3, and COX7A2. However, different sub-structures are also revealed. In the patient group, several genes, including SDHD, NDUFA1, NDUFAB1, ATP5G3, ATP5A1 and ATP5C1 have directed connections to SDHB, whereas in the control group, only two of them are connected to SDHB. This suggests that the differences in the two DAG networks reflect the disparity in the gene regulatory relations between an Alzheimer’s disease subject and a healthy subject.

Biologically, our estimated directed connections match the known biological pathway of Alzheimer’s disease in the KEGG database. For example, the estimated directed connections between NDU-genes, SDH-genes, ATP-genes and COX-genes match the pathway of the electron transport chain in mitochondria. Also, the estimated connection from CALM3 to PPP3CB matches the biological pathway from calcium-modulated protein(CaM) to protein phosphatase 3 catalytic subunit alpha(PP3CA).

5.3. Analysis of cytometry data

This section applies the proposed method to analyze the flow cytometry data in [19] to reconstruct causal relations between phosphorylated proteins and phospholipids in human primary naive CD4+ T cells of the immune system. These cells were perturbed with molecular interventions to drive the ordering of connections in an intracellular signaling network. The original flow cytometry data contains n = 7466 cell measurements, each consisting of the amount of p = 11 proteins and phospholipids. The data is collected from nine different experiments in which different components in the network are intervened, either by stimulatory cues or inhibitory interventions. For our analysis, we use the continuous version of the original data [19]. Our objective is to reconstruct this network while the consensus network [19] is used as a benchmark for discovery.

For interventional data, we pre-process by selecting nine out of the eleven components, four of which form a DAG with three directed links while remaining five serve as intervention nodes; see Figure 2 for a display of an enlarge network with the nine nodes, known as consensus network, where links between the five intervention nodes are purposely removed for the intervention purpose. Note that the removal of these links does not affect our analysis, because each node is independent of its non-descendants given its direct parents by the local Markov property. Among the five intervention nodes, only three of them are informative with the other two Plcg and PIP2 having no effects on the four nodes to be intervened.

We fit our proposed method with and without interventions. For tuning, we use one-tenth of the samples for training, and the rest for tuning. As displayed in Figures 3 and 4, the proposed method with intervention correctly identifies the directed link from Erk to Akt, while the observational method alters the direction oppositely. Both the methods enable to reconstruct the directed link from Praf to Pmek, whereas they both miss the link from Pmek to Erk. Consequently, the proposed method with intervention identifies more correct directed links than that without intervention. With respect to the intervention effects, our method identifies three intervention links in the network, which rule out the true non-informative intervention nodes Plcg and PIP2. PIP3 is also non-informative in our estimation, and a possible reason is that Akt is already intervened by PKA, and thus the intervention effect of PIP3 on Akt is unnecessary. Similarly, the intervention from PKC to Praf is identified by the proposed method, while bothPKA and PKC have interventions on Praf in the consensus network. Finally, the proposed method yields a sparse intervention pattern.

One plausible explanation of missing the link from Pmek to Erk is that the linear causal model fails to capture nonlinear functional relations among cytometry measurements of proteins Praf, Pmek, Erk, and Akt, as evident from nonlinear patterns revealed by their residual plots of the structural equation model in Figure 5. Another possibility is that the five nodes on the bottom-left in Figure 3 are not originally designed as intervention nodes in the experiments.

6. Discussion

In this paper, a constrained maximum likelihood method is proposed to reconstruct the structure of a DAG using interventional data and efficient ADMM algorithms are developed to solve the optimization problems. In particular, a novel variance constraint is introduced and leverages the information in the interventional data to improve identifiability. Theories are established and it is shown that with the introduction of our variance constraint, the graphical structure is shown to be fully identifiable under some mild assumptions. The theoretical results are also demonstrated in our simulation results, as our proposed method performs well against competitors and can reconstruct the DAG more accurately than the observational method.

The authors thank the editor, the associate editor and anonymous referees for helpful comments and suggestions. Research supported in part by NSF grants DMS-1712564, DMS1721216, DMS-1952539, and NIH grants 1R01GM126002, 2R01HL105397, 1R01AG065636, R01AG069895.

Appendix A: Technical Details

A.1. Computation details for solving (9)

For (9), after plugging in θ^ and fix D, we proceed in two steps. First, we relax nonconvex constraints (7) and (8) using a sequence of convex approximations involving 2p + 1 linear constraints, where each approximation is refined iteratively. Then we solve each subproblem by employing a constrained alternating direction method of multipliers to estimate. The underlying process iterates until convergence.

For convex relaxation of nonconvex constraints (7) and (8) in (9), we employ difference convex (DC) programming similar to [27]. In particular, we decompose Jτ into a difference of two convex functions: Jτ(z)=S1(z)−S2(z)≡min(|z|τ,1)=|z|τ−max(|z|τ−1,0). On this ground, we construct a sequence of convex approximating sets iteratively by replacing S2 in the decomposition at iteration m by its affine majorization at iteration m − 1. (9) becomes min(A,B,λ)l(A,B,λ)

subj to      1τ∑1≤j≠l≤p|Ajl|wjl(m−1)≤K1−∑1≤j&lt;l≤p(1−wjl(m−1)),

1τ∑1≤j≤p,1≤l≤W|Bjl|vjl(m−1)≤K2−∑1≤j≤p,1≤l≤W(1−vjl(k,m−1)),

λjs+τI(l≠s)−λls≥|Ajl|1wjl(m−1)+τ(1−wjl(m−1));

j,l,s=1,…,p,j≠l,

(19) BB⊤+D=θ^I,

where wjl(m−1)=I(‖A^jl(m−1)‖1≤τ), vjl(m−1)=I(|B^jl(m−1)|≤τ);1≤i,j≤p and (A^(m−1),B^(m−1)) is the solution at iteration m − 1.

Now consider a regularization version of (19), with a slack variable ξ added to the inequality constraint, yielding (16).

A.2. Analytic updating expressions for ADMM in (20)

At ADMM iteration step s + 1, the updating formula are A(s+1)=argminALρ(A,C(s),B(s),λ(s),ξ(s),y(s),U(s),V(s)),

C(s+1)=argminCLρ(A(s+1),C,B(s),λ(s),ξ(s),y(s),U(s),V(s)) subj to CC⊤+D^=σ2I,

B(s+1)=argminBLρ(A(s+1),C(s+1),B,λ(s),ξ(s),y(s),U(s),V(s)),

λ(s+1)=argminλLρ(A(s+1),C(s+1),B(s+1),λ,ξ(s),y(s),U(s),V(s)),

ξ(s+1)=argminξLρ(A(s+1),C(s+1),B(s+1),λ(s+1),ξ,y(s),U(s),V(s)), subj to ξijk≥0;i,j,k=1,…,p,j≠k,

yijk(s+1)=yijk(s)+(|Fij(s+1)|+ξijk(s+1)−τλik(s+1)−τI(j≠k)+τλjk(s+1)),

Ujl(s+1)=U(s)+(Ajl(s+1)−Fjl(s+1)),1≤j,l≤p,

Uj,l+p(s+1)=U(s)+(Bjl(s+1)−Fj,l+p(s+1)),1≤j≤p,1≤l≤W,

(20) Vjl(s+1)=Vjl(s)+(Cjl(s+1)−Bjl(s+1)),1≤j≤p,1≤l≤W.

A.2.1. A-step and B-step

For simplicity, denote H = (A, B) be the concatenation of adjacency matrix and intervention matrix, let Z = (Y, X) be the concatenated data matrices. For each row of H, the optimization problem is summarized as minHj,j−12‖zj−Zj−Hj,j−⊤‖2+ρ2‖Hj,j−−Fj,j−(s)+Uj,j−(s)‖F2, where Hj, j− is the jth row of H with Hjj excluded, Zj− is Z with its jth column removed and xj is the jth column of X. The minimizer is the solution to (Zj−⊤Zj−+ρI)Hj,j−=Zj−⊤zj+ρ(Fj,j−(s)−Uj,j−(s)), where the factorization of Zj−⊤Zj−+ρI can be cached to speed up subsequent updates.

A.2.2. C-step

By Lemma 1, the updating formula for C is C(s+1) = Λ1/2PO⊤, where O and P are obtained from singular value decomposition of (B(s) − V (s))⊤ Λ1/2 = OEP⊤.

A.2.3. F-step

F-step updates two parts of the matrix, one is the adjacency matrix, one is the intervention matrix.

Part I: adjacency matrix For i = 1, ⋯ , p and j = 1, ⋯ , p, we solve the following problem: minF μ1∑i,j|Fij|wijm−1+ρ2∑ijk(|Fij|wijm−1+τ(1−wijm−1)−Lijks)+ρ2∑i,j(Aijs+1−Fij+Uijs)2,

where Lijks=λiks+τI(j≠k)−λjks−ξijks−yijks.

We can solve Fij elementwise: Fijs+1={S(Aijs+1+Uijs1+p,μ1−pρ∑kLijksρ(1+p)) if     wijm−1=1, Aijs+1+Uijs if      wijm−1=0, where S(b,λ)={b−0.5λ if b&gt;0.5λ,b+0.5λ if b&lt;−0.5λ,0 otherwise, is the soft-thresholding operator.

Part II: intervention matrix For i = 1, ⋯ , p and j = p + 1, ⋯ , p + W, we solve the following problem: minF μ2∑i,j|Fij|vijm−1+ρ2∑i,j(Aijs+1−Fij+Uijs)2+ρ2∑i,j(Ci,j−ps−Fij+Zi,j−ps)2.

We can solve Fij elementwise: Fijs+1={S(12(Aijs+1+Uijs+Ci,j−ps+Zi,j−ps),μ22ρ) if      vijm−1=1,12(Aijs+1+Uijs+Ci,j−ps+Zi,j−ps) if      vijm−1=0.

A.2.4. λ-step and ξ-step

(λs+1, ξs+1) is updated by λs+1=Mp×pWp×ps+1,

where Mp×p=1τ(100…012p1p…1p11p2p…1p⋮⋮⋱⋮11p…1p2p)

W1js+1=1,

Wiks+1=12(τ+∑j(|Bkjs+1|wij(m−1)+τ(1−wij(m−1))+ξijks+1+yijks)−∑j(|Bkjs+1|wij(m−1)+τ(1−wij(m−1))+ξjiks+1+yjiks));     i≠k,

Wkks+1=12(−(p−1)τ+∑j(|Bkjs+1|wij(m−1)+τ(1−wij(m−1))+ξkjks+1+ykjks)−∑j(|Bkjs+1|wij(m−1)+τ(1−wij(m−1))+ξjkks+1+yjkks)),

for i, j, k = 1, … , p. ξijks+1≔max(0,(τλiks+τI(j≠k)−τλjks−|Bijs+1|−yijks));i,j,k=1,…,p.

A.3. Computation details for estimating D0

A good estimate of D0 is obtained by solving (9) without the variance constraint (3). First, we introduce a re-parametrization to exploit the convexity. Let R = D−1/2, Φ = D−1/2A and Ψ = D−1/2B, then (2) becomes Y = R−1ΦY + R−1ΨX + R−1ϵ, ϵ ~ N(0, I) and Φ = (ϕjk)p×p, Ψ = (ψjw)p×W are scaled versions of adjacency matrix A and intervention matrix B and R = (r1, ⋯ , rp). Under the new parametrization, the likelihood (4) becomes l(Φ,Ψ,R)=∑j=1p[−n log rj+12∑i=1n(rjyij−∑k≠jϕjkyik−∑w=1Wψjwxiw)2], an easy check will confirm that l(Φ, Ψ, R) is convex in (Φ, Ψ, R). Then (9) without variance constraint (3) is written as min(Φ,Ψ,R) l(Φ,Ψ,R)=∑j=1p[−n log rj+12∑i=1n(rjyij−∑k≠jϕjkyik−∑w=1Wψjwxiw)2],

subj to ∑1≤j&lt;l≤p Jτ(ϕjl)≤K1,     ∑1≤j≤p,1≤l≤W Jτ(ψjl)≤K2,

(21) ∑j1=jL+1:1≤k≤L Jτ(ϕjkjk+1)≤L−1; any (j1,…,JL),L=2,…,p.

To solve (21), following the computation strategy illustrated in Appendix 7.1 and 7.2, after transformation, in the mth DC step, we solve (22) min(Φ,Ψ,F,λ,ξ,y,U,R)Lρ(Φ,Ψ,F,λ,ξ,y,U,R)=l(Φ,Ψ,R)+μ1τ∑1≤j≠l≤p|Fjl|wjl(m−1)+μ2τ∑1≤j≤p,1≤l≤W|ψjl|vjl(m−1)+∑1≤s≤p∑1≤j≠l≤pρ2(|Fjl|wjl(m−1)+τ(1−wjl(m−1))+ξjls−λjl−τI(l≠s)+λls+yjls)2+ρ2∑1≤j,l≤p(ϕjl−Fjl+Ujl)2+ρ2∑1≤j≤p,1≤l≤W(ψjl−Fj,l+p+Uj,l+p)2,

which is solved over blocks (Φ, Ψ, F, λ, ξ, y, U, R). The updating formula are similar to those in Appendix 7.2, with one exception, which is R-step, which is illustrated in the following section.

After solving (21), estimates (Φ^, Ψ^, R^) are obtained, then D0=R^−2.

A.3.1. R-step

Denote T = (Φ, Ψ) be the concatenation of Φ and Ψ, let Z = (Y , X) be the concatenated data matrices we solve the minimization problem: minrj−n log rj+12‖rjzj−Zj−Tjj−T‖2 The derivative equation becomes −nrj+∑i=1n(rjzij−∑k≠jTjkzik)zij=0 , yielding a solution rj=b+b2+4na2a, where a=∑i=1nzij2 and b=∑i=1nzij∑k≠jTjkzik.

A.4. Technical proofs

Proof of Lemma 2. Let ϵ′ = BX + ϵ, under (3), (2) reduces to (23) Y=AY+ϵ′,

where ϵ′ ~ N(0, θI). In (23), Ω = (I − A)⊤ (I − A)/θ . Given A and the distribution of Y , the value of θ is unique. This completes the proof. □

Proof of Theorem 1. Let ϵ′= BX + ϵ. Note that X ~ N(0, ΣX). Hence ϵ′ ~ N(0, θI), which is independent of any specific value of B. Now (2) is written as (24) Y=AY+ϵ′.

By (3), ϵ′~ N(0, θI), (24) reduces to an observational model with equal error variance. By Theorem 1 of [18], A is identifiable from the distribution of Y , which in turn is identifiable from the joint distribution (Y , X) that is proportional to the distribution of Y . This completes the proof.□

Proof of Lemma 1. The update step for C in ADMM is (25) C(k+1)=argminC‖C−B(k)+V(k)‖F2, s.t. CC⊤=Λ,

(25) is equivalent to maxC Tr [C⊤ (B(k) − V(k))], s.t. CC⊤ = Λ , we can transform it into a standard form by introducing Q = C⊤ Λ−1/2 and S = (B(k) − V (k+1))⊤ Λ1/2. Then (25) becomes (26) maxQ  Tr [Q⊤S], s.t. Q⊤Q=I.

To solve (26), we can do a singular value decomposition S = OEP⊤, where O∈ℝw×p, E, P∈ℝp×p, O⊤O = P⊤P = PP⊤ = I. Then the cost function in (26) can be written in the form Tr [Q⊤OEP⊤]=Tr [(QP)⊤OE]=∑j=1p[M⊤O]jjEjj, where M=QP∈ℝw×p and [M⊤O]jj denotes the jth diagonal element of the cross product M⊤O. Note that the constraint Q⊤Q = I is equivalent to M⊤M = I since P is an orthogonal matrix. Since Ejj ≥ 0, if we can maximize each of [M⊤O]jj; j = 1, ⋯ , p at the same time, then the problem is solved.

Since [M⊤O]jj=∑i=1WMijOij, by the Cauchy-Schwartz inequality and the fact that M⊤M = O⊤O = I , we have ∑i=1WMijOij≤∑i=1WMij2∑i=1WOij2=1×1=1, where the equality holds if and only if Mij = Oij for i = 1, 2, ⋯ , W. If we maximize each [M⊤O]jj; j = 1, ⋯ , p at the same time, we need M = O, which leads to Q = OP⊤. Then the solution of (25) is C(k+1) = Λ1/2PO⊤. This completes the proof. □

Proof of Theorem 2. When we ignore the intervention covariates X and pull them into the error terms, under the variance constraint (3), (2) reduces to (24) with equal error variances, under which A is identifiable from Ω. Note that Ωobs = (I − Aobs)⊤(I − Aobs)/θ is a function of (Aobs, θ), it follows that P(θ^≠θ^O)≤P(Ω^obs≠Ω^Oobs). The rest of the proof follows from Theorem 3 of [27]. This completes the proof. □

Proof of Theorem 3. First, we define a complexity measure for the size of a space F. The bracketing Hellinger metric entropy of F, denoted by H(⋅,F), is the logarithm of the cardinality of the u-bracketing of F of the smallest size. That is, for a bracket covering S(ϵ,m)={f1l,f1u,⋯,fml,fmu}⊂L2 satisfying max1≤j≤m‖fju−fjl‖2≤ϵ and for any f∈F, there exists a j such that fjl≤f≤fju a.e. P , then H(u,F)=log (min{m:S(u,m)}), where ∥f∥2 = ∫ f2 (z) dμ, with μ the dominating measure.

Denote Eτ = {(i, j) : |Aij ≥ τ}. When K = |E0|, ∑1≤i≠j≤p Jτ(Aij)≤|E0|, so |E^τ|≤|E0|. If E^τ=E0, then ∑1≤i≠j≤p|Aij|I(|Aij|&lt;τ)=0, then A^=A^O. Therefore, it suffices to prove the case when E^τ≠E0.

Define ΩEτ=(I−AEτ)⊤(I−AEτ)/θEτ for any Eτ ⊂ {(i, j) : 1 ≤ i ≠ j ≤ p}. We can partition Eτ as Eτ = (Eτ \ E0) ∩ (E ∪ E0). Let Bkj={ΩEτ:Eτ≠E0,|Eτ∩E0|=k,|Eτ\E0|=j,(d1(|E0|−k)Cmin(Ω0)−d3qτd2)≤h2(ΩEτ,Ω0)};k=0,…,|E0|−1,j=1,…,|E0|−k. Then Bkj has (|E0|k)(p(p−1)−|E0|j) different elements Eτ’s of sizes |Eτ ∩ E0| = k, |Eτ \ E0| = j. By definition {ΩEτ:Eτ≠E0,|Eτ≠E0|≤|E0|,Cmin(Ω0)≤h2(ΩEτ,Ω0)}⊂∪k=0|E0|−1∪j=1|E0|−kBkj. Let L(Ω) = logf(Ω, y, x) where f(Ω, y, x) = f(Ω, y|x)fX(x) is the joint density. Then P(G^≠G0)≤P(Ω^≠Ω^O)≤P*(supΩEτ:Eτ≠E0,|Eτ|≤|E0|(L(ΩEτ)−L(Ω^O))≥0)≤P*(supΩEτ:Eτ≠E0,|Eτ|≤|E0|(L(ΩEτ)−L(Ω0))≥0)≤∑Eτ⊂{(i,j):1≤i&lt;j≤p}:Eτ≠E0,|Eτ|≤|E0|P*(supΩEτ∈Bkj(L(ΩEτ)−L(Ω^O))≥0)≡I,

where P* is the outer measure.

For I, we apply Theorem 1 of [26] to bound each term in the sum. We verify the entropy condition (3.1) there for the bracketing entropy over Bkj. Let Π denote the covariance matrix of the joint distribution of (Y , X) and Γ = Π−1, then Ω is the upper p×p diagonal block of Γ. Let Fkj={f1/2(Γ,⋅,⋅):Ω∈Bkj} be the class of square-root densities. Define Δ=Γ˜−Γ and let λ1, …, λp+W be the eigenvalues of ΠΔΠ,z=(y⊤,x⊤)⊤. Then max1≤i≤p+W λi ≤ λmax(Δ) × λmax(Π) ≤ c(p + W)∥Π∥max following Prob.III.6.14 [1]. By Lemma 6.5 of [28], it can be shown |log f(Γ˜,y,x)−log f(Γ,y,x)|≤max1≤i≤p+Wλi(D(z)+p+W)≤c(p+W)‖Δ‖max(D(z)+p+W),

where λmax denotes the largest eigenvalue, D(z) = λmax(Γ) tr(zz⊤) ≤ M4 × tr(zz⊤). Note that E tr(zz⊤) ≤ c(p + W) for some constant c &gt; 0. By Assumption A.2, λmax2(Π)≤1/M3. Then I′≡∫supΓ˜∈Bδ(Γ)(f1/2(Γ˜,y,x)−f1/2(Γ,y,x))2dμ≤supΓ˜∈Bδ(Γ)c′(p+W)‖Δ‖max2E(D(z)+p+W)2,

for some constant c′ &gt; 0.

Then for some positive constant Q, I′ ≤ Q(p + W)4δ2. By Lemma 1 of [15], it suffices to bound the entropy of Bij. There are |E| nonzero entries of A with (p(p−1)|E|) possible locations. By [13], for u ≥ ϵ2, H(u,Fij)≤c0(log (p(p−1)|E|)+|E|log (min(M21/2,1)u))≤c0(|E|log (ep(p−1)|E|)+|E|log (min(M21/2,1)u))≤c0(|E|log plog (1/u))

Then ϵ=ϵn,p,|E0|=min(1,(2c0)1/2c4−1log (21/2/c3)log p(|E0|/n)1/2) satisfies (27) sup0≤|Eτ|≤|E0|∫2−8ϵ221/2ϵH(t/c3,Fij)dt≤(2|E0|)1/2ϵlog (21/2/c3)≤c4n1/2ϵ2.

for some constants c3, c4 &gt; 0, say c3 = 10, c4=(2/3)5/2512. By Assumption B, Cmin(Ω0)≥ϵn,p,|E0| implies (27), provided that 2d0−1&gt;(2c0)1/2c4−1log (21/2/c3). Using the facts about binomial coefficients: ∑j=1|E0|−k(p(p−1)−|E0|j)≤(p(p−1)−|E0|+1)|E0|−k and (|E0|k)≤|E0|k, we have, by Theorem 1 of [26], that for a constant c2 &gt; 0, say c2=42711926, I≤∑k=0|E0|−1∑j=1|E0|−kP*(supΩEτ∈Bkj(L(ΩEτ)−L(Ω^O))≥0)

≤4∑k=0|E0|−1(|E0|k) exp (−c2n(d1Cmin(Ω0)−d3qτd2))∑j=1|E0|−k(p(p−1)−|E0|j)

≤4∑i=1|E0|exp(−i((c2d1/2)Cmin(Ω0)−log (p(p−1)−|E0|+1)−log |E0|))

≤R(exp(−(c2d1/2)Cmin(Ω0)−log (p(p−1)−|E0|+1)−log |E0|)),

provided that τ ≤ Cmin(Ω0)M1/4p, where R(x) = x/(1 − x). Moreover, since I ≤ 1 and log(p(p − 1) − |E0| + 1) + log|E0|) ≤ 2log((p(p − 1) + 1)/2), I≤5exp(−(c2d1/2)nCmin(Ω0)+2log ((p(p−1)+1)/2))≤exp(−c2nCmin+2log (p(p−1)+1)+3).

Under Assumption B, P(G^≠G0)→0 as n, p, |E0| → ∞. For parameter estimation, Eh2(Ω^,Ω0)≤E[h2(Ω^O,Ω0)I(Ω^=Ω^O)]+P(Ω^≠Ω^O)≤(1+o(1))Eh2(Ω^O,Ω0),

then Eh2(Ω,Ω0)Eh2(ΩO,Ω0)→1 as n, p, |E0| → ∞.

This completes the proof. □

Fig 1. Top: DAG network for Alzheimer’s disease patient group. There are 38 directed connections. Bottom: DAG network for healthy participant group. There are 30 directed connections, 11 of which are shared by the DAG network of patient group.

Fig 2. Consensus intracellular signaling network as benchmark. Four nodes in brown are intervened by five intervention nodes marked in blue, with directed links indicating their intervention directions by solid red lines. Note that no links are present between intervention nodes.

Fig 3. Intracellular signaling network reconstructed by the proposed method with intervention. Two directed links in solid black are correctly identified, denoted by solid black lines, while the directed link from Pmek to Erk is missed, denoted by a dashed black line. Three intervention links are identified by our model (denoted by solid red lines), all of which are present in the consensus network.

Fig 4. Intracellular signaling network estimated by the model without intervention. One directed connection is correctly identified, denoted by the solid black line, while another connection is reversed, denoted by dotted black line, and the directed connection from Pmek to Erk is missed, denoted by dashed black line.

Fig 5. Residual plots from our intervention model. Each subplot corresponds to one node in the DAG network.

Table 1 Averaged false discovery rate (FDR), false negative rate (FNR), and Structural Hamming Distance (SHD), as well as their standard errors for four competing methods based on 10 simulation replications in Example. Here “Our”, “Int”, “Obs”, “PC”, “GES”, “MMHC”, “ARGES” denote the proposed method subject to the variance constraint, the proposed method without the variance constraint, the proposed method without intervention, PC method, GES method, MMHC method and ARGES method, respectively. The best performers are in bold.

(n,p)	Method	FDR(A)	FNR(A)	SHD	
(100,50)	Our	0.17(0.03)	0.01(0.01)	10.60(2.32)	
(100,50)	Int	0.28(0.06)	0.02(0.02)	20.60(6.04)	
(100,50)	Obs	0.39(0.20)	0.23(0.07)	39.10(22.90)	
(100,50)	PC	0.29(0.06)	0.20(0.07)	21.10(5.20)	
(100,50)	GES	0.62(0.05)	0.24(0.06)	64.50(10.28)	
(100,50)	MMHC	0.23(0.08)	0.19(0.09)	22.60(6.08)	
(100,50)	ARGES	0.50(0.00)	1.00(0.01)	50.00(0.00)	
(200,50)	Our	0.05(0.03)	0.00(0.00)	2.50(1.84)	
(200,50)	Int	0.14(0.04)	0.00(0.01)	8.20(2.74)	
(200,50)	Obs	0.37(0.16)	0.13(0.06)	29.80(13.12)	
(200,50)	PC	0.29(0.05)	0.11(0.06)	19.10(3.63)	
(200,50)	GES	0.51(0.05)	0.15(0.08)	43.40(5.25)	
(200,50)	MMHC	0.18(0.05)	0.09(0.05)	16.30(5.14)	
(200,50)	ARGES	0.49(0.04)	0.73(0.15)	48.70(2.41)	
(500,50)	Our	0.01(0.02)	0.00(0.00)	0.50(0.97)	
(500,50)	Int	0.05(0.05)	0.00(0.00)	2.50(2.80)	
(500,50)	Obs	0.18(0.10)	0.00(0.01)	11.70(7.60)	
(500,50)	PC	0.28(0.05)	0.04(0.05)	19.00(3.89)	
(500,50)	GES	0.45(0.06)	0.13(0.06)	35.50(8.40)	
(500,50)	MMHC	0.19(0.07)	0.08(0.06)	13.90(5.53)	
(500,50)	ARGES	0.25(0.06)	0.10(0.07)	18.40(4.40)	

Table 2 Averaged false discovery rate (FDR), false negative rate (FNR), and Structural Hamming Distance (SHD), as well as their standard errors for four competing methods based on 10 simulation replications in Example 2. Here “Our”, “Int”, “Obs”, “PC”, “GES”, “MMHC”, “ARGES” denote the proposed method subject to the variance constraint, the proposed method without the variance constraint, the proposed method without intervention, PC method, GES method, MMHC method and ARGES method, respectively. The best performers are in bold. ‘N/A’ means the method did not return a result after 24 hours. The best performers are in bold.

(n,p)	Method	FDR(A)	FNR(A)	SHD	
(100,50)	Our	0.59(0.19)	0.05(0.05)	81.40(29.54)	
(100,50)	Int	0.63(0.13)	0.07(0.07)	91.90(35.95)	
(100,50)	Obs	0.67(0.16)	0.21(0.09)	115.20(65.37)	
(100,50)	PC	0.84(0.06)	0.90(0.04)	63.20(2.57)	
(100,50)	GES	0.61(0.05)	0.17(0.06)	66.00(10.80)	
(100,50)	MMHC	0.68(0.04)	0.80(0.02)	60.40(3.47)	
(100,50)	ARGES	0.50(0.00)	0.96(0.04)	49.00(0.00)	
(200,50)	Our	0.38(0.18)	0.01(0.02)	35.20(18.56)	
(200,50)	Int	0.37(0.24)	0.00(0.01)	39.40(29.05)	
(200,50)	Obs	0.29(0.25)	0.06(0.06)	36.60(51.85)	
(200,50)	PC	0.78(0.07)	0.82(0.06)	63.10(5.11)	
(200,50)	GES	0.44(0.05)	0.06(0.01)	32.80(7.15)	
(200,50)	MMHC	0.60(0.05)	0.69(0.02)	57.30(4.22)	
(200,50)	ARGES	0.07(0.02)	0.23(0.09)	11.90(4.12)	
(500,50)	Our	0.07(0.16)	0.00(0.01)	6.40(16.78)	
(500,50)	Int	0.29(0.10)	0.00(0.00)	20.90(8.27)	
(500,50)	Obs	0.28(0.18)	0.00(0.00)	22.50(15.20)	
(500,50)	PC	N/A	N/A	N/A	
(500,50)	GES	0.38(0.13)	0.03(0.02)	25.90(20.59)	
(500,50)	MMHC	N/A	N/A	N/A	
(500,50)	ARGES	0.11(0.14)	0.04(0.01)	7.30(15.02)	

Table 3 Averaged false discovery rate (FDR), false negative rate (FNR), and Structural Hamming Distance (SHD), as well as their standard errors for four competing methods based on 10 simulation replications in Example 3. Here “Our”, “Int”, “Obs”, “PC”, “GES”, “MMHC”, “ARGES” denote the proposed method subject to the variance constraint, the proposed method without the variance constraint, the proposed method without intervention, PC method, GES method, MMHC method and ARGES method, respectively. The best performers are in bold. ‘NaN’ in the FDR column means the ARGES method does not have any discoveries in all the replications.

(n,p)	Method	FDR(A)	FNR(A)	SHD	
(100,50)	Our	0.11(0.05)	0.04(0.04)	7.60(3.41)	
(100,50)	Int	0.22(0.08)	0.05(0.04)	15.80(5.63)	
(100,50)	Obs	0.44(0.08)	0.14(0.05)	36.70(9.48)	
(100,50)	PC	0.30(0.05)	0.21(0.07)	22.80(4.54)	
(100,50)	GES	0.61(0.05)	0.21(0.09)	63.30(7.97)	
(100,50)	MMHC	0.27(0.09)	0.22(0.11)	26.50(8.61)	
(100,50)	ARGES	NaN	1.00(0.00)	50.00(0.00)	
(200,50)	Our	0.04(0.06)	0.00(0.00)	2.30(3.40)	
(200,50)	Int	0.16(0.11)	0.00(0.01)	10.60(9.07)	
(200,50)	Obs	0.25(0.06)	0.03(0.04)	16.80(4.92)	
(200,50)	PC	0.27(0.06)	0.12(0.05)	18.00(4.14)	
(200,50)	GES	0.53(0.04)	0.17(0.05)	44.70(6.43)	
(200,50)	MMHC	0.22(0.07)	0.08(0.06)	18.10(5.34)	
(200,50)	ARGES	0.50(0.00)	1.00(0.00)	50.00(0.00)	
(500,50)	Our	0.01(0.01)	0.00(0.00)	0.40(0.52)	
(500,50)	Int	0.06(0.02)	0.00(0.00)	3.30(1.34)	
(500,50)	Obs	0.17(0.05)	0.02(0.02)	10.00(3.40)	
(500,50)	PC	0.27(0.04)	0.05(0.03)	18.10(3.31)	
(500,50)	GES	0.42(0.08)	0.10(0.06)	32.00(8.51)	
(500,50)	MMHC	0.14(0.04)	0.03(0.03)	8.80(3.01)	
(500,50)	ARGES	0.43(0.06)	0.46(0.16)	41.80(6.07)	

Table 4 Averaged false discovery rate (FDR), false negative rate (FNR), and Structural Hamming Distance (SHD), as well as their standard errors for four competing methods based on 10 simulation replications in Example 4. Here “Our”, “Int”, “Obs”, “PC”, “GES”, “MMHC”, “ARGES” denote the proposed method subject to the variance constraint, the proposed method without the variance constraint, the proposed method without intervention, PC method, GES method, MMHC method and ARGES method, respectively. The best performers are in bold. ‘N/A’ means the method did not return a result after 24 hours. The best performers are in bold. ‘NaN’ in the FDR column means the ARGES method does not have any discoveries in all the replications.

(n,p)	Method	FDR(A)	FNR(A)	SHD	
(100,50)	Our	0.65(0.09)	0.16(0.09)	93.40(41.51)	
(100,50)	Int	0.66(0.12)	0.15(0.05)	100.20(49.61)	
(100,50)	Obs	0.76(0.02)	0.22(0.08)	133.60(20.30)	
(100,50)	PC	0.87(0.05)	0.91(0.04)	65.70(3.86)	
(100,50)	GES	0.63(0.04)	0.24(0.06)	68.70(9.83)	
(100,50)	MMHC	0.68(0.03)	0.79(0.02)	60.20(2.70)	
(100,50)	ARGES	NaN	1.00(0.00)	49.00(0.00)	
(200,50)	Our	0.27(0.15)	0.20(0.03)	21.30(14.74)	
(200,50)	Int	0.33(0.13)	0.02(0.04)	28.00(15.24)	
(200,50)	Obs	0.61(0.09)	0.05(0.05)	81.90(29.97)	
(200,50)	PC	0.82(0.09)	0.86(0.06)	63.60(5.72)	
(200,50)	GES	0.46(0.04)	0.06(0.01)	34.60(7.18)	
(200,50)	MMHC	0.61(0.04)	0.70(0.02)	57.30(3.71)	
(200,50)	ARGES	0.50(0.00)	0.97(0.04)	49.00(0.00)	
(500,50)	Our	0.14(0.05)	0.00(0.00)	8.10(2.88)	
(500,50)	Int	0.29(0.24)	0.00(0.01)	27.10(23.13)	
(500,50)	Obs	0.38(0.05)	0.00(0.00)	29.80(5.85)	
(500,50)	PC	N/A	N/A	N/A	
(500,50)	GES	0.35(0.06)	0.04(0.00)	17.90(4.95)	
(500,50)	MMHC	N/A	N/A	N/A	
(500,50)	ARGES	0.05(0.02)	0.05(0.02)	2.80(0.92)	

Table 5 Averaged false discovery rate (FDR), false negative rate (FNR), and Structural Hamming Distance (SHD), as well as their standard errors for four competing methods based on 10 simulation replications in Example 2. Here “Our”, “Int”, “Obs”, “PC”, “GES”, “MMHC”, “ARGES” denote the proposed method subject to the variance constraint, the proposed method without the variance constraint, the proposed method without intervention, PC method, GES method, MMHC method and ARGES method, respectively. The best performers are in bold. ‘N/A’ means the method did not return a result after 24 hours. The best performers are in bold.

(n,p)	Method	FDR(A)	FNR(A)	SHD	
(1000,50)	Our	0.02(0.02)	0.00(0.00)	1.20(1.23)	
(1000,50)	Int	0.09(0.06)	0.00(0.00)	5.00(3.40)	
(1000,50)	Obs	0.17(0.12)	0.00(0.00)	11.10(8.37)	
(1000,50)	PC	N/A	N/A	N/A	
(1000,50)	GES	0.58(0.01)	0.00(0.00)	58.80(2.20)	
(1000,50)	MMHC	N/A	N/A	N/A	
(1000,50)	ARGES	0.46(0.15)	0.00(0.01)	44.40(14.90)	

Table 6 Averaged false discovery rate (FDR), false negative rate (FNR), and Structural Hamming Distance (SHD), as well as their standard errors for four competing methods based on 10 simulation replications in Example 4. Here “Our”, “Int”, “Obs”, “PC”, “GES”, “MMHC”, “ARGES” denote the proposed method subject to the variance constraint, the proposed method without the variance constraint, the proposed method without intervention, PC method, GES method, MMHC method and ARGES method, respectively. The best performers are in bold. ‘N/A’ means the method did not return a result after 24 hours. The best performers are in bold.

(n,p)	Method	FDR(A)	FNR(A)	SHD	
(1000,50)	Our	0.00(0.00)	0.00(0.00)	0.00(0.00)	
(1000,50)	Int	0.04(0.03)	0.00(0.01)	2.10(1.79)	
(1000,50)	Obs	0.07(0.14)	0.00(0.01)	5.20(10.00)	
(1000,50)	PC	N/A	N/A	N/A	
(1000,50)	GES	0.57(0.01)	0.00(0.00)	58.00(2.36)	
(1000,50)	MMHC	N/A	N/A	N/A	
(1000,50)	ARGES	0.50(0.00)	0.00(0.0)	49.10(0.32)	

Table 7 Averaged false discovery rate (FDR), false negative rate (FNR), and Structural Hamming Distance (SHD), as well as their standard errors for four competing methods based on 10 simulation replications in Example 5. Here “Our”, “Int”, “Obs”, “PC”, “GES”, “MMHC”, “ARGES” denote the proposed method subject to the variance constraint, the proposed method without the variance constraint, the proposed method without intervention, PC method, GES method, MMHC method and ARGES method, respectively. The best performers are in bold. ‘NaN’ in the FDR column means the ARGES method does not have any discoveries in all the replications.

(n,p)	Method	FDR(A)	FNR(A)	SHD	
(200,50)	Our	0.34(0.11)	0.02(0.21)	135.80(66.63)	
(200,50)	Int	0.47(0.14)	0.21(0.34)	211.20(74.60)	
(200,50)	Obs	0.53(0.01)	0.04(0.02)	277.30(9.79)	
(200,50)	PC	0.27(0.07)	0.78(0.03)	203.40(7.37)	
(200,50)	GES	0.86(0.02)	0.63(0.06)	604.10(23.94)	
(200,50)	MMHC	0.60(0.07)	0.90(0.02)	236.60(4.84)	
(200,50)	ARGES	NaN	1.00(0.00)	250.00(0.00)	
(500,50)	Our	0.18(0.11)	0.01(0.02)	61.70(43.24)	
(500,50)	Int	0.36(0.28)	0.21(0.32)	175.20(198.45)	
(500,50)	Obs	0.44(0.07)	0.11(0.21)	194.40(41.48)	
(500,50)	PC	0.19(0.04)	0.71(0.02)	187.50(5.70)	
(500,50)	GES	0.83(0.03)	0.64(0.05)	505.80(31.71)	
(500,50)	MMHC	0.55(0.06)	0.87(0.02)	225.70(6.58)	
(500,50)	ARGES	NaN	1.00(0.00)	250.00(0.00)	
(1000,50)	Our	0.02(0.01)	0.03(0.04)	10.60(10.51)	
(1000,50)	Int	0.09(0.06)	0.03(0.02)	29.20(19.47)	
(1000,50)	Obs	0.10(0.11)	0.04(0.04)	39.30(40.73)	
(1000,50)	PC	0.18(0.04)	0.69(0.02)	181.30(7.24)	
(1000,50)	GES	0.80(0.02)	0.71(0.03)	394.40(15.90)	
(1000,50)	MMHC	0.55(0.05)	0.85(0.02)	222.20(6.92)	
(1000,50)	ARGES	NaN	1.00(0.00)	250.00(0.00)	

Table 8 Averaged false discovery rate (FDR), false negative rate (FNR), and Structural Hamming Distance (SHD), as well as their standard errors for four competing methods based on 10 simulation replications in Example 6. Here “Our”, “Int”, “Obs”, “PC”, “GES”, “MMHC”, “ARGES” denote the proposed method subject to the variance constraint, the proposed method without the variance constraint, the proposed method without intervention, PC method, GES method, MMHC method and ARGES method, respectively. The best performers are in bold. ‘NaN’ in the FDR column means the ARGES method does not have any discoveries in all the replications.

(n,p)	Method	FDR(A)	FNR(A)	SHD	
(200,50)	Our	0.39(0.10)	0.04(0.03)	164.20(69.71)	
(200,50)	Int	0.50(0.06)	0.04(0.02)	243.60(50.96)	
(200,50)	Obs	0.51(0.15)	0.16(0.21)	273.10(126.29)	
(200,50)	PC	0.26(0.08)	0.78(0.02)	202.30(6.77)	
(200,50)	GES	0.86(0.03)	0.63(0.06)	588.30(27.56)	
(200,50)	MMHC	0.60(0.07)	0.90(0.02)	237.70(6.13)	
(200,50)	ARGES	NaN	1.00(0.00)	250.00(0.00)	
(500,50)	Our	0.14(0.04)	0.00(0.01)	41.50(15.35)	
(500,50)	Int	0.27(0.16)	0.10(0.26)	104.80(83.12)	
(500,50)	Obs	0.36(0.10)	0.02(0.02)	148.80(56.29)	
(500,50)	PC	0.16(0.03)	0.69(0.02)	178.80(5.25)	
(500,50)	GES	0.83(0.02)	0.65(0.04)	485.20(26.17)	
(500,50)	MMHC	0.59(0.07)	0.88(0.02)	231.30(5.42)	
(500,50)	ARGES	NaN	1.00(0.00)	250.00(0.00)	
(1000,50)	Our	0.02(0.01)	0.02(0.03)	8.60(6.50)	
(1000,50)	Int	0.10(0.04)	0.04(0.02)	31.00(14.91)	
(1000,50)	Obs	0.13(0.06)	0.04(0.03)	47.40(20.37)	
(1000,50)	PC	0.18(0.05)	0.68(0.02)	180.00(7.39)	
(1000,50)	GES	0.80(0.04)	0.71(0.06)	382.80(23.86)	
(1000,50)	MMHC	0.57(0.07)	0.85(0.03)	223.50(7.15)	
(1000,50)	ARGES	NaN	1.00(0.00)	250.00(0.00)	


References

[1] Bhatia R (2013). Matrix analysis 169 Springer Science &amp; Business Media.
[2] Boyd S , Parikh N , Chu E , Peleato B and Eckstein J (2011). Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning 3 1–122.
[3] Chickering DM (2003). Optimal structure identification with greedy search. The Journal of Machine Learning Research 3 507–554.
[4] Eaton D and Murphy KP (2007). Exact Bayesian structure learning from uncertain interventions. In International Conference on Artificial Intelligence and Statistics 107–114.
[5] Edwards D (2000). Introduction to graphical modelling. Springer Verlag.
[6] Ellis B and Wong WH (2008). Learning causal Bayesian network structures from experimental data. Journal of the American Statistical Association 103 778–789.
[7] Friedman N (2004). Inferring cellular networks using probabilistic graphical models. Science Signalling 303 799.
[8] Fu F and Zhou Q (2013). Learning sparse causal Gaussian networks with experimental intervention: regularization and coordinate descent. Journal of the American Statistical Association 108 288–300.
[9] Hauser A and Bühlmann P (2012). Characterization and greedy learning of interventional Markov equivalence classes of directed acyclic graphs. The Journal of Machine Learning Research 13 2409–2464.
[10] Huang S , Li J , Ye J , Fleisher A , Chen K , Wu T and Reiman E (2013). A Sparse Structure Learning Algorithm for Gaussian Bayesian Network Identification from High-Dimensional Data. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35 1328–1342.
[11] Kalisch M and Bühlmann P (2007). Estimating high-dimensional directed acyclic graphs with the PC-algorithm. The Journal of Machine Learning Research 8 613–636.
[12] Kanehisa M and Goto S (2000). KEGG: kyoto encyclopedia of genes and genomes. Nucleic acids research 28 27–30.10592173
[13] Kolmogorov AN and Tikhomirov VM (1959). ε-entropy and ε-capacity of sets in function spaces. Uspekhi Matematicheskikh Nauk 14 3–86.
[14] Nandy P , Hauser A , Maathuis MH (2018). High-dimensional consistency in score-based and hybrid structure learning. The Annals of Statistics 46 3151–3183.
[15] Ossiander M (1987). A central limit theorem under metric entropy with L2 bracketing. The Annals of Probability 15 897–919.
[16] Pearl J (2003). Statistics and causal inference: A review. Test 12 281–345.
[17] Peng S , Shen X and Pan W (2019). intdag: Reconstruction of a Directed Acyclic Graph with Interventions R package version 1.0.1
[18] Peters J and Bühlmann P (2013). Identifiability of Gaussian structural equation models with equal error variances. Biometrika, first published online, doi: 10.1093/biomet/ast043 .
[19] Sachs K , Perez O , Pe’er D , Lauffenburger DA and Nolan GP (2005). Causal protein-signaling networks derived from multiparameter single-cell data. Science 308 523.15845847
[20] Shen X , Pan W and Zhu Y (2012). Likelihood-based selection and sharp parameter estimation. Journal of the American Statistical Association 107 223–232.22736876
[21] Shen X , Pan W , Zhu Y and Zhou H (2013). On constrained and regularized high-dimensional regression. Annals of the Institute of Statistical Mathematics 1–26.
[22] Spirtes P , Glymour CN and Scheines R (2000). Causation, prediction, and search 81 The MIT Press.
[23] Tripathi S , Christie KR , Balakrishnan R , Huntley R , Hill DP , Thommesen L , Blake JA , Kuiper M and Lægreid A (2013). Gene Ontology annotation of sequence-specific DNA binding transcription factors: setting the stage for a large-scale curation effort. Database 2013 .
[24] Tsamardinos I , Brown LE and Aliferis CF (2006). The max-min hill-climbing Bayesian network structure learning algorithm. Machine Learning 65 31–78.
[25] Webster JA , Gibbs JR , Clarke J , Ray M , Zhang W , Holmans P , Rohrer K , Zhao A , Marlowe L , Kaleem M (2009). Genetic control of human brain transcript expression in Alzheimer disease. The American Journal of Human Genetics 84 445–458.19361613
[26] Wong WH and Shen X (1995). Probability inequalities for likelihood ratios and convergence rates of sieve MLEs. The Annals of Statistics 23 339–362.
[27] Yuan Y , Shen X , Pan W and Wang Z (2018). Constrained likelihood for reconstructing a directed acyclic Gaussian graph. Biometrika 106 109–125.30799877
[28] Zhou S , Shen X , Wolfe DA (1998). Local asymptotics for regression splines and confidence regions. The annals of statistics 26 1760–1782.
