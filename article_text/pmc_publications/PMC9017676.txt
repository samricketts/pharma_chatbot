LICENSE: This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.


9875635
7591
Stat
Stat
Stat
0038-9986

35450402
9017676
10.1002/sta4.433
NIHMS1746382
Article
Sequential Pathway Inference for Multimodal Neuroimaging Analysis
Li Lexin 1
Shi Chengchun 2
Guo Tengfei 3
Jagust William J. 4
1 Department of Biostatistics and Epidemiology, University of California, Berkeley, CA, USA
2 Department of Statistics, London School of Economics and Political Science, London, UK
3 Institute of Biomedical Engineering, Shenzhen Bay Laboratory, Shenzhen, China
4 Helen Wills Neuroscience Institute, University of California, Berkeley, CA, USA
* Correspondence: Lexin Li, Department of Biostatistics and Epidemiolog, University of California, Berkeley, CA, USA. lexinli@berkeley.edu
8 10 2021
12 2022
15 10 2021
01 12 2023
11 1 e433This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.
Motivated by a multimodal neuroimaging study for Alzheimer’s disease, in this article, we study the inference problem, i.e., hypothesis testing, of sequential mediation analysis. The existing sequential mediation solutions mostly focus on sparse estimation, while hypothesis testing is an utterly different and more challenging problem. Meanwhile, the few mediation testing solutions often ignore the potential dependency among the mediators, or cannot be applied to the sequential problem directly. We propose a statistical inference procedure to test mediation pathways when there are sequentially ordered multiple data modalities and each modality involves multiple mediators. We allow the mediators to be conditionally dependent, and the number of mediators within each modality to diverge with the sample size. We produce the explicit significance quantification and establish the theoretical guarantees in terms of asymptotic size, power, and false discovery control. We demonstrate the efficacy of the method through both simulations and an application to a multimodal neuroimaging pathway analysis of Alzheimer’s disease.

Alzheimer’s disease
Boolean matrix
Directed acyclic graph
High-dimensional inference
Mediation analysis
Multimodal neuroimaging analysis

pmc1 | INTRODUCTION

Alzheimer’s disease (AD) is an irreversible neurodegenerative disorder, characterized by progressive impairment of cognitive functions, then global incapacity and ultimately death. It is the leading form of dementia, and is currently affecting 5.8 million American adults aged 65 years or older. Its prevalence continues to grow, and is projected to reach 13.8 million by 2050 (Alzheimer’s Association 2020). Multimodal neuroimaging is frequently used in AD research, where different brain characteristics are measured using different imaging technologies for a common set of subjects. Notable AD imaging biomarkers include, among others, grey matter cortical thickness measured by structural magnetic resonance imaging (MRI), amyloid-beta (Aβ) protein deposition measured by positron emission tomography (PET) using a variety of radiopharmaceuticals such as 18F-florbetapir or 18F-florbetaben tracer, and tau protein measured by PET using radiopharmaceuticals such as 18F-flortaucipir tracer. While brain grey matter atrophy is a well-known measure associated with AD progression, Aβ and tau are two hallmark pathological proteins believed to be part of the driving mechanism of AD. Models of AD pathophysiology hypothesize a temporal sequence in which disruptions in Aβ production, clearance, or both initiate a biological cascade that leads to Aβ plaque formation, then neurofibrillary tangles of tau, followed by structural atrophy and neuronal dysfunction, and ultimately clinical decline in cognitions (Jack et al. 2013 2010). These models have been continuously supported by new evidences (see, e.g., Gordon et al. 2018; Guo, Korman, Baker, Landau, &amp; Jagust 2020; Jack et al. 2019, among others). However, many questions about the nature of this pathological cascade and its spatial distribution in the brain remain unanswered. As part of the Berkeley Aging Cohort Study, multimodal neuroimages were collected, and a key scientific question is to understand how Aβ deposition affects cognitive decline through possible intermediate pathways of regional tau deposition, then cortical thinning.

This problem can be cast into the framework of sequential mediation analysis. Mediation analysis seeks to identify and explain the mechanism, or pathway, that underlies an observed relationship between an exposure and an outcome variable, through the inclusion of an intermediate variable known as a mediator. It decomposes the effect of exposure on the outcome into a direct effect and an indirect effect, the latter of which indicates whether the mediator is on a pathway from the exposure to the outcome. In our AD example, the Aβ deposition, summarized by a single measure for each subject, serves as the exposure. The cognitive decline, measured by the decrease of memory scores at two consecutive visits about one year apart, serves as the outcome. The brain regional tau deposition and cortical thickness, each summarized by a vector of measures for multiple brain regions for each subject, serve as potential mediators.

Mediation analysis was first proposed with a single mediator (Baron &amp; Kenny 1986), and has been extended to the setting of multivariate and high-dimensional mediators; see VanderWeele (2016) for a review and the references therein. However, most of the existing work has focused on a single modality of features as potential mediators. By contrast, we target the problem of mediation analysis that involves multiple modalities and each modality includes multivariate mediators. Moreover, different sets of mediators are sequentially ordered on the potential pathways following certain biological constraints. For instance, in our example, tau deposition precedes cortical thickness that mediates cognition.

Recently, Lai, Shih, Huang, and Wang (2020) proposed a probit model for multiple sequentially ordered mediators for a dichotomous outcome, but only considered a univariate mediator for each modality. It is far more challenging to analyze multi-modality of multivariate mediators. Zhao, Li, and Caffo (2020) proposed a penalized Lasso approach for two modalities of sequentially ordered and multivariate mediators. However, they focused on estimation, instead of inference for sequential mediation analysis. Even though estimation and inference are closely related, and both can in effect identify important pathways, estimation does not produce an explicit quantification of statistical significance, and does not explicitly control the false discovery. By contrast, we aim at statistical inference for sequential mediation analysis with multi-modality of multivariate mediators.

Inference, or hypothesis testing, for individual mediators is challenging. The key difficulty lies in the fact that the total number of potential paths that go through any mediator is super-exponential in the number of mediators, rendering almost any testing procedure ineffective. Consequently, most existing mediation inference solutions either explicitly impose that the mediators are conditionally independent given the exposure, or simply ignore any potential directed paths among the mediators (e.g., Djordjilović et al. 2019; Huang &amp; Pan 2016; Zhang et al. 2016). Though proven useful in some applications, ignoring potential paths and interactions among the mediators seems not sensible in plenty of other scientific applications; e.g., different brain regions are generally conceived to influence each other, and different genes are expected to interact with each other. More recently, Chakrabortty, Nandy, and Li (2018) proposed a mediation test based on interventional calculus, and Shi and Li (2021) proposed a test based on logic of Boolean matrices, while both allowed interactions among the mediators. Nevertheless, none of the existing mediation inference solutions tackle multi-modality of mediators.

In this article, we propose a statistical inference procedure to test mediation pathways when there are sequentially ordered multiple modalities and each modality involves multivariate mediators. We allow the mediators to be conditionally dependent, and the number of mediators within each modality to diverge with the sample size. We produce the explicit significance quantification, and establish the theoretical guarantees, including the asymptotic size and power of the test, and a valid false discovery control. Our proposal makes several useful contributions. Scientifically, understanding mediation pathways of different imaging modalities in AD progression is a crucial but open question. Our test offers the first solution that does not have to impose any restrictive conditional independence condition and is theoretically guaranteed. Moreover, even though motivated by a multimodal neuroimaging example, our test is applicable to a wide range of multimodal analyses, e.g., the multi-omics data analysis (Richardson, Tseng, &amp; Sun 2016), and the multimodal healthcare application (Cai, Wang, Li, &amp; Liu 2019). Methodologically, our test extends Shi and Li (2021); however, it is far from a straightforward extension. Particularly, both our test and that of Shi and Li (2021) rely on an estimator of the directed acyclic graph (DAG) that encodes the directional relationships among all the variables. In our setup, we need to incorporate the sequential constraint to characterize the orders of multiple modalities of mediators. Naively employing the initial estimator in Shi and Li (2021) would fail this constraint, then fail the test. Actually, most existing DAG structure estimation methods neglect this constraint, and we need to devise an approach to carefully embed this constraint into DAG estimation. As such, our proposal also makes a useful addition to the toolbox of mediation inference.

The rest of the article is organized as follows. We formally specify the model and the hypotheses in Section 2. We develop the testing procedure in Section 3, and study the asymptotic properties in Section 4. We present the simulations in Section 5, and revisit the multimodal neuroimaging application in Section 6. We conclude the paper with a discussion in Section 7. We relegate all technical proofs to the online supplement.

2 | MODEL AND HYPOTHESES

We begin with a Gaussian graphical model, based on which we formulate the mediation testing problem and formally define our hypotheses. For simplicity, we only consider two sets of mediators, while our proposal can be extended to more than two modalities.

Let Y∈ℝ denote the outcome variable, E∈ℝ denote the exposure variable, X1=X11…,X1d1⊤∈ℝd1 and X2=X21…,X2d2⊤∈ℝd2 denote two sets of mediators. Let U=E,X1⊤,X2⊤,Y⊤∈ℝd1+d2+2 collect all variables, and suppose it follows a Gaussian graphical model that is associated with a directed acyclic graph, (1) (U−μ)=W0(U−μ)+ε.

Here μ= E(U), W0∈ℝd1+d2+2×d1+d2+2 encodes the directional relations among the variables in U, and ε∈ℝd1+d2+2 is a vector of random errors. We assume the error variables in ε follow a normal distribution with mean zero and variance σ2. The constant error variance is to ensure the identifiability of W0 and to simplify the analysis. This condition is often imposed; see, e.g., Peters and Bühlmann (2014); Yuan, Shen, Pan, and Wang (2019). Meanwhile, it is also possible to relax this condition; see Shi and Li (2021) for more discussion.

For any two variables Ui, Uj in U, if W0,j,i ≠ 0, then an arrow is drawn from Ui to Uj, i.e., Ui → Uj. In this case, Ui is called a parent of Uj. A directed path between Ui and Uj is a sequence of distinct nodes from Ui to Uj:Ui→Ui1→⋯→Uik→Uj for some {Ul}1≤l≤k, and Ui is called an ancestor of Uj. Suppose the random variables in U comply with the sequential mediation framework as illustrated in Figure 1. That is, no potential mediator in X1 and X2 is a parent of E, no mediator in X2 is a parent of any mediator in X1, and the response Y is not a parent of any other variables. Under this framework, we can decompose W0 accordingly as (2) W0=00d1⊤0d2⊤0W0,1W1,10d1×d20d1W0,2W1,2W2,20d2W0,3W1,3⊤W2,3⊤0∈ℝd1+d2+2×d1+d2+2,

where W0,1∈ℝd1, W0,2∈ℝd2, W0,3∈ℝ, W1,1∈ℝd1×d1, W1,2∈ℝd2×d1, W1,3∈ℝd1, W2,2∈ℝd2×d2, and W2,3∈ℝd2.

Based on model (1), we aim at testing the following pair of hypotheses: H0(q1, q2): There does not exist a path from the exposure E to the outcome Y that passes through some mediator X1,q1 in X1 and some mediator X2,q2 in X2;

H1(q1, q2): There exists a path from the exposure E to the outcome Y that passes through some mediator X1,q1 in X1 and some mediator X2,q2 in X2,

for some q1 = 1, …, d1, and q2 = 1, …, d2. In Figure 1, the null hypothesis means that, at least one potential pathway denoted by (ii), (iv) and (vi) is completely missing in this diagram. We remark that, this form of null hypothesis is motivated by our AD study, as the cascading pathway of Aβ, regional tau deposition, cortical thickness shrinking and cognition is of particular interest. Meanwhile, we can test other forms of null hypothesis; see our discussion in Section 7.

To test the above null hypothesis, we observe that it can be decomposed into the union of the following three null hypotheses: H0*0,q1: There does not exist a directed path from the exposure E to X1,q1;

H0*q1,q2+d1: There does not exist a directed path from X1,q1 to X2,q2;

H0*q2+d1,d1+d2+1: There does not exist a directed path from X2,q2 to the outcome.

To further elaborate on the relation between H0(q1, q2) and H0*0,q1, H0*q1,q2+d1, H0*q2+d1,d1+d2+1, we consider a directed path ζ:E→X1,i1→⋯→X1,im1→X1,q1→X2,j1→⋯→Xj,m2→Xj,q2→X2,k1→⋯→X2,km3→Y. The total effect of E on Y attributed to this path can be written as (3) ωζ=W0,1,i1 ∏t=1m1−1W1,1,it+1,it W1,1,q1,im1︸ωζ(1)W1,1,j1,q1 ∏t=1m2−1W1,2,jt+1,jt W1,2,q2,jm2︸ωζ(2)W2,2,k1,q2 ∏t=1m3−1W2,2,kt+1,kt W2,3,km3︸ωζ(3),

where W0,1, W1,1, W1,2, W2,2 and W2,3 are the submatrices defined in (2). Under H0(q1, q2), we have ωζ = 0 for any ζ. By the decomposition in (3), this is equivalent to requiring ωζ(1), ωζ(2), or ωζ(3) to be zero for any ζ. Note that ωζ(1), ωζ(2), and ωζ(3) correspond to the total effects from E to X1,q1, from X1,q1 to X2,q2, and from X2,q2 to Y, which in turn are characterized by H0*0,q1, H0*q1,q2+d, and H0*q2+d1,d1+d2+1, respectively.

By the union-intersection principle, it suffices to derive the individual p-values, p* (0,q1), p* (q1, q2 + d1) and p* (q2, d1 + d2 + 1), for H0*0,q1, H0*q1,q2+d, and H0*q2+d1,d1+d2+1, respectively, then compute the final p-value for H0(q1, q2) as pq1,q2=max p*0,q1,p*q1,q2+d1,p*q2,d1+d2+1.

Next, we propose the tests to compute p*(0, q1), p*(q1, q2 + d1), and p*(q2, d1 + d2 + 1), respectively.

3 | TESTING PROCEDURE

We first develop a testing procedure to test H0(q1, q2) through H0*0,q1, H0*q1,q2+d, and H0*q2+d1,d1+d2+1, for a given pair q1 and q2. We then augment it with a multiple testing procedure for all q1 = 1, …, d1 and q2 = 1, …, d2 with false discovery control.

3.1 | Testing procedure for a given pair of mediators

We begin with a summary of the testing procedure for a given pair q1 and q2 in Algorithm 1. We then detail the main steps of this algorithm.

Suppose we observe n i.i.d. copies of the data, {ui}1≤i≤n, where ui=ei,x1,i⊤,x2,i⊤,yi⊤∈ℝd1+d2+2. We first split the data {1, 2, ⋯, n} into two equal halves I1 and I2. The purpose of data splitting is to ensure a valid type-I error control under minimal conditions. This strategy has been commonly used in high-dimensional inferences in recent years (Chernozhukov et al. 2018; Romano &amp; DiCiccio 2019). One issue with data splitting is the potential loss of power due to the usage of only a fraction of data. In our setting, we construct two test statistics based on both halves of data, then combine them to derive the final p-value. We show the test constructed this way achieves a good power both asymptotically and numerically.

3.2 | Initial estimation of weight matrices

Next, we estimate W1,1, W1,2 and W2,2 in (2) based on each half I1 and I2 of the data.

Specifically, to estimate W1,1, we first linearly regress each variable X1j in X1 on the exposure variable E, using the data in the subset Is, s = 1,2. This is a simple regression with a single response and a single predictor, and it yields a residual estimate ε˜1,1,i,j(s), i∈Is, j = 1, … d1, s = 1, 2. We then apply a DAG estimation method to the residual to obtain an estimate of W1,1. There are multiple choices for this purpose, e.g., Yuan et al. (2019); Zheng, Aragam, Ravikumar, and Xing (2018). In our implementation, we have chosen the method of Zheng et al. (2018). That is, we obtain (4) W˜1,1(s)=arg minW∈ℝd1×d1∑i∈Isε˜1,1,i(s)−Wε˜1,1,i(s)22+λ1,1Is∑j,k=1d1Wj,k,    subject to   trace{exp(W∘W)}=d1,

where ε˜1,1,i(s)=ε˜1,1,1,i(s),…,ε˜1,1,d1,i(s)⊤∈ℝd1, λ1,1 is a penalty parameter, Is is the cardinality of Is and equals the number of data observations in split s, trace(A) is the trace of a matrix A, exp(A) is the matrix exponential of A, and ◦ is the Hadamard product. We tune the sparsity parameter λ1,1 by cross-validation. We also remark that, we only require the resulting DAG estimator to be estimation consistent, which is much weaker than requiring it to be selection consistent. The latter means all the zero entries of the DAG estimator have to match those of the true DAG. For the method of Zheng et al. (2018), Shi and Li (2021) established its estimation consistency.

To estimate W2,2, the idea is similar. We first regress each variable X2j in X2 on both the exposure variable E and all the variables in X1, again using the data in the subset Is, s = 1, 2. Note that X1 can be high-dimensional. We thus employ a penalized linear regression approach, i.e., min∑i∈Isx2,i,j−ei,x1,i⊤βj(s)−β0,j(s)2+∑k=1d2+1Is pλ2,1βj,k(s),

where pλ2,1 is some penalty function with the tuning parameter λ2,1. In our implementation, we have chosen the MCP penalty function of Zhang (2010), and tune λ2,1 by the Bayesian information criterion (BIC). This penalized regression in turn yields a residual estimate ε˜i,j,2,2(s), i∈Is, j = 1, …, d1, s = 1, 2. We then apply the DAG estimation method of Zheng et al. (2018) again to ε˜i,2,2(s)=ε˜i,1,2,2(s),…,ε˜i,d1,2,2(s)⊤∈ℝd2, and obtain (5) W˜2,2(s)=arg minW∈ℝd2×d2∑i∈Isε˜2,2,i(s)−Wε˜2,2,i(s)22+λ2,2Is∑j,k=1d2Wj,k,    subject to    trace{exp(W∘W)}=d2.

To estimate W1,2, we first compute the residual ε˜1,2,i(s)=x2,i−W˜2,2(s)x2,i∈ℝd2, i∈Is, where W˜2,2(s) is obtained from (5). We then regress each variable in this residual on both the exposure variable E and all the variables in X1, using the data in the subset Is, s = 1, 2. Again, considering the high-dimensionality of X1, we employ a penalized linear regression approach like MCP (Zhang 2010). That is, we estimate the jth row of W1,2 as, (6) W˜1,2,j(s),w0,j(s)=arg minwj(s),w0,j∑i∈Isε˜1,2,i,j(s)−ei,x1,i⊤wj(s)−w0,j2+∑k=1d2+1Is pλ1,2wj,k(s),

for j = 1, …, d2, and pλ1,2 is some penalty function with the tuning parameter λ1,2.

Putting these estimators together, we obtain the matrix, W˜1:2(s)=W˜1,1(s)0d1×d2W˜1,2(s)W˜2,2(s)∈ℝd1+d2×d1+d2.

3.3 | Refined and debiased estimation

Next, we discuss how to obtain an estimator of W0∈ℝd1+d2+2×d1+d2+2 based on W˜1:2(s) from the previous step, and how to further debias this estimator.

Specifically, we begin with setting the first row of W˜(s) to be a zero vector. We then obtain the (j + 1)th row of W˜(s), for j = 1, …, d1 + d2, by fitting a penalized linear regression using the data in the subset Is, s = 1, 2. The response of this regression is X1,j when j = 1, …, d1, and X2,j−d1 when j = d1 + 1, …, d1 + d2, the predictors come from the jth row of W˜1:2,j,k(s), i.e., W˜1:2,j,1(s),…,W˜1:2,j,d1+d2(s), and the estimated regression coefficient is the (j+1)th row of W˜(s). We end this step by obtaining the last row of W˜(s), from the regression coefficient of fitting another penalized linear regression with Y as the response, and E, X1, X2 as the predictors. This step gives an estimator of the entire (d1 + d2 + 2) × (d1 + d2 + 2) matrix of W0, and also helps improve the estimation accuracy of the initial estimator of W˜1:2(s).

We then compute a binary matrix B^(s) of the same dimension as W˜(s), and set each entry of B^(s) as one if and only the corresponding entry in W˜(s) is nonzero. This step serves for the screening purpose, as it helps reduce both the number of potential paths and the variance of the subsequent test statistics, which in turn helps improve the power of our test.

Given the estimator W˜(s), we further employ the cross-fitting strategy to compute a debiased estimator W˜(s) for W0. We first estimate the set of ancestors of the jth variable in U, j = 1, …, d1 + d2 + 2. Toward that end, let B0 denote the binary version of W0. Let ⊕ denote the Boolean matrix addition operator, such that the (j1, j2)th entry of A1 ⊕ A2 equals maxA1,j1,j2,A2,j1,j2, and ⊗ denote the Boolean matrix multiplication operator such that the (j1, j2)th entry of A1 ⊗A2 equals maxk minA1,j1,k,A2,k,j2. Define A(k) as the Boolean matrix power k of the square matrix A in a recursive fashion, such that A(k) = A(k−1) ⊗ A. Define B0,*=B0⊕B0(2)⊕…⊕B0d1+d2+2. Then, the key observation is that, finding the set of ancestors is equivalent to finding the transitive closure of the Boolean matrix B0 (Fischer &amp; Meyer 1971). That is, the set of ancestors of the jth variable is, (7) ACT(j)=1≤k≤d1+d2+2:B0,*,k,j≠0,

We note that, since B0 is a binary matrix, the maximum and minimum operators are equivalent to the logic operators “or” and “and” in Boolean algebra. This motivates us to estimate ACT(j) by (8) ACTj,B^(s)=1≤k≤d1+d2+2:B^*,k,j(s)≠0.

We also note that, we always include the exposure E in the set of ancestors, and include all the potential mediators when estimating the ancestors of the outcome Y.

We next debias W˜j1,j2(s) using the other half of the data in Isc, for any entry that B^j1,j2(s)≠0, (9) W^j1,j2(s)=∑i∈Iscu˜i,j2−β^j1,j2(s)⊤u˜iu˜i,j1−∑j≠j2u˜i,jW˜j1,j(s)∑i∈Iscu˜i,j2u˜i,j2−β^j1,j2(s)⊤u˜i,

where u˜i,j=ui,j−u¯j, u¯j is the sample average, and u˜i=ui,1,…,ui,d1+d+2+2⊤∈ℝd1+d+2+2, β^j1,j2(s) is the (j1, j2)th entry of the coefficient vector obtained from the penalized regression, β^j1,j2(s)=arg minβ:βj2=0supp(β)∈ACTj1,B^(s)1Isc∑i∈Iscu˜i,j2−β⊤u˜i2+∑k:k≠j2,k∈ACTj1,B^(s)pλβk,

and the support of β is constrained to the estimated set of ancestors as in (8). We remark that, we have used the cross-fitting strategy in the estimation of W^(s), since it is based on the complement data set Isc, instead of the data set Is that is used to construct W^(s). This strategy guarantees that each entry of the debiased estimator W^(s) is asymptotically normal, regardless of whether the estimator W^(s) is selection consistent or not.

3.4 | Test statistic and p-value

We next compute the test statistic and the corresponding critical value for H0*q1,q2.

Similar to (7), we can show that H0*q1,q2 holds, if and only if the (q2 + 1, q1 + 1)th entry of W0,*=W0⊕W0(2)⊕…⊕W0d1+d2+2 equals zero, where |A| denotes the matrix of the same dimension as A whose (j1, j2)th entry is Aj1,j2. Note that W0,* is a real-valued matrix. A larger value of W0,*,q1,q2 implies a stronger evidence against the null hypothesis. This motivates us to define our test statistic by the (q2 + 1, q1 + 1)th entry of the debiased version of the matrix, i.e., IscW^*(s)=IscW^(s)⊕W^(s)(2)⊕…⊕W^(s)d1+d2+2.

The purpose of using the Boolean algebra here is to ensure the resulting test statistic has a tractable limiting distribution. More specifically, under H0*q1,q2, W^*(s) is stochastically dominated by the maximum of Gaussian random variables whose distribution can be well approximated via bootstrap; see the proof of Theorem 1 for more details. By contrast, the asymptotic distribution of the usual power of the matrix W^(s) is extremely challenging to derive (Shi &amp; Li 2021). The p-value associated with this test statistic is then given by, (10) p^*(s)q1,q2=1R∑b=1RIT(s,b)q1,q2≥IscW^*,q2+1,q1+1(s),

where R is the number of bootstrap replicates, and T(s,b)(q1, q2) is a bootstrap sample. The explicit form of T(s,b)(q1, q2) is similar to the one in Shi and Li (2021), and we present the detailed definition in Section S1 of the Supplementary Material. The validity of multiplier bootstrap is ensured by the asymptotic normality of W^j1,j2(s).

We then compute the p-value, p^*q1,q2=2 minp^*(1)q1,q2,p^*(2)q1,q2 for H0*q1,q2. By the union-intersection principle, we obtain the p-value for the null H0(q1, q2), (11) p^q1,q2=max p^*0,q1,p^*q1,q2+d1,p^*q2+d1,d1+d2+1.

For a given significance level α, we reject H0(q1, q2) if p^q1,q2≤α.

3.5 | Multiple testing procedure

Finally, we present a multiple testing procedure for simultaneous inference of all pairs of q1 = 1, …, d1 and q2 = 1, …, d2. The objective is to identify all significant pairs (q1, q2) such that the null H0(q1, q2) is rejected, meanwhile to control the false discovery properly. We adopt and extend the ScreenMin procedure of Djordjilovic et al. (2019) to our setting.

Given the individual p-values p^(s)q1,q2, q1 = 1, …, d1, q2 = 1, …, d2, we begin by computing the minimum p-values, p^min(s)q1,q2=min p^(s)0,q1,p^(s)q1,q2+d1,p^(s)q2+d1,d1+d2+1.

We then screen and select those mediators whose corresponding p^min(s)q1,q2 is smaller than a threshold c(s), with c(s) determined adaptively as c(s)=max c∈α/d1d2,…,α/2,α:cH0(s)(c)≤α, and H0(s)(c) denotes the set of prescreened mediators when the threshold value is c. We next order the mediators in H0(s) according to p^(s)q1,q2 obtained from (11). We then apply the Benjamini-Hochberg procedure to the ordered mediators, and select h(s) pairs with the smallest p-values, where h(s)=max i:p^(i)(s)≤(iα)/2H0(s)∑j=1H0(s)j−1, and p^(i)(s) denotes the ith minimum p-value of the set p^max(s)q1,q2:1≤p1≤d1,1≤p2≤d2. Letting H(s) denote the selected important pairs for each half of the data, s = 1, 2, respectively. We set the final set of selected pairs as H=H(1)∪H(2).

4 | THEORETICAL GUARANTEES

We first establish the consistency of our test by deriving the asymptotic size and power. We then show that our multiple testing procedure achieves a valid FDR control.

We begin with two regularity conditions. Define the limit of the estimator β^j1,j2(s) as β0,j1,j2(s)=arg minβ:βj2=0,supp(β)∈ACTj1,W˜(s)EUj2−β⊤U2.

Let W˜j(s) and W0,j denote the jth row of W˜(s) and W0, respectively. For any directed path ζ:E=U1→Ui1→⋯→Uik→Ud1+d2+2=Y, define the signal strength along this path by ωζ*=min W0,i1,1,minj∈1,⋯,k−1W0,ij+1,ij,W0,d1+d2+2,ik. Under H0(q1, q2), we have ωζ*=0 for any path that passes through X1,q1 and X2,q2. Under the H1(q1, q2), there exists at least one such path that ωζ*&gt;0.

(C1) With probability approaching one, ACTj,W˜(s) as defined in (8) contains all parents of j, for any j = 1, …, d1 + d2 + 2 and s = 1, 2.

(C2) With probability approaching one, there exist some constants κ1, κ2, κ3, κ4 &gt; 0, such that κ1 + κ2 &gt; 1/2, β^j1,j2(s)−β0,j1,j2(s)2=On−κ1, and W˜j(s)−W0,j2=On−κ2.

Conditions (C1) and (C2) are both mild. Condition (C1) is weaker than the sure screening condition that requires W˜(s) to be selection consistent. It holds when some omega-min condition is satisfied (van de Geer &amp; Bühlmann 2013). Condition (C2) essentially requires the oracle parameters W0,j and β0,j1,j2(s) to satisfy certain sparsity constraints. The convergence rates of W˜j(s) and β^j1,j2(s) are satisfied for most of the commonly used penalized regression methods, including Lasso, SCAD, MCP, or Dantzig selector (Bickel, Ritov, &amp; Tsybakov 2009; Fan &amp; Lv 2011; Ning &amp; Liu 2017).

We next derive the asymptotic size and power of the proposed testing procedure for H0(q1, q2) for a given pair of q1 and q2.

Theorem 1 (Asymptotic size). Suppose conditions (C1) and (C2) hold. Suppose d1 + d2 = O(nκ3) for some κ3 ≥ 0, and ∥W0∥2 is bounded. Then, for a given significance level 0 &lt; α &lt; 1, and any pair (q1, q2) of the mediators, for q1 = 1, …, d1 and q2 = 1, …, d2, PrH0q1,q2 is rejected∣H0q1,q2 holds≤α+o(1).

Theorem 2 (Asymptotic power). Suppose the conditions in Theorem 1 hold. Suppose there exists some path ζ from E to Y that passes through X1,q1 and X2,q2, such that ωζ*≫max n−κ2,n−1/2log n. Then, PrH0q1,q2 is rejected∣H0q1,q2 holds→1.

Finally, we show that our proposed multiple testing procedure asymptotically controls the false discovery rate.

Theorem 3 (Asymptotic FDR control). Suppose the conditions in Theorem 1 hold. Then the selected set of mediators H satisfies that FDR(H)≤α+o(1).

5 | SIMULATIONS

We first describe the simulation setup. We next examine the empirical size and power of testing a pair of mediators, then the empirical FDR control of multiple testing.

5.1 | Simulation setup

We simulate the data following models (1) and (2). Specifically, we set μ to a vector of ones and the variance of the random errors σ2 = 1. We generate the adjacency matrix W0 as follows: We begin with a zero matrix, then replace every entry W0,j1,j2 in the lower off-diagonals by the product of two random variables Rj1,j2(1)Rj1,j2(2), where Rj1,j2(1)~Bernoullip1, if j2 = 0, or j1 = d + 1, and Rj1,j2(1)~Bernoullip2, otherwise, and Rj1,j2(2) is uniformly distributed on [−1.4, −0.6] ∪ [0.6, 1.4]. Here p1 and p2 control the number of significant mediators, and all variables are generated independently. We consider two settings of the number of mediators, d1 = d2 = 35, and d1 = d2 = 70. For the first setting, we set p1 = 0.05, p2 = 0.1, and the sample size n = 200, 400, and for the second setting, we set p1 = 0.025, p2 = 0.05, and n = 300, 500. Figure 2 shows one instance of the generated W0 for the two settings. In the first setting, 5 pairs of mediators have nonzero sequential mediation effects, whereas in the second setting, 11 pairs are significant.

5.2 | Empirical size and power

We first evaluate the empirical performance of our test for a single pair of mediators (q1, q2). We compare it with a sequential test modified from the interventional calculus-based method that was proposed by Chakrabortty et al. (2018). To adopt Chakrabortty et al. (2018) to the sequential mediation setting, we note that, by the union-intersection principle, it suffices to test H0*0,q1, H0*q1,q2+d1, and H0*q2+d1,d1+d2+1. We then test each of these two hypotheses using Chakrabortty et al. (2018). On the other hand, we remark that the interventional calculus-based test does not directly target the null H0*q1,q2. Instead, it constructs a confidence interval for the aggregated causal effects along all paths from q1 to q2, and reject the null if zero is not covered by the confidence interval.

We evaluate the performance by the empirical rejection rate, i.e., the percentage of times the test rejects the null hypothesis at the significance level α = 5% out of 500 data replications. This criterion reflects the empirical size of the test when the null hypothesis holds, and the empirical power otherwise. We also compute the average receiver operating characteristic (ROC) curves, aggregated over 500 replications, when we vary the significance level α.

Figure 3 and Figure 4 report the results for the setting of d1 = d2 = 35, and d1 = d2 = 70, respectively. We make a few observations. First, our test achieves a valid size under the null hypothesis. In the first setting, the empirical rejection rate is well below the nominal level for all cases. In the second setting, our test has a few inflated type-I errors when the sample size n is small, but the results improve with a larger sample size. Second, our test consistently achieves a larger empirical power over the method of Chakrabortty et al. (2018), which fails to identify two significant pairs of mediators in the first setting, and five significant pairs in the second setting. This decreased power may be due to that the effects calculated by Chakrabortty et al. (2018) along different paths may have different signs, and thus may cancel each other. Consequently, it may result in a zero sum, even though there are significant positive and negative mediation effects along the paths. Finally, we see that the ROC curve of our test consistently lies above that of Chakrabortty et al. (2018) in all settings as α varies, demonstrating a competitive performance of our test.

5.3 | Multiple testing FDR control

We next evaluate the empirical performance of our multiple testing procedure. We compare it with the standard Benjamini-Yekutieli (BY) procedure. For the latter, instead of applying ScreenMin to determine the set H0(s), one simply sets H0(s)=q1,q2:1≤q1≤d1,1≤q2≤d2, i.e., the set of all pairs of mediators. We evaluate the performance by the false discovery rate (FDR) and the true positive rate (TPR) over 500 data replications.

Figure 5 reports the results under the varying significance level α from 0.1 to 0.4. It is seen that both methods achieve a valid false discovery control, in that the FDRs are very close to zero. However, our method is more powerful than the BY procedure, as reflected by a considerably larger TPR in all cases. Actually, when d1 = d2 = 35 and n = 200, the BY procedure fails to identify any significant sequential mediators.

6 | MULTIMODAL AD PATHWAY ANALYSIS

We revisit the motivating example of multimodal AD pathway analysis. Amyloid-beta plaques and neurofibrillary tangles are two key hallmarks of AD, and appear 20 years or more before the presence of manifest clinical symptoms (Jack et al. 2013). Aβ and tau aggregation is evaluated using PET imaging, and the downstream consequence of neurodegeneration is examined with structural MRI. Recent studies generally support a unidirectional model of AD pathogenesis in which Aβ appears early, followed by deposition of abnormal tau aggregates, and eventually subsequent neurodegeneration and cognitive decline (Jack et al. 2010 2019) While it is generally conceived that the association between Aβ deposition and cognition may be mediated by tau deposition and cortical thickness, it remains unclear in which brain regions tau deposition and cortical thickness mediate the association of Aβ deposition and subsequent cognitive decline. In this analysis, we aim to determine the brain regions of tau aggregation and cortical thickness shrinking that are involved in subsequent cognitive decline using a dataset from the Berkeley Aging Cohort Study.

Each participant received concurrent Aβ PET imaging using the 18F-florbetapir (FBP) or 18F-florbetaben (FBB) tracers, tau PET imaging using the 18F-flortaucipir (FTP) tracer, and 3T structural MRI, all acquired within a year. The PET data was acquired in 5-min frames from 50–70min (FBP), 90–110min (FBB), and 75–105 min (FTP) post-injection, averaged over time, and processed with Freesurfer v5.3.0. All fully preprocessed PET scans were coregistered to the structural MRI scan that was closest in time to the baseline PET. Regions of interest (ROIs) were defined on each structural MRI scan using Freesurfer, and were used to extract regional florbetapir, florbetaben, and flortaucipir uptake from the co-registered PET images. FBP or FBB standardized uptake value ratios (SUVRs) were calculated by referring regional florbetapir or florbetaben to that in the whole cerebellum. SUVR in a composit cortical area made up of frontal, cingulate, parietal and temporal regions was used to represent total Aβ deposition. This is to serve as the exposure variable E, which takes a continuous value, in our sequential mediation analysis. FTP SUVRs in 34 Freesurfer-defined cortical ROIs and amygdala were calculated by using inferior cerebellar gray matter intensity normalization (Maass et al. 2017). These regional tau deposition measures serve as the first modality of mediators, with d1 = 35, in our analysis. All structural MRI scans were processed with the Freesurfer cross-sectional pipeline to derive ROIs in each subject’s native space using the Desikan-Killiany atlas (Desikan et al. 2006). Cortical thickness in 34 Freesurfer-defined cortical ROIs were calculated from the Freesurfer output. These regional cortical thickness measures serve as the second modality of mediators, with d2 = 34.

Each participant also received a PACC composite testing score at each of two consecutive visits with a median of 1.1 years in between. The PACC score combines tests that assess episodic memory, timed executive function, and global cognition. This score has been well established as showing sensitivity to decline in prodromal and mild dementia, and with sufficient range to detect early decline in the preclinical stages of AD (Donohue, Sperling, &amp; Others 2014). Following Guo et al. (2020), we compute the change of the PACC score over the two time points using a linear mixed effect model including time, age at baseline scan, sex and education as covariates, and a random slope and intercept for each participant. We treat this change of the PACC score as the outcome variable Y. The dataset we analyze consists of n = 182 subjects who were classified as the Aβ positive group, who are biologically regarded as individuals on the AD continuum following the research framework of the National Institute on Aging and Alzheimer’s Association (Jack et al. 2018).

We apply the proposed test to this data, with the false discovery level controlled at α = 0.1. We regress out age, sex and education from both the exposure E and the potential mediators X1 and X2. Figure 6 shows the identified pathways, as well as the identified regions overlaid on a template brain. It is seen that, there is a pathway from Aβ to tau deposition in the inferior parietal region, then to cortical thickness in the entorhinal, inferior temporal, precuneus, inferior parietal, and superior parietal regions, then to the cognitive change as measured by the PACC score. A few other significant pathways include banks of the superior temporal sulcus tau to entorhinal cortical thickness, rostral middle frontal tau to lateral occipital cortical thickness, and superior frontal tau to transverse temporal cortical thickness. These findings are interesting and are consistent with the literature. Particularly, the inferior parietal region is very likely the most typical tau deposition for individuals on the AD continuum, because this region captures the most of the regions of downstream neurodegeneration that leads to subsequent cognitive decline. Consistent with our findings, Bischof et al. (2016) found AD patients had higher tau deposition in inferior parietal and more hypometabolism in inferior parietal, inferior temporal and superior parietal regions than cognitively healthy elderly adults. Das et al. (2018) found high inferior parietal tau deposition was related to reduced cortical thickness in entorhinal, inferior temporal, inferior parietal Aβ positive individuals, which was partially in line with our findings. Cho et al. (2016) found higher tau deposition in inferior parietal, middle temporal, inferior temporal, and more cortical thinning in entorhinal, inferior parietal, inferior temporal, precuneus, superior parietal, middle temporal and superior temporal in late tau stages (VVI) but not in earlier tau stages (I- IV) when compared to stage 0 individuals without tau deposition according to tau images, supporting the spatial patterns of tau and cortical thickness identified on the AD continuum in our study. Similarly, Harrison et al. (2019) also found high tau deposition in inferior parietal, and significant longitudinal gray matter loss in inferior parietal, superior parietal, precuneus and inferior temporal in AD patients.

7 | DISCUSSION

In this article, we have proposed a statistical hypothesis testing procedure to test mediation pathways when there are sequentially ordered multiple modalities and each modality involves multivariate mediators. Our proposal addresses an important type of scientific questions, and provides a useful tool to the mediation analysis in general.

In Section 2, we have primarily focused on testing the potential pathway passing through (ii), (iv) and (vi) in Figure 1, which has been motivated by our AD study. Meanwhile, the proposed inferential framework can handle other forms of hypotheses as well. We discuss them with more details in Section S2 of the Supplementary Material.

Supplementary Material

Supplement

ACKNOWLEDGEMENTS

Chengchun Shi’s research was partially supported by the LSE New Research Support Fund. Lexin Li’s research was partially supported by the NSF grant CIF-2102227 and the NIH grant R01AG061303. Tengfei Guo and William Jagust’s research was partially supported by the NIH grants R01AG062542 and R01AG034570. The authors thank the Editor, the Associate Editor, and the referee for their constructive comments and suggestions. The data is publicly available at http://adni.loni.usc.edu/.

FIGURE 1 Illustration of sequential mediation pathways. Since X1 and X2 are multivariate, each potential pathway denoted by (ii) to (vi) can represent multiple paths.

FIGURE 2 The weight adjacency matrix W0. The left panel: d1 = d2 = 35, and the right panel: d1 = d2 = 75.

FIGURE 3 Empirical size and power when d1 = d2 = 35. First column: the vertical axis denotes the indices of the mediators in the first set, and the horizontal axis the second set. The black dots indicate the true significant mediator pairs. Second and third columns: the empirical rejection rate by the method of Chakrabortty et al. (2018), and our test, respectively. Fourth column: the average ROC curve with a varying α. First row: n = 200, and second row: n = 400.

FIGURE 4 Empirical size and power when d1 = d2 = 75. First column: the vertical axis denotes the indices of the mediators in the first set, and the horizontal axis the second set. The black dots indicate the true significant mediator pairs. Second and third columns: the empirical rejection rate by the method of Chakrabortty et al. (2018), and our test, respectively. Fourth column: the average ROC curve with a varying α. First row: n = 300, and second row: n = 500.

FIGURE 5 False discover rate (FDR) and true positive rate (TPR) of multiple testing. The horizontal axis denotes the varying significance level α.

FIGURE 6 Multimodal AD analysis: the identified pathways and brain regions overlaid on a template brain.


References

Alzheimer’s Association. (2020). 2020 Alzheimer’s disease facts and figures. Alzheimer’s &amp; Dementia, 16 (3 ), 391–460.
Baron RM , &amp; Kenny DA (1986). The moderator-mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations. Journal of Personality and Social Psychology, 51 (6 ), 1173–1182.3806354
Bickel PJ , Ritov Y , &amp; Tsybakov AB (2009). Simultaneous analysis of lasso and Dantzig selector. The Annals of Statistics, 37 (4 ), 1705–1732.
Bischof GN , Jessen F , Fliessbach K , Dronse J , Hammes J , Neumaier B , … the Alzheimer’s Disease Neuroimaging Initiative (2016). Impact of tau and amyloid burden on glucose metabolism in alzheimer’s disease. Annals of Clinical and Translational Neurology, 3 (12 ), 934–939.28097205
Cai Q , Wang H , Li Z , &amp; Liu X (2019). A survey on multimodal data-driven smart healthcare systems: Approaches and applications. IEEE Access, 7 , 133583–133599.
Chakrabortty A , Nandy P , &amp; Li H (2018). Inference for individual mediation effects and interventional effects in sparse high-dimensional causal graphical models. arXiv preprint arXiv:1809.10652
Chernozhukov V , Chetverikov D , Demirer M , Duflo E , Hansen C , Newey W , &amp; Robins J (2018). Double/debiased machine learning for treatment and structural parameters: Double/debiased machine learning. The Econometrics Journal, 21 , C1–C68.
Cho H , Choi JY , Hwang MS , Kim YJ , Lee HM , Lee HS , … Lyoo CH (2016). In vivo cortical spreading pattern of tau and amyloid in the alzheimer disease spectrum. Annals of Neurology, 80 (2 ), 247–258.27323247
Das SR , Xie L , Wisse LE , Ittyerah R , Tustison NJ , Dickerson BC , … Wolk DA (2018). Longitudinal and cross-sectional structural magnetic resonance imaging correlates of av-1451 uptake. Neurobiology of Aging, 66 , 49–58.29518752
Desikan RS , Ségonne F , Fischl B , Quinn BT , Dickerson BC , Blacker D , … Killiany RJ (2006). An automated labeling system for subdividing the human cerebral cortex on mri scans into gyral based regions of interest. NeuroImage, 31 (3 ), 968–980.16530430
Djordjilović V , Page CM , Gran JM , Nø st TH , Sandanger TM , Veierø d MB , &amp; Thoresen M (2019). Global test for high-dimensional mediation: testing groups of potential mediators. Statistics in Medicine, 38 (18 ), 3346–3360.31074092
Donohue MC , Sperling RA , &amp; Others. (2014). The Preclinical Alzheimer Cognitive Composite: Measuring Amyloid-Related Decline. Journal of the American Medical Association: Neurology, 71 (8 ), 961–970.24886908
Fan J , &amp; Lv J (2011). Nonconcave penalized likelihood with NP-dimensionality. IEEE Transactions on Information Theory, 57 (8 ), 5467–5484.22287795
Fischer MJ , &amp; Meyer AR (1971). Boolean matrix multiplication and transitive closure. In 12th annual symposium on switching and automata theory (swat 1971) (pp. 129–131).
Gordon B , Blazey T , Su Y , Hari-Raj A , Dincer A , Flores S , … Benzinger T (2018). Spatial patterns of neuroimaging biomarker change in individuals from families with autosomal dominant alzheimer’s disease: a longitudinal study. The Lancet Neurology, 17 (3 ), 241–250.29397305
Guo T , Korman D , Baker SL , Landau SM , &amp; Jagust WJ (2020). Longitudinal cognitive and biomarker measurements support a unidirectional pathway in alzheimer’s disease pathophysiology. Biological Psychiatry.
Harrison TM , La Joie R , Maass A , Baker SL , Swinnerton K , Fenton L , … Jagust WJ (2019). Longitudinal tau accumulation and atrophy in aging and alzheimer disease. Annals of Neurology, 85 (2 ), 229–240.30597624
Huang Y-T , &amp; Pan W-C (2016). Hypothesis test of mediation effect in causal mediation model with high-dimensional continuous mediators. Biometrics, 72 (2 ), 401–413.
Jack Bennett , D. A , Blennow K , Carrillo MC , Dunn B , Haeberlein SB , … Silverberg N (2018). Nia-aa research framework: Toward a biological definition of alzheimer’s disease. Alzheimer’s &amp; Dementia, 14 (4 ), 535–562.
Jack Knopman , D. S , Jagust WJ , Petersen RC , Weiner MW , Aisen PS , … Trojanowski JQ (2013). Tracking pathophysiological processes in alzheimer’s disease: an updated hypothetical model of dynamic biomarkers. The Lancet. Neurology, 12 (2 ), 207–216.23332364
Jack Knopman , D. S , Jagust WJ , Shaw LM , Aisen PS , Weiner MW , … Trojanowski JQ (2010). Hypothetical model of dynamic biomarkers of the alzheimer’s pathological cascade. The Lancet Neurology, 9 (1 ), 119–128.20083042
Jack Wiste , H. J , Therneau TM , Weigand SD , Knopman DS , Mielke MM , … Petersen RC (2019). Associations of Amyloid, Tau, and Neurodegeneration Biomarker Profiles With Rates of Memory Decline Among Individuals Without Dementia. JAMA, 321 (23 ), 2316–2325.31211344
Lai E-Y , Shih S , Huang Y-T , &amp; Wang S (2020). A mediation analysis for a nonrare dichotomous outcome with sequentially ordered multiple mediators. Statistics in Medicine, 39 (10 ), 1415–1428.32074390
Maass A , Landau S , Baker SL , Horng A , Lockhart SN , La Joie R , … Jagust WJ (2017). Comparison of multiple tau-pet measures as biomarkers in aging and alzheimer’s disease. NeuroImage, 157 , 448–463.28587897
Ning Y , &amp; Liu H (2017). A general theory of hypothesis tests and confidence regions for sparse high dimensional models. The Annals of Statistics, 45 (1 ), 158–195.
Peters J , &amp; Bühlmann P (2014). Identifiability of Gaussian structural equation models with equal error variances. Biometrika, 101 (1 ), 219–228.
Richardson S , Tseng GC , &amp; Sun W (2016). Statistical methods in integrative genomics. Annual review of statistics and its application, 3 , 181–209.
Romano J , &amp; DiCiccio C (2019). Multiple data splitting for testing (Tech. Rep.). Technical report.
Shi C , &amp; Li L (2021). Testing mediation effects using logic of boolean matrices. Journal of the American Statistical Association, accepted.
van de Geer S , &amp; Bühlmann P (2013). ℓ0-penalized maximum likelihood for sparse directed acyclic graphs. The Annals of Statistics, 41 (2 ), 536–567.
VanderWeele TJ (2016). Mediation analysis: A practitioner’s guide. Annual Review of Public Health, 37 (1 ), 17–32.
Yuan Y , Shen X , Pan W , &amp; Wang Z (2019). Constrained likelihood for reconstructing a directed acyclic Gaussian graph. Biometrika, 106 (1 ), 109–125.30799877
Zhang . (2010). Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, 38 (2 ), 894–942.
Zhang , Zheng Y , Zhang Z , Gao T , Joyce B , Yoon G , … others (2016). Estimating and testing high-dimensional mediation effects in epigenetic studies. Bioinformatics, 32 (20 ), 3150–3154.27357171
Zhao Y , Li L , &amp; Caffo BS (2020). Multimodal neuroimaging data integration and pathway analysis. Biometrics, 1–20.
Zheng X , Aragam B , Ravikumar PK , &amp; Xing EP (2018). Dags with no tears: Continuous optimization for structure learning. In Advances in neural information processing systems (pp. 9472–9483).
