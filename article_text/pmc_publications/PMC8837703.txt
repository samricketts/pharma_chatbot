LICENSE: This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.


9614434
32559
Neuropsychol Dev Cogn B Aging Neuropsychol Cogn
Neuropsychol Dev Cogn B Aging Neuropsychol Cogn
Neuropsychology, development, and cognition. Section B, Aging, neuropsychology and cognition
1382-5585
1744-4128

34382482
8837703
10.1080/13825585.2021.1962790
NIHMS1733877
Article
Examination of the Reliability and Feasibility of Two Smartphone Applications to Assess Executive Functioning in Racially Diverse Older Adults
John Samantha E. PhD 1*
Evans Sarah A. BS 2
Kim Bona MA 3
Ozgul Petek MS 4
Loring David W. PhD 3
Parker Monica MD 3
Lah James J. MD, PhD 34
Levey Allan I. MD, PhD 34
Goldstein Felicia C. PhD 34
1 Department of Brain Health, School of Integrated Health Sciences, UNLV, Las Vegas, NV, USA
2 Department of Psychology, Marquette University, Milwaukee, WI, USA
3 Department of Neurology, Emory University School of Medicine, Atlanta, GA, USA
4 Emory Goizueta Alzheimer’s Disease Research Center, Atlanta, GA, USA
* Corresponding author: (S.E.J.), Tel: (702-895-4580), samantha.john@unlv.edu
9 9 2021
11 2022
12 8 2021
01 11 2023
29 6 10681086
This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.
Inclusion of Black participants in clinical research is a national priority, particularly for diseases in which they face disproportionate risk. Currently, Black participants are significantly underrepresented within clinical trials and longitudinal research. In an effort to overcome logistical barriers that may limit research participation, this study examined the reliability and feasibility of two mobile smartphone application-based cognitive measures in a diverse middle aged and older adult sample. Black (n=44; Mage=59.93) and non-Hispanic white (NHW; n=50; Mage=61.06) participants completed traditional paper-based neuropsychological testing and two app-based measures, Arrows and Number Match. Arrows and Number Match are adaptations of traditional neuropsychological measures, the Flanker Task and Symbol Digit Modalities Test, respectively. Intraclass correlations demonstrated poor to moderate reliability (range: .417-.569) between performance on the app-based versions and performance on the traditional versions. There were no race related differences in performance on Arrows. Performance score differences by racial group were not statistically significant on Number Match, but trended toward significance, (t (81) = 1.91, p = .06). Both Black and NHW participants rated the applications as feasible and acceptable, though Black participants endorsed a stronger likelihood of future use (M=3.95, SD=0.94) than their NHW counterparts (M=3.50, SD=1.15), p = .04. These findings add to the growing literature on remote cognitive testing in response to the necessity of increased accessibility within research.

remote neuropsychological assessment
smartphone application
reliability
feasibility
non-Hispanic white
Black

pmcIntroduction

Adequate representation of Black older adults in clinical research is critical to identify disease related risk factors, mechanisms, and outcomes in order to ensure that conclusions and interventions are generalizable to a broad population base. Yet, stark racial imbalances exist in research participation. A prime example of this is the field of Alzheimer’s disease AD) where although Black older adults are at increased risk for development of AD relative to non-Hispanic whites (NHW) (Demirovic et al., 2003; Katz et al., 2012; Steenland et al., 2015; Tang et al., 2001), they are significantly underrepresented in clinical trials(Barnes &amp; Bennett, 2014; Corriveau et al., 2017; Shin &amp; Doraiswamy, 2016). Barriers that impact recruitment and retention into clinical research include lack of “convenience” (Wong et al., 2019) and presence of “competing demands,” defined as time and financial challenges such as managing work schedules, arranging for childcare, and caring for relatives (George et al., 2014). Traditional approaches to conducting research require participants to be available onsite for a lengthy block of time, limit the number of participants that can be evaluated simultaneously, and require resources (e.g., personnel and space) that are costly. In recognition of these challenges, federal agencies including the National Institutes of Health have emphasized the need to develop strategies to overcome logistical barriers by improving the methods, tools, and design of clinical research.

Technological advances in remote cognitive assessment, particularly the use of smartphone mobile applications, show promise in removing some of the barriers that currently limit research participation of Black individuals. The widespread use of smartphones has led investigators to adopt this accessible technology for cognitive research (Dufau et al., 2011) through development of mobile applications that specifically assess cognitive functioning (Dahmen et al., 2017; Ruggeri et al., 2016). Some traditional neuropsychological tests have been modified to be compatible with smartphone technology, including the Stroop test, Trail Making Test, and n-Back (Jongstra et al., 2017; Sliwinski et al., 2018). While the use of smartphone technology for evaluating cognitive functioning offers promise for clinical phenotyping and diagnosis, only a handful of published studies have evaluated the feasibility and reliability of smartphone applications in older adults (Brouillette et al., 2013; Lancaster et al., 2019; Sliwinski et al., 2018). In a sample of cognitively normal persons aged 60 years and older, Brouillette and colleagues (2013) reported good test-retest reliability (r=0.726) and validity of measures administered via smartphone when compared to paper and pencil administration. Similarly, Lancaster et al. (2017) demonstrated feasibility of the use of assessment through a smartphone application in middle-aged adults, with 88% participant engagement even after 26 days. While this study assessed a predominantly NHW sample, another study using cognitive assessment via smartphone application found both good between-person reliability (r = .97) and within-person reliability (0.41–0.53) in a racially and ethnically diverse sample (Sliwinski et al., 2018). However, race/ethnicity was not included as a variable to evaluate any potential differences in reliability.

Research suggests that using a smartphone-based method of collecting self-report psychosocial data, otherwise known as Ecological Momentary Assessment (EMA), demonstrated feasibility and acceptability in an older African-American sample (Fritz et al., 2017). However, we are not aware of studies that have evaluated and compared reliability and feasibility/acceptability of smartphone administered cognitive tests in a NHW and Black sample. While race represents an incomplete proxy for a host of important variables that influence performance on neuropsychological tests, including socioeconomic status, educational quality, and acculturation, the effect of race on performance is important to evaluate, given the potential to improve diagnostic accuracy and to reduce misclassification bias (Manly et al., 2004; Sisco et al., 2015).

In the current study, we aim to establish the reliability and feasibility of two mobile smartphone applications designed to assess executive functioning abilities in a middle-aged and older adult sample of community residing Black and NHW. The cognitive domain of executive functioning and, in particular, measures of response inhibition and set shifting, were selected due to their association with instrumental activities of daily living (IADLs) (Cahn-Weiner et al., 2000; McDougall et al., 2019; Vaughan &amp; Giovanello, 2010). As screening instruments, these might eventually be used in longitudinal studies to track meaningful changes in IADLs that could signal the transition from mild cognitive impairment to dementia. We developed two cognitive smartphone apps, Arrows and Number Match, that are adaptations of existing traditional cognitive measures, the Flanker test (Kramer, 2011) and the Symbol Digit Modalities Test (Smith, 1991), respectively. We hypothesized that performance on these two app-based tasks would be significantly and positively correlated to performance on the traditional versions of the tasks within a sample of NHW and Black participants. In addition, we hypothesized that there would be no significant group differences in performance. Mehta and colleagues (2004) found that performance differences between older Black and White participants on a Digit Symbol Substitution Test were less reduced by socioeconomic indicators including education, literacy, income, and financial adequacy (e.g., whether income is sufficient to meet basic needs) than performance on the Modified Mini-Mental State Examination. The authors concluded that measures emphasizing memory and language domains are more susceptible to socioeconomic indicators. As our smartphone tasks were visually based and did not stress these domains, we hypothesized that they would be unbiased measures of performance across groups. We also aimed to establish acceptability and feasibility of using a smartphone application to assess cognition. We hypothesized that both Black and NHW participants would rate the developed application as easy to understand and use.

Materials &amp; Methods

Participants

Participants were recruited through the Emory Healthy Aging Study (EHAS), an online longitudinal study assessing factors that influence healthy aging and disease development across the lifespan (Goetz et al., 2019). Participants were recruited from Emory Clinics and through online advertisement. Participants self-enroll in EHAS through an online registration process and indicate, as part of the IRB approved consent, if they are interested in being contacted for future research projects. A subset of participants from the larger EHAS sample was selected for potential participation in this study according to the following inclusion criteria: 1) aged 45–75 years, 2) ability to read and understand English, 3) ownership of a smartphone, and 4) comfort accessing and using the internet.

All participants meeting the above criteria were stratified by race, age, and education for the purpose of sample matching and future development of normative data. Within each stratified block, a random number was assigned to each potential participant. Potential participants were then contacted by phone according to their randomly assigned number by study researchers who explained the purpose of the mobile app project and further assessed eligibility. Participants were invited to attend on-site visits for initial testing if they did not possess: 1) a diagnosis of Minor or Major Neurocognitive Disorder; 2) significant neurologic disease such as Parkinson’s disease, stroke, seizure disorder, significant head trauma with loss of consciousness for &gt; 30 minutes, structural brain abnormalities; 3) a psychiatric disorder including current DSM-V diagnoses such as Major Depressive Disorder, Generalized Anxiety, or history of psychosis; 4) DSM-V criteria for alcohol/substance abuse or dependence; 5) systemic illness or unstable medical condition which could affect cognition or cause difficulty complying with the protocol such as severe arthritis and visual impairment sufficient to interfere with reading ability; or 6) concomitant antipsychotic medication use, such as non-SSRI antidepressants, neuroleptics, chronic anxiolytics or sedative hypnotics. For each identified Black participant who met the above criteria and was scheduled for an on-site visit, a demographically comparable NHW participant was selected to help maintain the creation of a race-matched stratified sample (see Table 1 for sample characteristics). Final eligibility and enrollment in the study was determined at the on-site visit through cognitive screening. All enrolled participants were confirmed to be cognitively healthy by obtaining a Montreal Cognitive Assessment (MoCA) score (Nasreddine et al., 2005) ≥ 24 points based on a normative research study in African American community residing older adults (Rossetti et al., 2017). Of the individuals who were invited to the on-site visit, 23 were screened out from the present sample due to either a MoCA score &lt; 24 points (N=21) or technical glitches that prevented them from accessing the smartphone application (N=2). Overall study sample and group size was predetermined based on having 48 participants per group and 80% power to detect an intraclass correlation (ICC) of 0.85 or higher under the null hypothesis that the true ICC value is at least 0.70 using an F-test with significance level of 0.70 ( PASS 14, 2015).

Measures

Cognitive and Psychological Functioning

Wide Range Achievement Reading Subtest (WRAT-4) –

The WRAT Reading Subtest is a measure of letter and word decoding and reading level that requires participants to read aloud progressively more difficult words. Each correctly pronounced word earns one point. In the current study, the subtest was used to assess quality of education and was a covariate in the analyses to control for differences in educational exposure/experience (Manly et al., 2004; Wilkinson &amp; Robertson, 2006). Manly and colleagues (2004) found that neuropsychological test performance differences between African American and NHW individuals are attenuated when scores on the WRAT are included as a measure of quality of education.

Patient Health Questionnaire-eight item version (PHQ-8) –

The PHQ-8 is a self-reported depression scale that measures current mood and behavior symptoms (Kroenke et al., 2009). The respondent is asked to rate how often they have been bothered over the past two weeks by symptoms, including endorsement of decreased interest or pleasure in doing things. Each of eight items is scored as 0 (not at all), 1 (several days), 2 (more than half the days) or 3 (nearly every day), with scores ≥10 points indicative of major depressive disorder. The PHQ-8 is a validated instrument to diagnose and measure severity of depressive symptoms, and it has been used in clinical trials and epidemiological studies evaluating racial/ethnic differences in depression in Black and NHW older adults (Villarroel &amp; Terlizzi, 2019; Vyas et al., 2020).

Cognitive Measures - Traditional Versions

Flanker –

The computerized Flanker task measures visual attention and inhibitory control (Eriksen &amp; Eriksen, 1974; Eriksen &amp; Schultz, 1979). The Flanker Task from the EXAMINER battery (Kramer, 2011) was used. It includes 48 testing trials in which multiple arrows all point in the same direction (congruent trials) or in which a single target arrow points in the opposite direction from the “flankers” (incongruent trials). The participant is instructed to indicate the direction of the central target via keyboard response. Patients with mild cognitive impairment and neurodegenerative diseases including AD and frontotemporal dementia have shown decreased task accuracy, which significantly correlated with greater left hemisphere atrophy of the anterior cingulate cortex and dorsolateral prefrontal cortex. Slower response times have shown association with greater atrophy of the right lateral prefrontal cortex and right temporal-parietal junction (Luks et al., 2010).

Symbol Digit Modalities Test (SDMT) –

The SDMT measures visual scanning (Shum et al., 1990), perceptual speed ability, and memory (Laux &amp; Lane, 1985; Smith, 1991). A coding key is presented at the top of the page consisting of nine abstract symbols; each symbol is paired with a different number. The participant has 90-seconds to use the coding key to correctly fill in the empty boxes with the appropriate number that is paired with the symbol presented. Within neurodegenerative samples, the SDMT is sensitive to deficits in working memory, attention, and processing speed and is a useful tool in predicting cognitive decline and brain atrophy (Benedict et al., 2006; Deloire et al., 2006; Paulsen et al., 2013).

Cognitive Measures - Mobile Application Versions

The smartphone cognitive applications were developed for both iOS and Android platforms. Individuals in the EHAS who agreed to be contacted for research opportunities by investigators and staff had their identifying information (e.g., demographics, contact preferences) and a user specific PIN code entered in Salesforce, a HIPPA compliant, cloud based platform for participant recruitment and study tracking. Secure access on the smartphone was verified via authentication of research participation status in Salesforce using their unique credentials (i.e., login and PIN code for the smartphone app). The cognitive assessment raw scores are initially stored in the internal storage of the phone and periodically pushed to wireless data storage in Amazon Web Services. The mobile application updates the research participant’s profile with the cognitive assessment summary results. Protected health information is not stored and participant assessment results are encrypted with a unique hash identifier. A visual depiction of the smartphone tasks are shown in Figure 1 and described below.

Arrows –

The Arrows application is our adaptation of Flanker. It presents 20 trials of congruent and incongruent targets in pseudorandom order. Participants respond by swiping the device screen within a response box in the same direction as the middle target arrow. Performance scores from the app include the overall number of correct trials, the average reaction time across all trials, and the average reaction time for congruent and incongruent trials.

Number Match –

The Number Match application is our adaptation of the Symbol Digit Modalities Test. It consists of two parts. In part one, participants are presented with a coding key at the top of the screen with nine abstract symbols, each paired with a number. Below the coding key, the participants are presented with a single row of paired boxes, one consisting of the abstract symbol and the other an empty box. Participants are instructed to use an on-screen keyboard to correctly fill in the empty boxes with the number that corresponds to the correct abstract symbol, as quickly as possible. A new row of paired boxes is presented once the previous row is completed. Participants have 90-seconds to fill in as many empty boxes as they can. Following a 30-second delay, the participants complete part two of the task, a measure of implicit memory for the coding key. The nine abstract symbols are presented with a paired empty box. Participants are asked to pair each symbol with the correct number based on their recollection of the coding key. The primary performance scores from the app are the number of correct items within each part.

Feasibility of Mobile Application

Participants filled out a six-item feasibility /acceptability rating form that was created specifically for this study. The rating form allowed participants to report their experience using the mobile applications. Ratings were conducted at the end of the study visit. Questions pertaining to acceptability and likelihood of use were posed to participants who rated each question according to a Likert scale ranging from 1 – 5, with 1 = Strongly Disagree and 5 = Strongly Agree. Item 1 was analyzed separately as a measure of likelihood of future use. The summed total of the last five items (with items 2, 4, and 6 reverse scored for consistency) provided an overall feasibility rating.

Procedure

In-person Baseline Procedure

NHW and Black middle-aged and older adult smartphone users were seen in the Emory Brain Health Center and administered both the traditional versions of the cognitive measures and their smartphone analogs. The cognitive tests and smartphone adaptations were administered in blocked fashion (all traditional measures or all smartphone measures) and the order was counterbalanced across participants in each race-based sample. Participants were also administered the additional above-mentioned cognitive and psychological measures and the feasibility/acceptability rating Form. The entire baseline visit lasted 60 minutes and all participants were compensated $25 dollars via gift card.

Analyses

Performance on the apps and ratings of feasibility and likelihood of use were compared between Black and NHW participants through independent samples t-tests. Black and NHW participants were compared across demographic and other measured variables, to verify the success of our sample creation and characterize the overall study sample. To evaluate overall performance differences between racial groups on the novel applications, independent samples t-tests were performed on the summary scores from each of the two measures. As a follow-up to these analyses, we performed linear regression analyses to evaluate the influence of demographic variables and overall cognition on test performance, including age, and years of education, as well as verbal intellectual functioning/quality of education as measured by the WRAT-4 reading subtest, and global cognition as measured by the MoCA. Due to the exploratory and preliminary nature of this study, follow up analyses were only pursued when group differences were statistically significant in an effort to avoid type I error. Initial validity of the mobile applications was assessed through intraclass correlation (ICC) between traditional and app-based measures. ICC was calculated by fitting a two-way mixed effects model with participants as a random effect and test version (smartphone vs. traditional) as a fixed effect. A consistency model was specified and we evaluated agreement according to the single measure value of agreement (Cullum et al., 2014; Koo &amp; Li, 2016). Guidelines for interpretation of ICC are as follows: reliability values are interpreted with respect to the 95% confidence interval of the ICC, where values less than 0.5 are considered poor, values between 0.5 and 0.75 are moderate, values between 0.75 and 0.90 are good, and values greater than 0.90 are excellent (Koo &amp; Li, 2016). Additional qualitative assessment of the level of agreement between test versions was graphed using Bland-Altman plots (Ludbrook, 2010).

Results

Participant Characteristics

Table 1 presents details of participant characteristics. Consistent with our approach for sample creation, we did not observe any statistically significant differences in our Black and NHW participants on age, gender, or years of education. Both Black and NHW participants endorsed low levels of depressive symptoms on the PHQ-8. Black and NHW participants did differ significantly in mean scores on both the MoCA and WRAT-4 Reading subtest, with NHW participants scoring higher. Previous research has supported a different MoCA cut-off score for the determination of ‘normal cognitive status’ for African Americans (Goldstein et al., 2014; Rossetti et al., 2017) and our inclusion criteria of ≥24 points versus ≥26 points for NHW participants likely contributed to this overall difference in MoCA score between groups. The proportion of the sample that owned an iPhone versus an Android phone model differed by race as well. The majority of NHW participants owned an iPhone while Black participants were roughly split between iPhone and Android users.

Data Screening

Prior to conducting the planned analyses, the total score measures for the two apps were examined for normality. Scores from the SDMT and Number Match were both slightly positively skewed, but were not significantly different from normal according to both Komolgorov-Smirnov and Shapiro-Wilk tests of normality. Performance on the Flanker and Arrows tests was evaluated through reaction time measures for the three trial conditions, congruent trials, incongruent trials, and all trials averaged together. All reaction time measures were positively skewed with distributions significantly different from normal. Reaction time values were screened for extreme outliers; any values greater than 3 standard deviations from the mean value of the sample were removed (N=3). We also checked for any values less than 200 milliseconds, of which there were none (Whelan, 2008). After removing the outliers from the sample, reaction time means and medians were examined and compared on normality. Values for skewness and normality were not significantly different between mean and median values, so the raw score means were used in all subsequent analyses for ease of interpretation.

Inter-Version Correlations

Table 2 shows the average performance scores of the overall sample and each racial group on each test as well as the ICC for the overall sample. Results of the ICC revealed poor to moderate reliability between the app and traditional versions of each measure. The highest ICC value occurred for the relationship of SDMT to Number Match (ICC=0.545); however, interpretation of the 95% confidence interval [.40-.70] reveals that the lower bound limit falls below the threshold of .50 and places the reliability estimate within poor range. Figure 2 depicts Bland-Altman plots of the relationship between the app-based versions and the traditional formats of the test. Visual inspection shows relatively no proportional bias for the relationship of SDMT to Number Match; proportional bias exists for the relationship between Flanker and Arrows.

Smartphone Cognitive Test Performance between Groups

Results demonstrated no significant differences in performance on the three reaction time measures for Arrows (ps &gt; .30) between NHW and Black participants. A performance difference on Number Match trended toward significance (t (81) = 1.91, p = .06) with NHW participants (M = 34.32, SD = 8.69) demonstrating a slightly higher mean performance score than Black participants (M = 30.92, SD = 7.29). To further evaluate the trending performance difference between racial groups on Number Match, we regressed demographic and cognitive predictors onto the Number Match performance score. The overall model was significant and explained 37.8% of the variance in Number Match performance, F (5, 82) = 9.63, p &lt; .001, with only participant age as a statistically significant predictor (beta = −.49, p &lt; .001). Older age was associated with worse scores. Cognitive predictors, including global cognition as measured by the MoCA (p = .07) and quality of education as measured by the WRAT-4 Reading subtest (p = .68) did not predict performance. Other demographic variables, including race (p = .11) and years of education (p = .60), were not significant predictors of Number Match performance. Given that our primary interest was to establish the validity of the app measures across racial groups, we performed one additional exploratory model, using race within a hierarchical regression model. In block 1 of the model, we included age as well as the cognitive performance scores on the WRAT-4 and MoCA. In block 2, we included only race, in order to determine if racial group contributed additional variance above and beyond cognitive performance scores (which differed between groups). Within the hierarchical regression, block 1 predictors, age, WRAT-4, and MoCA, together accounted for 33.6% of the variance in Number Match performance, with only age (beta = −.50, p &lt; .001) and MoCA score (beta = .92, p = .03) as significant predictors. The addition of race in block 2 contributed an additional 3.5% of the variance in Number Match performance, with an overall model explaining 37.1% of the variance in Number Match performance, F (4, 78) = 11.51, p &lt; .001. In the final model, only age (beta = −.55, p &lt; .001) and race (beta = −3.45, p = .04) were statistically significant, such that older age and Black race were associated with worse performance scores.

Traditional Cognitive Test Performance Between Groups

Performance of Black and NHW participants was significantly different on traditional versions of both tests, with NHW demonstrating better performance on both tasks. Overall reaction time on trials of Flanker was faster among NHW, all trials averaged together, t (72.59) = 2.98, p = .004, equal variance not assumed), and scores from the SDMT were higher among NHW t (82.88) = 3.16, p = .002, equal variance not assumed).

Feasibility Ratings

Items from the feasibility rating form are presented in Table 3 in addition to the average ratings by racial group and the results of the statistical comparison. Overall feasibility did not significantly differ between Black and NHW participants (p = .10). Item 1 of the form was assessed individually to compare perceived likelihood of future use, and Black participants endorsed a slightly higher likelihood of future app use (p = .04)

Discussion

Within recent years, both research and clinical initiatives have aimed to increase and facilitate the engagement of participants in research by providing remote means of protocol administration and data collection (Whitehead &amp; Seaton, 2016; Wolff-Hughes et al., 2018). For neuropsychology, this has resulted in the presence of online, tablet, and more recently, mobile applications for the assessment of cognitive functioning (Miller &amp; Barr, 2017). The development of rigorous remote data collection methods promises to facilitate testing in rural and underserved communities (Franco-Martín et al., 2012), increase diversity of study samples (Adjorlolo, 2015; Kim &amp; Lee, 2017; Lunn et al., 2019), and provide alternatives to resource and cost-intensive in-person longitudinal tracking (Espay et al., 2019). Rigorous evaluation of these methods is critical in the early phases of tool development.

The current study sought to contribute to the emerging literature on remote cognitive testing by establishing the reliability and feasibility of two novel cognitive smartphone applications, Arrows and Number Match. As hypothesized, performance on the app-based versions of the tasks was significantly and positively correlated to performance on the traditional versions, as measured through intraclass correlations. However, both apps demonstrated only poor to moderate relationships to the respective traditional versions of the task. There may be multiple reasons contributing to these poor relationships. First and foremost, mobile cognitive measures introduce additional sources of both systematic and random error to the measurement of reliability. Given the early stage of our app development, we did not perform quality control analyses between the different phone models and software versions that were utilized across our sample. In addition, we are now aware of multiple backend metrics that can be recorded about participant performance that were not yet captured by this version of our tests. Metrics regarding the timing and kind of touch response performed by our older adult users would might add clarity to our results. In particular, we note the large standard deviations for our Arrows reaction time measures relative to the Flanker task, which may capture relative differences in comfort and familiarity with smartphones vs. the computer. Additional methodological approaches may be useful for expanding the validity assessment of these apps, including a more comprehensive assessment of construct and convergent validity through comparison of app performance to a wider range of traditional neuropsychological measures. Initial evaluations of app-based tests should also consider restricting the phone type and model to only 1 or 2 kinds, perhaps provided by the research team, so as to allow for greater control of varied settings and general QA testing of model differences.

While the apps were designed to measure the same cognitive constructs as the established traditional versions, they were not designed to be smartphone replicas of the established versions. Unlike some computerized or tablet-based assessments that directly replicate the methods and design of established paper and pencil tasks, the apps evaluated here were designed with user interface principles in mind, with emphasis placed on length of assessment, ease of independent use, and engagement of participants. As a result, design choices for the app versions at times departed from the structure of the traditional task. For example, the Arrows task was shorter in length, with only 20 total trials, as compared to the Flanker’s 48 trials that follow an extensive acclimation and pre-test phase. Likewise, the response style of Number Match, which requires participants to select a number 1–9 for each displayed item along a row of responses, does not replicate the process of writing responses by hand. The horizontal tracking necessary for the paper-based SDMT also cannot easily be replicated on the mobile phone screen, given size limitations, so only one row of responses was displayed at a time.

We anticipated that performance on the apps would not be related to race, based on previous research evidence that visual tasks possess less racial and cultural bias (Mehta et al., 2004), as well as evidence of widespread smartphone ownership and comparable patterns of smartphone use across Black and NHW samples (Langford et al., 2019; Perrin &amp; Turner, 2019). Our hypothesis was supported, but we do note and further explore a racial difference that trended toward significance (p = .06) for Number Match. On the Arrows test, there were no significant or trending differences between racial groups on performance. Given our somewhat small sample size, as well as the restricted range of cognitive performance within this sample of healthy controls, we do not want to over-interpret a trending result. Additional exploratory regression analyses suggest that racial group is only contributing 3.5% of the variance in Number Match performance, above and beyond the contribution of age and MoCA score. Given that MoCA score and race were not simultaneous significant predictors of performance, it seems plausible that these variables are accounting for shared variance and may be proxies for one another. Although we did not anticipate any racial differences on Number Match test, we note that the paper-based SDMT demonstrated a performance difference by race in our sample (González et al., 2007).

Both Black and NHW participants endorsed comfort and familiarity with a smartphone prior to enrollment in the study, which likely contributed to the sample’s ability to perform the tasks without issue. Although the app-based measures were novel to all participants, the apps were rated as generally feasible for use. The feasibility ratings for our two applications are consistent with previous research evaluating feasibility of remote applications and other tablet-based assessments (Brouillette et al., 2013; Jongstra et al., 2017; Lancaster et al., 2019; Sliwinski et al., 2018). To our knowledge, no study has evaluated preferences for use of cognitive apps among different racial or ethnic groups. Within our study sample, Black participants rated a slightly higher likelihood of future use for our applications than NHW participants, but both groups rated future use as generally likely. Of note, both applications include an animated tutorial that explains how to complete the task, allowing participants to practice responding and view correct and incorrect responses. Unlike in-person testing, the instructions and app tutorials are self-led, giving the participant control over their pacing and task initiation.

Limitations &amp; Future Directions

Additional research within larger samples is needed to create norms and evaluate cross-cultural differences in performance. Our study is limited by a relatively small, predominantly female sample, that may be underpowered to detect the hypothesized effects and which may not generalize to other samples. Within our cognitively healthy sample, we also have a restricted range of cognitive scores, which likely influenced the relationships among variables. A much larger, more diverse, and cognitively heterogeneous sample is necessary for additional validation. Fortunately, the remote design of smartphone apps lends itself to large study designs within heterogeneous samples. This is a future direction. Given the inter-version comparisons of this evaluation, in-person study visits were necessary and this may have contributed to some of the measured effects within the study. Because the apps were performed during in-person visits, participants had access to a researcher who remained in the room during the app administration, similar to in-person neuropsychological testing. This may have influenced ratings of feasibility and likelihood of use. Future research will evaluate exclusively independent use of the apps. Additional studies of construct validity will assess the claim that these apps are measuring true cognitive abilities, beyond smartphone familiarity. A limitation of the current study is that participants were not administered other neuropsychological measures such as those assessing components of executive functioning, and therefore we are unable to determine whether the app based measures correlate with other abilities examined via traditional pen and paper tests. Test-retest and longitudinal studies with a variety of different smartphone models and software versions will also increase the credibility of these measures.

One of the most difficult aspects of comparing performance between the Flanker and Arrows tasks was our reliance on reaction time data, which may be influenced by inconsistencies in measurement across Internet connection and physical phone characteristics (such as screen protectors and cases). Other potential confounds that could have influenced reaction time data are problems with manual dexterity caused for example, by arthritis, or the small size of the telephone screen which could have made it harder to see the stimuli and thus slowed down an individual’s response times. We would have preferred to use total scores to evaluate performance on both Flanker and Arrows; however, both measures produce nearly perfect scores within the entire sample, removing necessary variability. Ceiling effects are a common feature of Flanker and similar inhibition measures (Eriksen &amp; Eriksen, 1974; Eriksen &amp; Schultz, 1979). Future research will evaluate alternative scoring methods and development of tasks that capture greater performance variability. Moreover, we aim to validate the apps in a clinical sample and eventually, within preclinical neurodegenerative samples.

Benefits of Remote Data Collection

While tele-neuropsychology may still be in its infancy, the need for remote neuropsychological assessment is at its highest. For example, substantial disruptions to the field of neuropsychology have taken place during the COVID-19 pandemic, emphasizing the urgency of alternatives to face-to-face evaluations. The potential benefits of remote neuropsychological assessment are far reaching, including improved access and continuity of care and increased convenience during uncertain times. However, the literature cites many limitations that must be overcome for remote assessment to be adopted. Utilizing smartphone technology may help to mitigate some of these limitations, including the required presence of a psychometrist or neuropsychologist and manipulation of shared physical and visual stimuli. Further, implementation of remote assessment via smartphone applications may help assist in monitoring possible disease progression without the need for extended face-to-face testing. Though this method is certainly not yet capable of replacing face-to-face neuropsychological evaluations, the current study’s findings demonstrate the broader use of smartphones and remote testing in a diverse sample.

Funding Acknowledgement

This work was supported by an NIH/NIA Grant, Remote Ambulatory Cognitive Assessment of African American and Caucasian Adults (R03 AG055810).

Figure 1. Depiction of Arrows (left side) and Number Match (right side) tasks.

Figure 2. Bland-Altman plots showing levels of agreement between the app-based and traditional measures.

Table 1. Participant Characteristics

	Black (n = 44)	NHW (n = 50)	
Characteristics	M (SD)	M (SD)	
Age	59.93 (7.51)	61.06 (7.84)	
Sex (% female)	88.60%	90.00%	
Education	16.86 (2.17)	16.60 (2.19)	
Handedness (% right hand)	84.10%	86.00%	
WRAT-4 Word Reading subtest*	48.93 (3.76)*	50.40 (3.00)*	
PHQ-8 total	1.86 (2.24)	2.72 (3.19)	
MoCA total**	26.09 (1.84)**	27.48 (1.80)**	
Phone model (% iPhone)**	47.70%**	76.00%**	
			
Abbreviations: SD, standard deviation; WRAT-4, Wide Range Achievement Test Fourth Edition; PHQ-9, Patient Health Questionnaire-8; MoCA, Montreal Cognitive Assessment.

* Groups are statistically different, p&lt;.05;

** Groups are statistically different, p&lt;.01

Table 2. Participant Performance

	Overall Sample
M (SD)	Black
M(SD)	NHW
M(SD)	Overall Sample
ICC
[95% CI]	p	
SDMT vs. Number Match (total score)						
SDMT	50.10 (7.39)	47.61 (7.79)	52.28 (6.33)	0.569
[.40, .70]		
Number Match	32.72 (8.20)	30.92 (7.29)	34.32 (8.69)	.000	
 						
Computerized Flanker vs. Arrows (ms)						
Flanker reaction time - incongruent trials	924.94 (184.48)	981.31 (207.35)	876.46 (147.85)	0.417
[.23, .57]		
Arrows reaction time - incongruent trials	1259.58 (335.36)	1298.24 (350.54)	1225.65 (321.23)	.000	
 						
Flanker reaction time - congruent trials	816.53 (173.95)	876.44 (198.78)	765.01 (130.71)	0.435
[.25, .59]		
Arrows reaction time - congruent trials	1204.68 (358.51)	1240.19 (414.59)	1173.53 (301.91)	.000	
 						
Flanker reaction time across all trials	870.57 (177.45)	942.50 (202.96)	827.79 (149.28)	0.460
[.28, .61]		
Arrows reaction time across all trials	1233.13 (323.37)	1271.31 (362.16)	1199.63 (284.66)	.000	
	
 						

Table 3. Likelihood and Feasibility of Application Use

Scale Items	Black
M (SD)	NHW
M (SD)	t	p	
Likelihood					
1. I think I would like to use this app frequently	3.95 (0.94)	3.50 (1.15)	−2.05	0.04	
 	
Feasibility					
2. I found the app unnecessarily complex*	4.48 (.67)	4.39 (.70)			
3. I thought the app was easy to use	4.50 (.67)	4.20 (1.14)			
4. I think I would need support of a tech person to be able to use app*	4.83 (.38)	4.67 (.52)			
5. I would imagine most people would learn to use this app quickly	4.21 (1.00)	3.88 (.96)			
6. I needed to learn a lot of things before I could get going with this app*	4.67 (.72)	4.55 (.84)			
 	
Feasibility Total Rating					
	22.69 (2.15)	21.86 (2.52)	−1.68	0.10	
* Note: Items 2, 4, and 6 were reverse coded to be consistent with the direction of items 1, 3, and 5. Higher item scores therefore represent greater disagreement with the items, as worded here. Ratings for each item were on a scale from 1 (strongly disagree) to 5 (strongly agree) or in the case of reverse scored items 1 (strongly agree) to 5 (strongly disagree).

Abbreviations: M=mean; SD=standard deviation; t=t statistic; p=p-value (significance).

Disclosure of Interest Statement

The authors report no conflict of interest.


References

Adjorlolo S . (2015). Can teleneuropsychology help meet the neuropsychological needs of Western Africans? the case of Ghana. Applied Neuropsychology:Adult, 22 (5 ), 388–398. 10.1080/23279095.2014.949718 25719559
Barnes LL , &amp; Bennett DA (2014). Alzheimer’s disease in African Americans: risk factors and challenges for the future. Health Affairs, 33 (4 ), 580–586.24711318
Benedict RHB , Bruce JM , Dwyer MG , Abdelrahman N , Hussein S , Weinstock-Guttman B , Garg N , Munschauer F , &amp; Zivadinov R . (2006). Neocortical atrophy, third ventricular width, and cognitive dysfunction in multiple sclerosis. Archives of Neurology, 63 (9 ), 1301–1306.16966509
Brouillette RM , Foil H , Fontenot S , Correro A , Allen R , Martin CK , Bruce-Keller AJ , &amp; Keller JN (2013). Feasibility, reliability, and validity of a smartphone based application for the assessment of cognitive function in the elderly. PloS One, 8 (6 ), e65925.23776570
Cahn-Weiner D , Malloy PF , Boyle PA , Marran M , &amp; Salloway S . (2000). Prediction of functional status from neuropsychological test in community-dwelling elderly individuals. The Clinical Neuropsychologist, 14 (2 ),187–195.10916193
Corriveau RA , Koroshetz WJ , Gladman JT , Jeon S , Babcock D , Bennett DA , Carmichael ST , Dickinson SL-J , Dickson DW , Emr M , Fillit H , Greenberg SM , Hutton ML , Knopman DS , Manly JJ , Marder KS , Moy CS , Phelps CH , Scott PA , … Holtzman DM (2017). Alzheimer’s Disease-Related Dementias Summit 2016: National research priorities. Neurology, 89 (23 ), 2381–2391. 10.1212/WNL.0000000000004717 29117955
Cullum CM , Hynan LS , Grosch M , Parikh M , &amp; Weiner MF (2014). Teleneuropsychology: Evidence for video teleconference-based neuropsychological assessment. Journal of the International Neuropsychological Society: JINS, 20 (10 ), 1028.25343269
Dahmen J , Cook D , Fellows R , &amp; Schmitter-Edgecombe M . (2017). An analysis of a digital variant of the trail making test using machine learning techniques. Technology and Health Care, 25 (2 ), 251–264.
Deloire MSA , Bonnet MC , Salort E , Arimone Y , Boudineau M , Petry KG , &amp; Brochet B . (2006). How to detect cognitive dysfunction at early stages of multiple sclerosis? Multiple Sclerosis Journal, 12 (4 ), 445–452.16900758
Demirovic J , Prineas R , Loewenstein D , Bean J , Duara R , Sevush S , &amp; Szapocznik J . (2003). Prevalence of dementia in three ethnic groups: the South Florida program on aging and health. Annals of Epidemiology, 13 (6 ), 472–478.12875807
Dufau S , Duñabeitia JA , Moret-Tatay C , McGonigal A , Peeters D , Alario F-X , Balota DA , Brysbaert M , Carreiras M , &amp; Ferrand L . (2011). Smart phone, smart science: how the use of smartphones can revolutionize research in cognitive science. PloS One, 6 (9 ), e24974.21980370
Eriksen BA , &amp; Eriksen CW (1974). Effects of noise letters upon the identification of a target letter in a nonsearch task. Perception &amp; Psychophysics, 16 (1 ), 143–149.
Eriksen CW , &amp; Schultz DW (1979). Information processing in visual search: A continuous flow conception and experimental results. Perception &amp; Psychophysics, 25 (4 ), 249–263.461085
Espay AJ , Hausdorff JM , Sánchez-Ferro Á , Klucken J , Merola A , Bonato P , Paul SS , Horak FB , Vizcarra JA , Mestre TA , Reilmann R , Nieuwboer A , Dorsey ER , Rochester L , Bloem BR , &amp; Maetzler W . (2019). A roadmap for implementation of patient-centered digital outcome measures in Parkinson’s disease obtained using mobile health technologies. Movement Disorders, 34 (5 ), 657–663. 10.1002/mds.27671 30901495
Franco-Martín MA , Bernardo-Ramos M , &amp; Soto-Pérez F . (2012). Cyber-Neuropsychology: Application of new technologies in neuropsychological evaluation. Actas Espanolas de Psiquiatria, 40 (6 ).
Fritz H , Tarraf W , Saleh DJ , &amp; Cutchin MP (2017). Using a smartphone-based ecological momentary assessment protocol with community dwelling older African Americans. Journals of Gerontology Series B: Psychological Sciences and Social Sciences, 72 (5 ), 876–887.28057696
George S , Duran N , &amp; Norris K . (2014). A systematic review of barriers and facilitators to minority research participation among African Americans, Latinos, Asian Americans, and Pacific Islanders. American Journal of Public Health, 104 (2 ), e16–e31.
Goetz ME , Hanfelt JJ , John SE , Bergquist SH , Loring DW , Quyyumi A , Clifford GD , Vaccarino V , Goldstein F , &amp; Johnson TM 2nd (2019). Rationale and Design of the Emory Healthy Aging and Emory Healthy Brain Studies. Neuroepidemiology, 53 (3–4 ), 187–200.31454799
Goldstein FC , Ashley AV , Miller E , Alexeeva O , Zanders L , &amp; King V . (2014). Validity of the montreal cognitive assessment as a screen for mild cognitive impairment and dementia in African Americans. Journal of Geriatric Psychiatry and Neurology, 27 (3 ), 199–203.24614202
González HM , Whitfield KE , West BT , Williams DR , Lichtenberg PA , &amp; Jackson JS (2007). Modified-symbol digit modalities test for African Americans, Caribbean Black Americans, and non-Latino Whites: Nationally representative normative data from the National Survey of American Life. Archives of Clinical Neuropsychology, 22 (5 ), 605–613.17493782
Jongstra S , Wijsman LW , Cachucho R , Hoevenaar-Blom MP , Mooijaart SP , &amp; Richard E . (2017). Cognitive testing in people at increased risk of dementia using a smartphone app: the iVitality proof-of-principle study. JMIR MHealth and UHealth, 5 (5 ), e68.28546139
Katz MJ , Lipton RB , Hall CB , Zimmerman ME , Sanders AE , Verghese J , Dickson DW , &amp; Derby CA (2012). Age-specific and Sex-specific Prevalence and Incidence of Mild Cognitive Impairment, Dementia, and Alzheimer Dementia in Blacks and Whites. Alzheimer Disease &amp; Associated Disorders, 26 (4 ), 335–343. 10.1097/WAD.0b013e31823dbcfc 22156756
Kim BYB , &amp; Lee J . (2017). Smart devices for older adults managing chronic disease: a scoping review. JMIR MHealth and UHealth, 5 (5 ), e69.28536089
Koo TK , &amp; Li MY (2016). A guideline of selecting and reporting intraclass correlation coefficients for reliability research. Journal of chiropractic medicine, 15 (2 ), 155–163.27330520
Kramer J . (2011). Executive abilities: Measures and instruments for neurobehavioral evaluation and research (EXAMINER). NINDS.
Kroenke K , Strine TW , Spitzer RL , Williams JBW , Berry JT , &amp; Mokdad AH (2009). The PHQ-8 as a measure of current depression in the general population. Journal of Affective Disorders, 114 (1–3 ), 163–173. 10.1016/j.jad.2008.06.026 18752852
Lancaster C , Koychev I , Blane J , Chinner A , Wolters L , &amp; Hinds C . (2019). The Mezurio smartphone application: Evaluating the feasibility of frequent digital cognitive assessment in the PREVENT dementia study. MedRxiv, 19005124. 10.1101/19005124
Langford AT , Solid CA , Scott E , Lad M , Maayan E , Williams SK , &amp; Seixas AA (2019). Mobile phone ownership, health apps, and tablet use in US adults with a self-reported history of hypertension: Cross-sectional study. Journal of Medical Internet Research, 21 (1 ), 31344667. 10.2196/12228
Laux LF , &amp; Lane DM (1985). Information processing components of substitution test performance. Intelligence, 9 (2 ), 111–136.
Luks TL , Oliveira M , Possin KL , Bird A , Miller BL , Weiner MW , &amp; Kramer JH (2010). Atrophy in two attention networks is associated with performance on a Flanker task in neurodegenerative disease. Neuropsychologia, 48 (1 ), 165–170.19747928
Lunn MR , Capriotti MR , Flentje A , Bibbins-Domingo K , Pletcher MJ , Triano AJ , Sooksaman C , Frazier J , &amp; Obedin-Maliver J . (2019). Using mobile technology to engage sexual and gender minorities in clinical research. PLoS ONE, 14 (5 ), 1–19. 10.1371/journal.pone.0216282
Manly JJ , Byrd DA , Touradji P , &amp; Stern Y . (2004). Acculturation, reading level, and neuropsychological test performance among African American elders. Applied Neuropsychology, 11 (1 ), 37–46.15471745
McDougall GJ , Han A , Staggs VS , Johnson DK , McDowd JM (2019). Predictors of instrumental activities of daily living in community-dwelling older adults. Arch Psychiatr Nurs. 33 (5 ), 43–50.31711593
Mehta KM , Simonsick EM , Rooks R , Newman AB , Pope SK , Rubin SM , Yaffe K , &amp; Health A and B. C. S. (2004). Black and white differences in cognitive function test scores: what explains the difference? Journal of the American Geriatrics Society, 52 (12 ), 2120–2127.15571554
Miller JB , &amp; Barr WB (2017). The technology crisis in neuropsychology. Archives of Clinical Neuropsychology, 32 (5 ), 541–554.28541383
Nasreddine ZS , Phillips NA , Bédirian V , Charbonneau S , Whitehead V , Collin I , Cummings JL , &amp; Chertkow H . (2005). The Montreal Cognitive Assessment, MoCA: a brief screening tool for mild cognitive impairment. Journal of the American Geriatrics Society, 53 (4 ), 695–699.15817019
PASS 14 Power Analysis and Sample Size Software (2015). NCSS, LLC. Kaysville, Utah, USA, ncss.com/software/pass.
Paulsen JS , Smith MM , Long JD , investigators PHD , &amp; Group C of the H. S. (2013). Cognitive decline in prodromal Huntington Disease: implications for clinical trials. Journal of Neurology, Neurosurgery &amp; Psychiatry, 84 (11 ), 1233–1239.23911948
Perrin A , &amp; Turner E . (2019). Smartphones help blacks, Hispancis bridge some - but not all - digital gaps with whites. Fact Tank: News in the Numbers, 1–8. http://www.pewresearch.org/fact-tank/2018/10/19/5-charts-on-global-views-of-china/
Rossetti HC , Lacritz LH , Hynan LS , Cullum CM , Van Wright A , &amp; Weiner MF (2017). Montreal cognitive assessment performance among community-dwelling African Americans. Archives of Clinical Neuropsychology, 32 (2 ), 238–244.28365749
Ruggeri K , Maguire Á , Andrews JL , Martin E , &amp; Menon S . (2016). Are we there yet? Exploring the impact of translating cognitive tests for dementia using mobile technology in an aging population. Frontiers in Aging Neuroscience, 8 , 21.27014053
Shin J , &amp; Doraiswamy PM (2016). Underrepresentation of African-Americans in Alzheimer’s trials: a call for affirmative action. Frontiers in Aging Neuroscience, 8 , 123.27375473
Sisco S , Gross AL , Shih RA , Sachs BC , Glymour MM , Bangen KJ , Benitez A , Skinner J , Schneider BC , &amp; Manly JJ (2015). The role of early-life educational quality and literacy in explaining racial disparities in cognition in late life. Journals of Gerontology Series B: Psychological Sciences and Social Sciences, 70 (4 ), 557–567.24584038
Sliwinski MJ , Mogle JA , Hyun J , Munoz E , Smyth JM , &amp; Lipton RB (2018). Reliability and validity of ambulatory cognitive assessments. Assessment, 25 (1 ), 14–30.27084835
Smith A . (1991). Symbol digit modality test (SDMT). Los Angeles, Western Psychological Services.
Steenland K , Goldstein FC , Levey A , &amp; Wharton W . (2015). A Meta-Analysis of Alzheimer’s Disease Incidence and Prevalence Comparing African-Americans and Caucasians. Journal of Alzheimer’s Disease, 50 (1 ), 71–76. 10.3233/JAD-150778
Tang M , Cross P , Andrews H , Jacobs DM , Small S , Bell K , Merchant C , Lantigua R , Costa R , Stern Y , &amp; Mayeux R . (2001). Incidence of AD in African-Americans, Caribbean Hispanics, and Caucasians in northern Manhattan. American Academy of Neurology, 56 , 49–56.
Vaughan L , &amp; Giovanello K . (2010). Executive function in daily life: Age-related influences of executive processes on instrumental activities of daily living. Psychology and Aging. 25 (2 ), 343–355.20545419
Villarroel MA , &amp; Terlizzi EP (2020). Symptoms of Depression Among Adults: United States, 2019. Centers for Disease Control and Prevention: Atlanta, GA, USA.
Vyas CM , Donneyong M , Mischoulon D , Chang G , Gibson H , Cook NR , Manson JE , Reynolds CF 3rd , &amp; Okereke OI (2020). Association of Race and Ethnicity with Late-Life Depression Severity, Symptom Burden, and Care. JAMA network open, 3 (3 ), e201606. 10.1001/jamanetworkopen.2020.1606 32215634
Whelan R . (2008). Effective analysis of reaction time data. The Psychological Record, 58 (3 ), 475–482.
Whitehead L , &amp; Seaton P . (2016). The effectiveness of self-management mobile phone and tablet apps in long-term condition management: a systematic review. Journal of Medical Internet Research, 18 (5 ), e97.27185295
Wilkinson GS , &amp; Robertson GJ (2006). WRAT 4: Wide range achievement test; professional manual. Psychological Assessment Resources, Incorporated.
Wolff-Hughes DL , Conroy R , McClain JJ , Nilsen WJ , &amp; Riley WT (2018). Building the infrastructure to accelerate evidence-generating mobile and wireless health research: National Institutes of Health and National Science Foundation perspectives. Translational Behavioral Medicine, 8 (2 ), 295–298.29385566
Wong R , Amano T , Lin S-Y , Zhou Y , &amp; Morrow-Howell N . (2019). Strategies for the recruitment and retention of racial/ethnic minorities in alzheimer disease and dementia clinical research. Current Alzheimer Research, 16 (5 ), 458–471.30907319
