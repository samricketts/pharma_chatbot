LICENSE: This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.


8310780
20511
IEEE Trans Med Imaging
IEEE Trans Med Imaging
IEEE transactions on medical imaging
0278-0062
1558-254X

37643099
10764000
10.1109/TMI.2023.3309821
NIHMS1928491
Article
Developing Explainable Deep Model for Discovering Novel Control Mechanism of Neuro-Dynamics
Dan Tingting Department of Psychiatry, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA

Kim Minjeong Department of Computer Science, University of North Carolina at Greensboro, Greensboro, NC, 27402, USA

Kim Won Hwa Computer Science and Engineering / Graduate School of AI, POSTECH, Pohang, 37673, South Korea, Computer Science and Engineering, University of Texas at Arlington, TX, 76010, USA

Wu Guorong Department of Psychiatry, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA,
Department of Computer Science, Department of Statistics, the Operations Research and Carolina Institute for Developmental Disabilities, and the UNC NeuroScience Center, University of North Carolina at Chapel Hill, Chapel Hill, NC, 27599, USA.

Tingting_Dan@med.unc.edu
11 9 2023
1 2024
02 1 2024
03 1 2024
43 1 427438
This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.
Human brain is a complex system composed of many components that interact with each other. A well-designed computational model, usually in the format of partial differential equations (PDEs), is vital to understand the working mechanisms that can explain dynamic and self-organized behaviors. However, the model formulation and parameters are often tuned empirically based on the predefined domain-specific knowledge, which lags behind the emerging paradigm of discovering novel mechanisms from the unprecedented amount of spatiotemporal data. To address this limitation, we sought to link the power of deep neural networks and physics principles of complex systems, which allows us to design explainable deep models for uncovering the mechanistic role of how human brain (the most sophisticated complex system) maintains controllable functions while interacting with external stimulations. In the spirit of optimal control, we present a unified framework to design an explainable deep model that describes the dynamic behaviors of underlying neurobiological processes, allowing us to understand the latent control mechanism at a system level. We have uncovered the pathophysiological mechanism of Alzheimer’s disease to the extent of controllability of disease progression, where the dissected system-level understanding enables higher prediction accuracy for disease progression and better explainability for disease etiology than conventional (black box) deep models.

Index Terms—

Alzheimer’s disease
Graph neural networks
Partial differential equations
Aβ-tau interaction
Systems biology

pmcI. Introduction

We live in a world of complex systems. The commonality of all complex systems is that their behavior is intrinsically difficult to model, due to dynamic interactions between components oftentimes intervened by the environment. From a holistic thinking perspective, it is useful to represent the system as a network where the nodes represent the components and links for their interactions. The fundamental science interest is to understand the working mechanism of how part-to-part interactions give rise to its collective behaviors and how the system reacts to the new environment. Without a doubt, machine learning has a long history of studying the dynamic behavior of complex systems (e.g., determining Lyapunov exponents, prediction of future evolution, and inferring causality of interactions) [1]–[3].

Like any object in our mother nature, the human brain is a physically inter-wired system that supports ubiquitous neuron-to-neuron synapses [4], [5]. However, the human brain is the most complex wiring system at a variety of levels, from single cells to entire networks that emerge complex behavior. Under the umbrella of the complex system, network neuroscience technique [6] comes to the stage, promoting a holistic underpinning of the human brain: the dynamic nature of a complex system cannot be understood by the thing of the system as comprised of independent pieces [5]. Instead, it is important to utilize the principles of complex interactions within a system to understand the behavior of the system overall. For instance, the human brain is an optimized information-exchanging system that supports transient selforganized functional fluctuations. Unfortunately, the concept of fast transport also applies to toxic pathology proteins such as amyloid plaques (Aβ) and neurofibrillary tangles (tau) [7] that hijack the network to spread rapidly throughout the brain in the progression of Alzheimer’s disease (AD). In this context, the reaction-diffusion model (RDM) [8] has shown promising results by explicitly modeling the Aβ-tau interaction and the prion-like propagation of tau aggregates using PDEs [9], [10]. Since each real-world complex system has unique characteristics, we sought to integrate the principle of systems biology and the power of machine learning in a unified mathematical framework, with a focus on an RDM-based deep model for uncovering the novel biological mechanism.

In this context, we present a unified data-driven approach to uncover the latent working mechanisms in the human brain from the spatiotemporal neuroimaging data. Following the notion in complex system [11], each brain region is regarded as a node in the system, which has several sensors receiving the biological measurements. In addition, multiple brain regions are coupled via a network topology such as the nerve fibers [12] or the co-activation of functional fluctuations [13]. The implication of the brain’s node-to-node coupling mechanism leads to the hypothesis that localized perturbations (such as stimulation or neurodegeneration) not only yield regional effects but also induce remote (indirect) effects that propagate throughout the brain network [14], [15]. In this regard, we conceptualize that our brain forms a closed-loop feedback system [16] that reacts to external disturbances while moving toward a stable state. Different neurobiological process manifests unique collective behaviors in the brain which largely depend on (i) how each node reacts to the environment, and (ii) how the network topology regulate the diffusion process across nodes.

The major contributions of our work are summarized below, along with the demonstration of a toy example (tilting dolls) in Fig. 1.

We have derived the closed-form solution of the optimal control law for the closed-loop feedback RDM system. In analogy to the tilting doll (Fig. 1(a)), the feedback, following the control law, allows us to keep the tilting doll wobbling between left and right positions (full state) with minimum effort.

Like each biological process is unique, tilting dolls might have diverse characteristics, such as the height of the mass center (indicated by color lines in Fig. 1(b)). We further present a learning-based framework to generalize the common control mechanism across individual RDM systems that have different system properties.

The real-life system often works in a complex environment. For example, the tilting systems might operate on a rolling plate (Fig. 1(c)). It is important to understand how the whole system interacts with external disturbances while maintaining controllability. Taken together, these technical backbones allow us to uncover the interaction mechanisms of how the self-organized neurobiological system reacts to external stimulations and emerges remarkable dynamic system behaviors.

There is an overwhelming consensus that the cascade of Aβ is the trigger of tau propagation throughout the brain cortex [17]. As a proof-of-concept application, we devise a new explainable machine learning approach to uncover the control mechanism of Aβ-tau interaction in the AD-related neurodegeneration process. In addition to the more accurate prediction result than conventional black box models, our system-informed deep model allows us to understand the mechanistic role of the Aβ-tau interaction in the spreading of tau aggregates throughout the brain, which sets the stage for precision medicine in therapeutic treatment for AD.

II. Related work

In this section, we provide a concise overview of the existing research regarding the dynamic system modeling of machine learning and its extension to underlying neurobiological processes.

Dynamic system modeling in machine learning aims to capture the temporal behaviors of complex systems. One approach that has been widely studied in the field of complex systems is the use of the RDM with closed-loop feedback control [16], [18]. RDM involves modeling systems using mathematical equations that describe reaction and diffusion processes. By incorporating closed-loop feedback control mechanisms, these models can capture the dynamic behaviors and interactions within complex systems.

However, there are challenges associated with applying existing systems biology approaches to machine learning. One challenge is the empirical setting of hyperparameters, such as the a-priori choice of basis functions. This empirical process can limit the applicability of current systems biology approaches in practical machine learning scenarios. More critically, designing appropriate PDEs to model complex biological processes can be challenging, especially when a complete understanding of these processes is not yet available. Fortunately, the availability of vast amounts of spatiotemporal data opens up alternative solutions that leverage the power of deep neural networks (DNNs). The conjecture is that the latent mechanisms underlying complex systems can be uncovered from observed data, and DNNs excel at extracting valuable information from such data.

Among diverse research topics of deep learning, the integration between PDE and DNN is gaining more and more attraction in the machine learning field. Under the hood of current works, most of them either address the DNN limitation by replacing it with the PDE solver or the other way around. For example, Neuro-ODE [19] used a differential equation solver to effectively regulate the depth of neural network. Furthermore, a liquid time-constant network [20] was proposed to improve the stability and expressivity for dynamic time series using a fused ODE solver. On the other hand, deep learning techniques have been used for understanding, modeling, and optimizing the governing equations of a dynamic system from the observed physical measurement [21]. Meanwhile, there is a trend towards the explainable deep model by elucidating the intrinsic relationship between PDE and DNN. For instance, Chamberlain et al. presented a graph neural diffusion model that casts machine learning on graph data as a continuous graph diffusion process and derives the equivalent graph neural networks (GNNs) as the discretization of an underlying PDE [22]. After that, recent work shows the potential of designing new GNN architecture from the PDEs through a weighted combination of non-linear graph diffusion and hyperbolic equations [23]. Although the common plight of the oversmoothing issues in conventional GNN has been addressed, the roadmap to explainable deep models requires a more principled mathematical framework to fully understand the mechanisms at a system level.

In the context of optimal control, machine learning techniques have been employed to achieve optimal control by enhancing a system’s robustness to external disturbances [24]–[28]. As shown in the right side of Fig. 2, a major body of current works focuses on the design of optimal control components (pink box) that enhance the system’s robustness to external disturbances. With that being said, system mechanics (blue box) is pre-defined in most applications such as physics and fluid dynamics. However, the governing equation of the underlying neurobiological process is often elusive. In this regard, a significant contribution of our work is the development of a unified framework that uncovers latent systemlevel understanding from spatiotemporal data, which allows us to investigate the control mechanism of neurobiological processes.

As shown in Fig. 2, we first integrate the regulator of optimal control into the parabolic PDE in the widely studied RDM system that explicitly models the interaction between the system mechanics and external disturbances. Since the variational analysis offers great flexibility to formulate domain-specific knowledge of neurobiological processes into the method of Lagrange multipliers [29], we propose a new framework for generating new PDE-based solutions from the application-specific constrained functional that can forecast the outcome of the underlying neurobiological process. Inspired by the pioneering works in PDE-informed GNN [20], [22], [23], we further design the explainable deep model based on the new PDE, where the neural network is trained to explain the existing observations under the guidelines of the system principles.

III. Method

At the heart of our work is an explainable deep model based on RDM for discovering novel system mechanisms of neurodynamics. In general, we first unify RDM and its equivalent deep model into a variational analysis framework. Since the functional of the resulting open-loop system is formulated in a continuous domain, we design the closed-loop feedback system of RDM with an optimal control mechanism, to the extent that we enable conventional RDM to communicate with external disturbance. Considering that the phenotypic traits (such as assessment scores or clinical outcomes) are available at the end of the neurobiological process, we propose a novel supervised control regulator to enforce the characteristic system behaviors being aligned with the progression of the underlying neurobiological process. Inspired by the recent PDE-based deep models, we eventually present the explainable deep neural network that operates as a real-world neurodynamic system, which offers a new window to elucidate the latent neurobiological mechanism with mathematical insight.

Following this clue, we provide a brief background on the variational analysis framework of RDM in Section III-A. on which we derive the constrained functional underlying a controlled system in Section III-B, followed by the equivalent deep model for learning the characteristics of new closed-loop feedback RDM system with optimal control in Section III-C We apply our method to the spatiotemporal neuroimaging data, in the seek of the latent mechanisms that emerge dynamic behaviors associated with the widespread neurodegeneration process in Section III-D.

A. Open-loop RDM and Deep Model

1) Variation Functional of RDM:

Suppose we have a graph data denoted by 𝒢=(Ξ,W) with n nodes Ξ=ξi∣i=1,…N} and the adjacency matrix W=wiji,j=1N∈ℛN×N. On each node, xi denotes the measurements associated with node ξi. Here, we formulate reaction-diffusion process [8] constrained by the network topology 𝒢 as the evolution of system state v∈ℛN given the initial condition v(0)=v0: (1) dvdt=−∇⋅∇(v)+h(x,v,t),

where Δ=∇⋅(∇) denotes the graph Laplacian operator and h(x,v,t)=xTΦxv denotes a non-linear reaction function with learnable parameters Φ∈ℛN×N. Since x is the sensor observations (i.e., constant variables), we simply let H=xTΦx, for clarity. Note, the graph gradient is defined as ∇v=wijvi−vj. Usually, h(⋅) is a non-linear function that describes the reaction process regarding the input x=xii=1N (such as the build-up of tau aggregates in neurodegeneration process [9]). The dynamic evolution equation can be written in a variational form dvdt=−δℱδv and the functional ℱ is defined as ℱ=∫−∞+∞ |∇v|2−G(x,v,t)dx, with a potential energy G(x,v,t) such that h(x,v,t)=dG(x,v,t)dt.

2) Equivalent deep model of RDM:

A recent work (named Neuro-RDM) [30] has demonstrated the potential of using neural networks to predict the state of brain activities underlying the RDM mechanistic. Specifically, the reaction function h(⋅) is turned into a discretization of the fully connected neural network where the input is the external stimulation approximated by the snapshot of whole-brain fMRI (functional Magnetic Resonance Imaging) signals. The output is a collection of hidden states that support the observed cognitive tasks. Since multiple brain regions are functionally connected [6], GNN is employed to propagate the brain states throughout the network. In the machine learning perspective, the reference (aka. terminal) state vT in RDM is expected to be aligned with the annotated labels (e.g., assessment scores). It is apparent that the current RDM-based deep model is limited in an openloop system, which is often less stable and lacks the capacity to uncover novel mechanisms of system dynamics. To address this limitation, we present our PDE-based DNN to uncover the mechanistic role of interaction between the intrinsic RDM system and environmental stimulations.

B. Uncover Interaction Mechanisms between RDM System and Environment

The complexity of dynamic control mainly attributes to the dynamic interactions between the intrinsic system and environmental exposures. Suppose u(t)∈ℛM is the spontaneous feedback force on a subset of control nodes (i.e., M≤N) to maintain the system stability. The interactions between graph nodes and control nodes are captured by an interaction matrix Θ∈ℛN×M. Thus, the machine learning problem is formulated as: estimating the latent interaction matrix Θ such that the RDM (1) remains controllable, (2) interacts with the system feedback u(t), and (3) yields the reasonable terminal state vT. To better cast it into a well-posed problem, we further introduce a supervised linear quadratic regulator (LQR) to the extent that the (desired) terminal state vT is highly correlated to its associated phenotypic trait η on an individual basis.

1) Optimal Control Law in Closed-loop Feedback RDM:

On top of the conventional RDM (in Eq. 1), we seek controllability by adding the feedback u(t) through the (pre-define) interaction matrix Θ as: (2) dvdt=f(x,u,v,t),v(0)=v0,f(x,u,v,t)=Δv(t)+h(x,v,t)+Θu(t)

Under the assumption that the RDM system in Eq. 2 is controllable, it is possible to manipulate the eigenvalues of the closed-loop system (Δv(t)+h(x,v,t)−ΘKv(t)) through the choice of a full-state feedback control law u(t)=−Kv(t), where K is the gain matrix. In control theory, the eigenvalues of the closed-loop system are essential for understanding its behavior and stability. The eigenvalues determine the stability of the closed-loop system. Stable eigenvalues indicate that the system will settle down to a desired state over time. Unstable eigenvalues, on the other hand, suggest that the system will exhibit unbounded growth or oscillations, making it challenging to control effectively [11], [31]. In the spirit of optimal control, the classic LQR solution [32] is to optimize K by minimizing the quadratic cost: (3) ℒ=12v⊤Qv+12u⊤Ru,

where Q and R are symmetric matrices of size N×N and M×M, respectively. The magnitudes of entries in Q and R penalize errors from the equilibrium point and control cost, separately. Essentially, the LQR solution is obtained through a combination of calculus of variations and the method of Lagrange multipliers, which underlines the Pointryagin’s minimum principle [33]. Since there are infinite number of equality constraints dvdt=f(x,u,v,t) (Eq. 2) over time, the Lagrange multipliers is rather a trajectory λ(t), which is also known as costate trajectory. Thus, we have the solution of vˆ and uˆ via the following Hamiltonian functional: (4) 𝒥=∫0T ℒ(v,u,t)dt+λ(t)f(x,v,u,t)

Algorithm 1 Closed-loop RDM with feedback control Input: v0, Δ, Θ, Q, R, Φ	
Output: Optimal control ut	
Repeat	
 for t=1 to T do	
  Compute the feedback gain K(t) by solving Eq. 6;	
  Produce the optimal control by u(t)=−K(t)v0;	
 end for	
until System reaches a desired state.	

By plugging u=−Kv and H=x⊤Φx into Eq. 4, the gain matrix K has a closed-form solution as: (5) K=−R−1Θ⊤P

Note, P is an auxiliary N×N matrix by solving the Riccati equation [34]: (6) P(Δ+H)+H⊤(Δ+H)⊤P+Q−PΘR−1Θ⊤P=0

The detailed derivation process is shown in Appendix-A Compared to the conventional mechanics of closed-loop feedback systems, the LRQ control law u(t)=−K(t)v0 is not only time variant but also related to the individual’s reaction component H. The evolution of closed-loop RDM system with optimal feedback control is summarized in Algorithm 1.

2) Learning Common Control Mechanisms:

In many real-world applications, it may be difficult to measure the parameters Φ in the reaction function h(v,x,t) and design the interaction matrix Θ. Suppose we have a set of training data where we have the observations x0,…,xT for each instance. In general, we formulate the estimation of Φˆ and Θˆ as a regression problem: find a non-linear mapping function h to generate the initial system state v0 from x0 such that the final state vT from the closed-loop feedback RDM system (under LQR control regulator) can reproduce the observation xT through the inverse mapping h−1. In this context, we present an auto-encoder model integrated with PDE solver, as shown in Fig. 3 Specifically, we regard the encoder output as the initial state v0. Instead of reconstructing x0, we apply Algorithm 1 to obtain optimal control ut (green dash box) and then predict the final state vT, which depends on the current estimation of Φˆ and Θˆ. The driving force of learning Φ and Θ is the reconstruction error between xT and h−1vT through the back-propagation with respect to the loss function. By doing so, we constrain the solution space of Φ to be more aligned with the system mechanics of closed-loop feedback RDM.

3) Supervised Optimal Control for System-Environment Interaction:

Algorithm 1 allows us to maintain the RDM system under good controllability by applying optimal feedback u(t) (in Eq. 6. However, most of the neurobiological processes exhibit remarkable controllability even when external disturbances ω(t) have a substantial amount of offsets against the optimal feedback u(t). One possible explanation is that the feedback u(t) is calculated in a general sense of optimal control, not specific to the underlying neurobiological process of interest. With that being said, the multi-factorial mechanism behind the offset ω(t)−u(t) might underscore the latent neuroscience principle.

Since the final status ηT of biological process (such as assessment scores) is often available, we sought to link the terminal state vT to ηT in the way that the final status ηT can be predicted from vT by a prediction network ρ, i.e., ηˆT=ρvT,Ψ(Ψ denotes the parameters of mapping function ρ). To that end, we extend the Hamiltonian functional in Eq. 4 to a new regime of supervised learning: (7) 𝒥¯=Z(vT,ηT)︸Terminalcost+∫0Tℒ(v(t),u(t),t)+λ(t)f(x,v,u,t)︸Lagrangian dt

where ZvT,ηT=KLρvT,ηT is the terminal cost measuring the Kullback–Leibler (KL) divergence [35] between predicted final status vˆT and the ground truth ηT. Following the cliché of augmented Lagrangian multiplier, the integrated supervised feedback gain K can be solved by Eq. 5 where the dummy matrix P is solved from the equation (see Appendix-A): (8) P(Δ+H)+H⊤(Δ+H)⊤P+Q−PΘR−1Θ⊤P+Δ⊤Ψ⊤=0

C. Design Explainable Deep Model for Uncovering Latent Control Mechanism of Neuro-Dynamics

By integrating the learning components in Section III-B. we present the explainable deep model that allows us to uncover the latent control mechanism of neurobiological process from spatiotemporal data. The network hyper-parameters Γ={Θ,Φ,Ψ} include interaction matrix Θ, reaction parameters Φ, prediction parameters Ψ. The overall network design is shown in Fig. 3. In general, the deep model uses an autoencoder architecture. The encoder consists of a fully connected network (FCN) for reaction function h and a graph convolution network (GCN) for diffusion over the graph topology Δ. The hidden layer is a recurrent component where we use a PDE solver to characterize the system behavior of RDM with optimal control law. After that, the decoder is another FCN that share the parameter Φ with the encoder. Since the timeevolving feedback u(t) depends on (1) reconstruction error xT−xˆTF2 and (2) the prediction error ηT−ηˆTF2, our deep model opts to fine-tune the network parameters Γ in a supervised manner while maintain the explainability to the extent that the neuroscience insight from learned RDM system complies with the mechanic principles.

The cost paramters Q and R in the LQR regularizer (Eq. 3) are initilized with identity matrix. We use the re-wiring technique [22] to adjust parameters such as R,Q based on the learned attention weights (with respect to R and Q separately) on each graph node ξi.

D. Neuroimaging Application in Alzheimer’s Disease

In this section, we demonstrate the application of developing the explainable RDM-based deep model for assessment of system-level control enforced neural dynamics, coined ASCEND. The scientific question that we seek to answer using ASCEND is illustrated in Fig. 4

1) Application Background and Motivations:

Amyloid and tau are pathological hallmarks of AD, which can be measured from PET (positron emission tomography) scan [36]. The rapid development of neuroimaging technologies such as magnetic resonance imaging (MRI) and diffusion-weighted imaging (DWI) allow us to investigate the wiring of neuronal fibers (aka. structural connectomes) of human brain in-vivo [37], [38]. Since mounting evidence shows AD is characterized by the propagation of tau aggregates triggered by the Aβ build-up [7], [39], tremendous machine learning efforts have been made to predict the spreading of tau pathology in the progression of AD from longitudinal PET scans [40]–[43].

A large body of deep models de facto have been proposed to predict clinic outcomes by combining network topology heuristics and pathology measurements (i.e., Aβ and tau) at each brain region [44]–[47]. In spite of various machine learning backbones such as graph convolution neural (GCN) networks [48], most methods are formulated as a graph embedding representation learning problem, that is, aggregating the node features with a graph neighborhood in a “black box” such that the diffused graph embedding vectors are aligned with the one-hot vectors of outcome variables (i.e., healthy or disease condition). Although the graph attention technique [49], [50] allows us to quantify the contribution of each node/link in predicting outcome, it has limited power to fully dissect the mechanistic role of Aβ-tau interactions that emerges the dynamic prion-like pattern of tau propagation throughout the brain network. On the contrary, the PDE-based systems biology approach [9], [10] studies biological pathways and interactions between Aβ and tau at the system level, allowing us to uncover the intrinsic mechanism that steers the spatiotemporal dynamics of tau propagation throughout the brain. Our proposed ASCEND model carves nature at its joints by integrating the power of machine learning and system-level insight of neuroscience.

The input consists of longitudinal PET scans of amyloid and tau pathologies [36], where the RDM system is driven by tau aggregate (x in Eq. 2in Eq. 1 while the amyloid is the external disturbances (u in Eq. 2). Since we have the pathological measurements on each node, we set M=N. For each subject, we obtain the graph topology Δ based on the brain network constructed from the DWI scans by the tractography algorithm [51]. The output is the diagnostic label ηT (cognitive healthy or AD) predicted by the final state vT. The learned interaction matrix Θ between Aβ and tau allows us to understand the control mechanism of Aβ on the spreading of tau aggregate throughout the brain in the following studies.

2) Understanding the Control Mechanism of Aβ-Tau Interactions:

Following the notion of control theory [52], the wholebrain controllability of the tau propagation system (in Eq. 2) indicates the possibility of remaining cognitively normal (CN) or escalating the neurodegeneration process into AD by means of the external disturbances from Aβ plaques. The global controllability is defined on a Gramian matrix as: (9) 𝒦=∑t=0T(Δ)tΘˆΘˆ⊤(Δ⊤)t

It can be verified that the controllability Gramian matrix 𝒦 is positive definite if and only if the closed-loop feedback RDM system is controllable in T steps [53] (more details state in Appendix-B). Thus, the quantitative measurement in Eq. 9 allows us to address the question: “Is it possible to control the RDM system of tau propagation into an arbitrary state by modifying the Aβ-tau interaction?” Specifically, the positive sign of the smallest eigenvalues of 𝒦 indicates that the system is theoretically controllable through a single node. In general, the smaller the ratio of the smallest over largest eigenvalues is, the harder we can control the evolution of RDM system through a single node. Furthermore, we leverage Eq. 9 to choose control node one at a time (thus, the interaction matrix Θ is a vector) such that we can examine the node-specific average controllability based on the regional Gramian matrix 𝒦ξi(i=1,…,N). To that end, larger trace norm Trace⁡𝒦ξi indicates that node ξi has higher influence over the control of tau propagation system [52].

IV. Experiments

A. Data Description and Experimental Setting

1) Data description:

In our experiments, we utilize two datasets, namely the Alzheimer’s Disease Neuroimaging Initiative (ADNI) [54] and the Open Access Series of Imaging Studies-3 (OASIS3) [55], to evaluate the performance of our proposed method in the context of Alzheimer’s disease research.

ADNI dataset.

ADNI dataset comprises a total of 1205 subjects with A β-PET data, 549 samples for tau-PET, and 1012 structural connectomes (we processed). From the ADNI dataset [54], we selected 126 subjects who have diffusionweighted imaging (DWI) scans and longitudinal Aβ and tau PET scans (consisting of 2–5 time points) to ensure the availability of tau, Aβ, and structural connectomes for each subject.

OASIS3 dataset.

OASIS3 dataset consists of 1379 subjects, with 2842 MR sessions and 2157 PET sessions. From the OASIS3 dataset [55], we chose 81 subjects who had diffusionweighted imaging (DWI) scans and longitudinal Aβ and tau PET scans (consisting of 2 time points).

Table I lists the detailed demographic statistics for ADNI and OASIS3 (highlighted in gray shadow) data. We show age, MMSE (mini-mental state examination for baseline), gender, and longitudinal PET image acquisition interval period (from baseline to the last follow-up visit).

For each dataset, we performed the following data preprocessing steps. First, we segment the T1-weighted image into white matter, gray matter, and cerebral spinal fluid using FSL software [56]. On top of the tissue probability map, we parcellate the cortical surface into 148 cortical regions and 12 sub-cortical regions, using the Destrieux atlas [57] (yellow arrows in Fig. 5). Second, we convert each DWI scan to diffusion tensor images (DTI) [58]. Second, we apply surface seed-based probabilistic fiber tractography [51] using the DTI data, thus producing a 160×160 anatomical connectivity matrix (white arrows in Fig. 5). Note, the weight of the anatomical connectivity is defined by the number of fibers linking two brain regions normalized by the total number of fibers in the whole brain (Δ for graph diffusion in ASCEND). Third, following the region parcellations, we calculate the regional concentration level of the amyloid and tau pathologies for each brain region (red arrows in Fig. 5), yielding the input u,x∈ℛ160 for training ASCEND, respectively.

2) Experimental setup:

For ADNI data, since the clinical diagnostic label is available at each visit time, we split long time series (≥ 3 time points) into a collection of 2-time-point temporal segments. By doing so, we augment the sample pool to a magnitude of 1.6 times larger. Following the clinical outcomes, we partition the subjects into the cognitive normal (CN), subjective memory complaints (SMC), early-stage mild cognitive impairment (EMCI), late-stage mild cognitive impairment (LMCI), and AD groups. Regarding the diagnostic label, we roughly divide the cohort into CN and AD based on the disease severity (i.e., CN, SMC and EMCI as ‘CN’ group, while LMCI and AD as ‘AD’ groups), in order to set the scene for binary classification. For the OASIS3 dataset, we also develop binary classifiers with preclinical stage Alzheimer’s patients or patients suffering from other dementia problems labeled as ‘AD’ group and healthy individuals as ‘CN’ group. We train the proposed ASCEND on ADNI and OASIS3 data, respectively. For the classification task, the loss function is cross-entropy. For the prediction task, the loss function is the mean square error (MSE) between the ground truth (observed BOLD signal of the next time point) and the predicted one. The variation adaptive moment estimation (Adam) [59] (learning rate 0.001 and epoch 500) is used in gradient back-propagation (see Section IV-A.3).

We report the testing results using 5-fold cross-validation. In Section IV-B we conduct the ablation study with the comparison to the following counterpart methods: (1) Graph conventional network (GCN) [60] (the amyloid and tau measurement are treated as graph embedding vectors); (2) Liquid time-constant network (LTC-Net) [20] (a PDE-based learning model to predict motion outcome from time series), and (3) Neuro-RDM [30] (a neural network for open-loop RDM on tau pathology only); (3) Graph attention networks (GAT) [49]; (4) A sample and deep graph convolutional networks (GCNII) [61]; (5) DeepGCNs [62] and (6) Graph neural diffusion (GRAND) [22]. The evaluation metrics include predicting the level of tau burden and clinical outcomes. It is worth noting that none of these counterpart methods model the control mechanism in machine learning. Thus, we interpret the results on controllability of tau propagation only with reference to the current clinical findings in Section IV-C.

3) Implementation details:

Table II lists the detailed parameter setting on ASCEND, GCN, LTC-Net, Neuro-RDM, GAT, GCNII, DeepGCNs and GRAND. In all experiments, we set all output dimensions equal to the input dimension (160 elements) since the number of brain nodes can not be changed, and the hidden dimension is all set to 32 (for GAT, we set head=8, hidden dimension=4). Table III compares the running times between our ASCEND and other comparison methods. All experiments are conducted on a PC specs: 12th Gen Intel(R) Core(TM) i7–12700F 2.10 GHz with 32.0 GB of RAM with NVIDIA GeForce RTX 3070 Ti with 8GB of dedicated GPU memory.

B. Ablation Study in Prediction Disease Progression

We design an ablation study in the scene of predicting the future tau burden xT using the baseline tau level x0, where we model the influence of amyloid build-up u0…uT in the time course of tau propagation dvdt. Mean absolute error (MAE) between xˆT and xT is used to measure the prediction accuracy. Take the ADNI data as an example, as shown in the bottom of Table IV (middle), the prediction error by our ASCEND method (0.022) shows 45% reduction compared to our (downgraded) method without LQR control regulator (0.040), and 30% reduction compared to our full method but without the supervised LQR control (0.028), where ‘*’ indicate the accuracy improvement is statistically significant (p&lt;0.001). We also display the prediction MAE results on ADNI and OASIS3 data by LTC-Net, Neuro-RDM, GCN, GAT, GCNII, DeepGCNs and GRAND using tau only and tau +Aβ as the input respectively. It is clear that (1) the prediction accuracy by the counterpart methods is much less reliable than our ASCEND since the machine learning model does not fully capture neurobiological mechanism, and (2) adding additional information (Aβ) does not contribute to the prediction, partially due to the lack of modeling Aβ-tau interaction.

We conduct additional experiments using GCN-based models with varying numbers of layers to predict the future level of tau aggregate. The results in Table V reveal that increasing the number of layers did not consistently improve performance. This unexpected finding may be attributed to over-smoothing, where node representations become too similar, hindering accurate predictions. The observed diminishing returns with deeper GCN layers offer valuable insights for future research. By tackling the challenges associated with over-smoothing, we can potentially discover new approaches to enhance the accuracy and effectiveness of GCN-based methods in predicting tau aggregate levels. GCNII, DeepGCNs and GRAND, specifically designed for GCN, can alleviate this issue while demonstrating the validity of our hypothesis. On the other hand, GCN-based methods initially process scalar (or two-dimension) features, specifically regional tau SUVR w/o (or w/) Aβ. Deepening the number of layers and complexity of the network does not improve prediction performance since the limited availability of rich information. The fact that increasing the complexity of the network (e.g., GAT, GCNII and GRAND) does not improve prediction accuracy effectively validates this speculation.

C. Clinical Value and Neuroscience Insight in AD

First, suppose we have the baseline amyloid and tau scans, we evaluate the prediction accuracy on ADNI and OASIS3 data in forecasting the risk of developing AD by LTC-Net, Neuro-RDM, GCN, GAT, GCNII, DeepGCNs, GRAND and ASCEND in Table IV (right). At the significance level of 0.001, our method outperforms all other counterpart methods in terms of classification accuracy (indicated by ‘*’). Second, we sought to uncover the Aβ-tau control mechanism through the explainable deep model, by answering the following scientific questions.

(1) In what mechanism that local or remote Aβ-tau interaction promotes the spreading of tau aggregates?

We seek for the answer from the interaction matrix Θ in Eq. 2, which is essentially the main motivation of our explainable deep model. By considering all brain regions as the control nodes, Θ is a N×N matrix where the diagonal-line elements indicate the local Aβ-tau interaction (on the same node) while the rest of elements in Θ captures remote Aβ-tau interaction (between distinct nodes). We visualize the interaction matrix Θ and the corresponding brain mapping (node size and link bandwidth are in proportion to the strength of local and remote interaction, respectively) in Fig. 6(a). As shown in Fig. 6(b), we find that the predominant interaction mechanism is that Aβ plaques contribute to the local cascade of tau aggregates. However, there are a few number of strong remote interactions in the middle-temporal lobe, where the high activity of tau pathology has been frequently reported [43], [63].

(2) Does the whole-brain propagation of tau a controllable process?

Yes. Following the notion of global controllability (Eq. 9), we find all eigenvalues of each 𝒦ξi are consistently greater than 0, indicating that tau propagation is theoretically controllable through a single region. Nevertheless, the smallest eigenvalues (0.28×10−6 ±0.20 ×10−6) are surprisingly 10−6 times less than the large eigenvalue (1.08 ± 0.31), indicating that the system built by the human brain might be hard to control through single brain region.

(3) Which nodes are most vulnerable in the progression of AD ?

We retrain ASCDEND on ADNI and OASIS3 data using AD subjects only, respectivity. Then, we calculate the nodewise average controllability that projects the amount of effort (amyloid build-up in the scenario of AD progression) needed to reach the terminal state vT (i.e., developing AD). Note, a small degree of average controllability indicates that the underlying node is vulnerable to the intervention of amyloid plaques. In this context, we display the top 10 most vulnerable brain regions in Fig. 6(c). It is apparent that most brain regions are located in temporal, occipital and limbic lobes, and subcortical areas, which is in line with the current clinical findings [63].

D. Discussion

Due to the potential issue of multi-site effect, we report the computer-assisted diagnosis and Aβ-tau interaction results for ADNI and OASIS separately. In the right panel of Table IV. we show the prediction accuracy of developing AD based on the longitudinal neuroimages, where the prediction accuracy on OASIS is consistently higher than the counterpart results on ADNI. We postulate the underlying reasons. (1) Unlike ADNI, the 81 longitudinal image sequences selected from the OASIS database show no label change (e.g., progression from CN to AD). As a result, the cognitive decline trajectories in OASIS appear to be relatively less heterogeneous, which makes the AD prognosis less challenging compared to ADNI. (2) The average age gap between the baseline and the last follow-up visit is about 6 months (the range of age gap is 1–12 months) in the selected 81 longitudinal image sequences from the OASIS database. In contrast, the average age gap in ADNI is about 18 months (1.5 years). As a result, the much longer prediction window in ADNI makes the AD prognosis more challenging compared to OASIS.

OASIS study primarily focused on pre-clinical cohort during the initial enrollment and followed longitudinal progression, whereas ADNI enrolled individuals with dementia or mild cognitive impairment [64]. As shown in Table I the average age in OASIS and ADNI are 62.5 and 70.5, respectively. Due to such differences in terms of population age and disease progression stage, the results on Aβ interactions vary between ADNI and OASIS (as shown in Fig. 6)

In the future, we will employ data harmonization methods [65] to combine the neuroimages from multiple sites, which could increase the sample size and lead to more integrated neuroscience discoveries. Also, we will apply the proposed deep models to functional connectomes, which allows us to understand the mechanistic role of functional coactivations in tau propagation.

V. Conclusion

In this work, we initialize an explainable machine learning effort to uncover the system-level control mechanism from the unprecedented amount of spatiotemporal data. Since RDM has been well studied in the neuroscience field, we formulate optimal control law in the closed-loop feedback RDM and dissect it into the deep model. In light of this, we leverage the physics principle to guarantee the deep model is learning the intrinsic biological mechanism that describes the footprints manifested in various spatiotemporal data. We have applied our RDMbased deep model to investigate the prion-like propagation mechanism of tau aggregates as well as the downstream association with clinical manifestations in AD, where we find integrating the control mechanism in the machine not only achieves significant improvement in prediction accuracy of developing AD, but also sheds the new light to discover the latent pathophysiological mechanism of disease progression using a data-driven approach.

Appendix

A. Proof of Eq. (6) and Eq. (8)

The Linear Quadratic Regulator (LQR) is a well-studied method that provides optimally controlled feedback gains to enable the closed-loop stable and high-performance design of systems [11]. For the derivation of the linear quadratic regulator, we consider the system state-space representation as Eq. 2. Towards a generic procedure for solving optimal control problems, we derive a methodology based on the calculus of variations. The problem statement for a fixed end time tT is to minimize (10) 𝒥=Z(v(tT))︸Terminalcost+∫t0tTℒ(v(t),u(t),t)︸Lagrangiandtsujectto:dvdt=f(x(t),v(t),u(t),t)

where ZvtT is the terminal cost, and the total cost 𝒥 is a sum of the terminal cost and an integral along the evolutionary trajectories. We assume that ℒ(v(t),u(t),t) is nonnegative. The first step is to augment the cost using a Lagrange multiplier λ(t): (11) 𝒥¯=Z(v(tT))+∫t0tT(ℒ+λ⊤(f−dvdt))dt

Note, λ⊤ can have arbitrary expression, since it multiplies f− dvdt=0.λ is also known as a costate vector, which enforces the dynamic constraints. Along the optimum, variations in 𝒥 and hence 𝒥− should vanish. This follows from the fact that 𝒥 is chosen to be continuous in v,u and t. We write the variation as: (12) δ𝒥¯=ZvδvtT+∫t0tTℒvδv+ℒuδu+λ⊤fvhvδv+λ⊤fuδu−λ⊤δdvdtdt

where subscripts denote partial derivatives. The last term above can be evaluated using integration by parts as: (13) ∫t0tT−λ⊤δdvdtdt=−λ⊤tTδvtT+λ⊤t0δvt0+∫t0tTλ˙⊤δvdt

therefore the total variation of the cost function in Eq. 12 can be rewritten as: (14) δ𝒥¯=ZvvtTδvtT+∫t0tTLu+λ⊤fuδudt+∫t0tTℒv+λ⊤fvhv+λ˙⊤δvdt−λ

The last term is zero, since we cannot vary the initial of the state by changing something later in time. This writing of 𝒥− indicates that there are three components of the variation that must independently be zero for an optimal control solution. Thus, we may break this up into three equations: (15) {ℒu+λ⊤fu=0ℒv+λ⊤fvhv+λ˙⊤=0Zv(v(tT))−λ⊤(tT)=0

It is clear that the second and third requirements can be satisfied by explicitly letting (16) {λ˙⊤=−ℒv−λTfvhvλ⊤(tT)=Zv(v(tT))

The evolution of λ is given in reverse time, from a final state to the initial. Hence we see the primary difficulty of solving optimal control problems: the state propagates forward in time, while the costate propagates backward. The state and costate are coordinated through the above equations.

Proof of Eq. (6): In the case of the Linear Quadratic Regulator (with zero terminal cost), we set Z(⋅)=0, and define the quadratic cost as Eq. 3 In the case of linear plant dynamics1 also, we have (17) {ℒv=v⊤Qℒu=u⊤Rfv=Δ+Hfu=Θ

then substitute into Eq. 15, so that (18) {Ru+Θ⊤λ=0λ˙=−Qv−H⊤(Δ⊤+H⊤)λλ(tT)=0

To solve the above equations, it is possible to posit a form and try a connection λ=Pv. Inserting this into λ˙ equation, and then using the dvdt equation, and a substitution for u, we obtain (19) PΔv+H⊤Δ⊤Pv+H⊤H⊤Pv+Qv−PΘR−1Θ⊤Pv+PHv=0

This has to hold for all x, so in fact it is a matrix equation, the matrix Riccatti equation. The steady-state solution is given satisfies: (20) PΔ+Hv⊤Δ⊤P+H⊤H⊤P+Q−PΘR−1Θ⊤P+PH=0

After merging like terms, Eq. 6 is proved (Note, in the main text, we let t0=0,tT=T,vt0=v0,vtT= vT for clarity). Eq. 20 is known as the Matrix Algebraic Riccati Equation (MARE) [66], whose solution P is needed to compute the optimal feedback gain K. The MARE can be easily solved by standard numerical tools in linear algebra [66]. The equation Ru+Θ⊤λ=0 gives the feedback law (21) u=−R−1Θ⊤Pv

According to u=−Kv, we can obtain feedback gain K= R−1Θ⊤P.

Proof of Eq. (8): In the case of the Linear Quadratic Regulator (with supervised terminal cost), we set Z (⋅)= ZvT,ηT, thus the third term is rewritten as λ⊤tT= ZvvtT=ZvvT,ηT=Ψv according to Eq. 15. Similar to the derivation of Eq. 19, we can obtain Eq. 8 in the main text.

B. A closed-look into the PDE-based neural network optimization

As shown in Fig. 3, the entire optimization process of network parameters Γ is formulated as the way of elucidating the system mechanism. In order to validate the estimated solution Γˆ does not result in the degenerated system, we monitor the rank of controllability matrix 𝒞 during the optimization, which can be derived by Gramian matrix 𝒦 (Eq. 9), i.e., 𝒦=𝒞T𝒞T⊤. Broadly, the controllability of the system is determined entirely by the column space of the controllability matrix 𝒞:𝒞=[ΘΔΘΔ2Θ⋯ΔT−1Θ] the number of linearly independent columns for the matrix 𝒞 determines the controllability of the system. We record the controllability matrix 𝒞 during the training, we found that the controllability matrix 𝒞 consistently maintains being a full-rank matrix, indicating that the estimation of system parameters Γ is physically reasonable.

Fig. 1. Our major contributions include (a) Elucidating the optima control mechanism for closed-loop feedback RDM system, (b) Learning common mechanisms across RDM systems, and (c) Understanding the interaction between system and environment. Note, we use tilting dolls as an analogy to the RDM system.

Fig. 2. The sketch of developing an explainable deep model by formulating the insight of complex system (such as human brain) and physics principles in the machine learning process.

Fig. 3. The overall network architecture of closed-loop feedback RDM system with the integrated supervised optimal control. There are three major components: (1) the auto-encoder (in orange) to learn interaction matrix Θ and reaction parameters Φ, (2) PDE solver for closed-loop feedback RDM system with optimal control (in green) which recursively yields final state vT given the initial condition v0 (indicated by dash arrow), and (3) the driving force (in blue) includes the reconstruction error between xT and xˆT and the prediction error of final status ηT.

Fig. 4. Left: We apply the explainable deep model ASCEND to the spatiotemporal data of amyloid and tau aggregates, with the intention to understand the mechanistic role of Aβ-tau interaction in the spreading of tau pathologies throughout the brain. Right: We study the global and regional controllability in the RDM system of tau propagation, which allows us to elucidate the control mechanism of Aβ in the disease progression of AD.

Fig. 5. General workflows for processing T1-weighted image (yellow arrows), diffusion-weighted image (white arrows), and PET images (red arrows). The output is shown at the bottom right, including the brain network, and regional concentration level of amyloid and tau aggregates.

Fig. 6. (a) The visualization of local (diagonal line in Θ) and remote (off-diagonal line) interaction of Aβ-tau interactions. The node size and link bandwidth are in proportion to the strength of local and remote interactions, respectively. (b) Interaction matrix Θ. (c) The top-ranked critical brain regions that are vulnerable to the intervention of amyloid build-up, where node size indicates the ranking. (Note, node color corresponds to the brain lobe parcellations [57].)

TABLE I Demographic statistics for ADNI and OASIS3 (Shaded) data. M/F denotes male/female and Y denotes year.

ADNI	Age	MMSE	Gender	Period	
Mean±std	70.5±6.5	28.0±2.0	50%/50%	1.5±0.5	
Range	55 ~ 86	20 ~ 30	63/63 (M/F)	1~4 (Y)	
OASIS3	Age	MMSE	Gender	Period	
Meanistd	62.5±7.5	29.0±1.0	71.6%/28.4%	0.6±0.5	
Range	45 ~ 76	24 ~ 30	58/23 (M/F)	0~1 (Y)	

TABLE II Parameters settings on ADNI/OASIS3 data.

Algorithm	Optimizer	learning rate	weight decay	dropout	epoch	
ASCEND	Adam	0.001	1 × 10−5	0.5	500	
GCN	Adam	0.01	5 × 10−4	0.5	500	
LTC-Net *	Adam	0.01	–	–	500	
Neuro-RDM	Adam	0.01	1 × 10−5	0.5	500	
GAT	Adam	0.001	5 × 10−4	0.6	800	
GCNII	Adam	0.005	5 × 10−4	0.6	1500	
DeepGCNs	Adam	0.005	5 × 10−4	0.1	500	
GRAND	Adam	0.001	5 × 10−4	0.5	500	
* LTC-Net uses ODE-solver instead of a graph neural network.

TABLE III Runing time of per subject (item) on ADNI and OASIS3 (shaded) data for the involved methods.

Algorithm	ASCEND	GCN	LTC-Net	Neuro-RDM	
Time (item/s)	2.02	0.94	1.46	1.73	
Time (item/s)	2.01	0.98	1.47	1.71	
Algorithm	GAT	GCNII	GRAND	DeepGCNs	
Time (item/s)	0.98	0.99	1.17	1.00	
Time (item/s)	0.99	0.98	1.19	0.99	

TABLE IV Middle: Ablation study on predicting future level of tau aggregate. Right: Prognosis accuracies on forecasting AD risk.

Input	Methods	MAE (ADNI)	MAE (OASIS3)	Accuracy (ADNI)	Accuracy (OASIS3)	
Tau	LTC-Net	0.063 ± 0.029	0.085 ± 0.047	0.8167 ± 0.0442 (*)	0.8849 ± 0.0414 (*)	
Tau+Aβ	LTC-Net	0.085 ± 0.047	0.092 ± 0.052	0.8224 ± 0.0486 (*)	0.8993 ± 0.0696 (*)	
Tau	Neuro-RDM	0.057 ± 0.025	0.063 ± 0.040	0.8255 ± 0.0381 (*)	0.9050 ± 0.0104 (*)	
Tau+Aβ	Neuro-RDM	0.058 ± 0.041	0.066 ± 0.039	0.8301 ± 0.0413 (*)	0.9065 ± 0.0145 (*)	
Tau	GCN	0.124 ± 0.077	0.256 ± 0.038	0.7707± 0.0215 (*)	0.8705 ± 0.0220 (*)	
Tau+Aβ	GCN	0.144 ± 0.091	0.277 ± 0.040	0.7875 ± 0.0359 (*)	0.8777 ± 0.0211 (*)	
Tau	GAT	0.306 ± 0.048	0.419 ± 0.040	0.8105 ± 0.0204 (*)	0.8842 ± 0.0205 (*)	
Tau+Aβ	GAT	0.357 ± 0.055	0.477 ± 0.040	0.8263 ± 0.0188 (*)	0.8921 ± 0.0117 (*)	
Tau	GCNII	0.207 ± 0.031	0.370 ± 0.044	0.8379 ± 0.0124 (*)	0.9052 ± 0.0114 (*)	
Tau+Aβ	GCNII	0.227 ± 0.037	0.371 ± 0.043	0.8405 ± 0.0119 (*)	0.9137 ± 0.0192 (*)	
Tau	DeepGCNs	0.079 ± 0.011	0.155 ± 0.037	0.8403 ± 0.0214 (*)	0.9169 ± 0.0136 (*)	
Tau+Aβ	DeepGCNs	0.087 ± 0.016	0.163 ± 0.049	0.8415 ± 0.0105	0.9209 ± 0.0118 (*)	
Tau	GRAND	0.259 ± 0.037	0.293 ± 0.031	0.8396 ± 0.0108 (*)	0.9249 ± 0.0100 (*)	
Tau+Aβ	GRAND	0.277 ± 0.043	0.316 ± 0.034	0.8414 ± 0.0100	0.9281 ± 0.0096 (*)	
Tau+Aβ	ASCEND(w/o LQR)	0.040 ± 0.031 (*)	0.052 ± 0.041(*)	–	–	
Tau+Aβ	ASCEND(w/ LQR)	0.028 ± 0.027 (*)	0.045 ± 0.035(*)	–	–	
Tau+Aβ	ASCEND	0.022 ± 0.021	0.047 ± 0.035	0.8434± 0.0475	0.9353 ± 0.0169	
BOLD denotes the best performance.

‘*’ indicates that there is a significant improvement in our method compared to the comparison methods.

TABLE V Performance of GCN-based methods with different network layers on ADNI and OASIS3 (shaded) data.

Layers (#)	ASCEND	GCN	Neuro-RDM	GAT	GCNII	DeepGCNs	GRAND	
1	0.022 ± 0.021	0.149 ± 0.101	0.058 ± 0.025	0.394 ± 0.044	0.228 ± 0.040	0.090 ± 0.020	0.281 ± 0.040	
2	0.023 ± 0.024	0.144 ± 0.091	0.060 ± 0.026	0.357 ± 0.055	0.227 ± 0.037	0.087 ± 0.016	0.277 ± 0.043	
4	0.028 ± 0.026	0.154 ± 0.094	0.061± 0.028	0.413 ± 0.051	0.210 ± 0.036	0.092 ± 0.021	0.271 ± 0.043	
1	0.047 ± 0.035	0.276 ± 0.057	0.073 ± 0.033	0.488 ± 0.047	0.376 ± 0.045	0.160 ± 0.046	0.321 ± 0.035	
2	0.049 ± 0.038	0.277 ± 0.040	0.066 ± 0.039	0.477 ± 0.040	0.371 ± 0.043	0.163 ± 0.049	0.316 ± 0.034	
4	0.054 ± 0.041	0.284 ± 0.042	0.067 ± 0.038	0.480 ± 0.042	0.369 ± 0.045	0.157 ± 0.026	0.310 ± 0.034	
BOLD denotes the best performance.

1 https://en.wikipedia.org/wiki/Plant_(control_theory).


References

[1] Tang Y , Kurths J , Lin W , Ott E , and Kocarev L , “Introduction to focus issue: When machine learning meets complex systems: Networks, chaos, and nonlinear dynamics,” Chaos: An Interdisciplinary Journal of Nonlinear Science, vol. 30 , no. 6 , p. 063151, 2020.
[2] Gao Z , Li Y , Yang Y , and Ma C , “A recurrence network-based convolutional neural network for fatigue driving detection from eeg,” Chaos: An Interdisciplinary Journal of Nonlinear Science, vol. 29 , no. 11 , p. 113126, 2019.
[3] Qi D and Majda AJ , “Using machine learning to predict extreme events in complex systems,” Proceedings of the National Academy of Sciences, vol. 117 , no. 1 , pp. 52–59, 2020.
[4] Bassett DS and Gazzaniga MS , “Understanding complexity in the human brain,” Trends in Cognitive Sciences, vol. 15 , no. 5 , pp. 200–209, 2011.21497128
[5] Telesford QK , Simpson SL , Burdette JH , Hayasaka S , and Laurienti PJ , “The brain as a complex system: Using network science as a tool for understanding the brain,” Brain Connectivity, vol. 1 , no. 4 , pp 295–308, 2011.22432419
[6] Bassett DS and Sporns O , “Network neuroscience,” Nature Neuroscience, vol. 20 , no. 3 , pp. 353–364, 2017.28230844
[7] Bloom GS , “Amyloid- and Tau: The Trigger and Bullet in Alzheimer Disease Pathogenesis,” JAMA Neurology, vol. 71 , no. 4 , pp. 505–508, 2014.24493463
[8] Kondo S and Miura T , “Reaction-diffusion model as a framework for understanding biological pattern formation,” Science, vol. 329 , no. 5999 , pp. 1616–1620, 2010.20929839
[9] Hao W and Friedman A , “Mathematical model on alzheimer’s disease,” BMC Systems Biology, vol. 10 , no. 1 , p. 108, Nov 2016.27863488
[10] Zhang J , Yang D , He W , Wu G , and Chen M , “A network-guided reaction-diffusion model of at [n] biomarkers in alzheimer’s disease,” in 2020 IEEE 20th International Conference on Bioinformatics and Bioengineering (BIBE). IEEE, 2020, pp. 222–229.
[11] Brunton SL and Kutz JN , Data-driven science and engineering: Machine learning, dynamical systems, and control. Cambridge University Press, 2022.
[12] Bajada CJ , Schreiber J , and Caspers S , “Fiber length profiling: A novel approach to structural brain organization,” NeuroImage, vol. 186 , pp. 164–173, 2019.30399419
[13] Power JD , Cohen AL , Nelson SM , Wig GS , Barnes KA , Church JA , Vogel AC , Laumann TO , Miezin FM , Schlaggar BL , “Functional network organization of the human brain,” Neuron, vol. 72 , no. 4 , pp. 665–678, 2011.22099467
[14] Lozano AM and Lipsman N , “Probing and regulating dysfunctional circuits using deep brain stimulation,” Neuron, vol. 77 , no. 3 , pp. 406–424, 2013.23395370
[15] Lynn CW and Bassett DS , “The physics of brain network structure, function and control,” Nature Reviews Physics, vol. 1 , no. 5 , pp. 318–332,2019.
[16] Karafyllis I , Christofides PD , and Daoutidis P , “Dynamical analysis of a reaction-diffusion system with brusselator kinetics under feedback control,” in Proceedings of the 1997 American Control Conference (Cat. No. 97CH36041), vol. 4 . IEEE, 1997, pp. 2213–2217.
[17] Goedert M , Eisenberg DS , and Crowther RA , “Propagation of tau aggregates and neurodegeneration,” Annual review of neuroscience, vol. 40 , pp. 189–210, 2017.
[18] Lhachemi H and Prieur C , “Local output feedback stabilization of reaction-diffusion PDEs with saturated measurement,” IMA Journal of Mathematical Control and Information, vol. 39 , no. 2 , pp. 789–805, 2022.
[19] Chen RT , Rubanova Y , Bettencourt J , and Duvenaud DK , “Neural ordinary differential equations,” Advances in neural information processing systems, vol. 31 , 2018.
[20] Hasani RM , Lechner M , Amini A , Rus D , and Grosu R , “Liquid time-constant networks,” in AAAI Conference on Artificial Intelligence, 2020.
[21] Brunton SL , Noack BR , and Koumoutsakos P , “Machine learning for fluid mechanics,” Annual Review of Fluid Mechanics, vol. 52 , no. 1 , pp. 477–508, 2020.
[22] Chamberlain BP , Rowbottom J , Gorinova MI , Webb SD , Rossi E , and Bronstein MM , “GRAND: Graph neural diffusion,” in The Symbiosis of Deep Learning and Differential Equations, 2021.
[23] Eliasof M , Haber E , and Treister E , “PDE-GCN: Novel architectures for graph neural networks motivated by partial differential equations,” in Advances in Neural Information Processing Systems, Beygelzimer A , Dauphin Y , Liang P , and Vaughan JW , Eds., 2021.
[24] Nakamura-Zimmerer T , Gong Q , and Kang W , “Qrnet: optimal regulator design with lqr-augmented neural networks,” IEEE Control Systems Letters, vol. 5 , no. 4 , pp. 1303–1308, 2020.
[25] Baby D and Wang Y-X , “Optimal dynamic regret in lqr control,” Advances in Neural Information Processing Systems, vol. 35 , pp. 24879–24892,2022.
[26] Arbabi H , Korda M , and Mezić I , “A data-driven koopman model predictive control framework for nonlinear partial differential equations,” in 2018 IEEE Conference on Decision and Control (CDC). IEEE, 2018, pp. 6409–6414.
[27] Scellier B , “A deep learning theory for neural networks grounded in physics,” arXiv preprint arXiv:2103.09985, 2021.
[28] Chen Z , Feng M , Yan J , and Zha H , “Learning neural hamiltonian dynamics: A methodological overview,” arXiv preprint arXiv:2203.00128, 2022.
[29] Kalman D , “Leveling with lagrange: An alternate view of constrained optimization,” Mathematics Magazine, vol. 82 , no. 3 , pp. 186–196, 2009.
[30] Dan T , Cai H , Huang Z , Laurienti P , Kim WH , and Wu G , “Neuro-RDM: An explainable neural network landscape of reactiondiffusion model for cognitive task recognition,” in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2022, pp. 365–374.
[31] Wescott T , Applied control theory for embedded systems. Elsevier, 2011.
[32] Bemporad A , Morari M , Dua V , and Pistikopoulos EN , “The explicit linear quadratic regulator for constrained systems,” Automatica, vol. 38 , no. 1 , pp. 3–20, 2002.
[33] Onori S , Serrao L , Rizzoni G , Onori S , Serrao L , and Rizzoni G , “Pontryagin’s minimum principle,” Hybrid Electric Vehicles: Energy Management Strategies, pp. 51–63, 2016.
[34] Bittanti S , LAUB A , and WILLEMS J , “The riccati equation(book),” Berlin and New York, Springer-Verlag, 1991, 347 , 1991.
[35] Kullback S and Leibler RA , “On information and sufficiency,” The annals of mathematical statistics, vol. 22 , no. 1 , pp. 79–86, 1951.
[36] Cecchin D , Garibotto V , Law I , and Goffin K , “Pet imaging in neurodegeneration and neuro-oncology: Variants and pitfalls,” Seminars in Nuclear Medicine, vol. 51 , no. 5 , pp. 408–418, 2021.33820651
[37] Van Essen DC , Smith SM , Barch DM , Behrens TE , Yacoub E , and Ugurbil K , “The wu-minn human connectome project: An overview,” NeuroImage, vol. 80 , pp. 62–79, 2013.23684880
[38] Elam JS , Glasser MF , Harms MP , Sotiropoulos SN , Andersson JL , Burgess GC , Curtiss SW , Oostenveld R , Larson-Prior LJ , Schoffelen J-M , Hodge MR , Cler EA , Marcus DM , Barch DM , Yacoub E , Smith SM , Ugurbil K , and Van Essen DC , “The human connectome project: A retrospective,” NeuroImage, vol. 244 , p. 118543,2021.34508893
[39] Guzman-Velez E , Diez IP , Schoemaker D , Pardilla-Delgado E , Fox-Fuller JT , Vila-Castelar C , Baena AY , Sperling RA , Johnson KA , Lopera F , Sepulcre J , and Quiroz YT , “Amyloid- and tau pathologies relate to distinctive brain dysconnectomics in autosomaldominant alzheimer’s disease,” Alzheimer’s &amp; Dementia, vol. 17 , no. S4 , p. e056134, 2021.
[40] Jack CR Jr. , Bennett DA , Blennow K , Carrillo MC , Dunn B , Haeberlein SB , Holtzman DM , Jagust W , Jessen F , Karlawish J , Liu E , Molinuevo JL , Montine T , Phelps C , Rankin KP , Rowe CC , Scheltens P , Siemers E , Snyder HM , Sperling R , Contributors, Elliott C , Masliah E , Ryan L , and Silverberg N , “Nia-aa research framework: Toward a biological definition of alzheimer’s disease,” Alzheimer’s &amp; Dementia, vol. 14 , no. 4 , pp. 535–562, 2018.
[41] Syaifullah AH , Shiino A , Kitahara H , Ito R , Ishida M , and Tanigaki K , “Machine learning for diagnosis of ad and prediction of mci progression from brain mri using brain anatomical analysis using diffeomorphic deformation,” Frontiers in Neurology, vol. 11 , 2021.
[42] Sharma DK , Chatterjee M , Kaur G , and Vavilala S , “Deep learning applications for disease diagnosis,” in Deep learning for medical applications with unique data. Elsevier, 2022, pp. 31–51.
[43] Vogel JW , Young AL , and , “Four distinct trajectories of tau deposition identified in alzheimer’s disease,” Nature Medicine, vol. 27 , no. 5 , pp. 871–881, 2021.
[44] Hernández-Lorenzo L , Hoffmann M , Scheibling E , List M , Matías-Guiu JA , and Ayala JL , “On the limits of graph neural networks for the early diagnosis of alzheimer’s disease,” Scientific Reports, vol. 12 , no. 1 , p. 17632, Oct 2022.36271229
[45] Song TA , Chowdhury SR , Yang F , Jacobs H , El Fakhri G , Li Q , Johnson K , and Dutta J , “Graph convolutional neural networks for alzheimer’s disease classification,” in 2019 IEEE 16th international symposium on biomedical imaging (ISBI 2019). IEEE, 2019, pp. 414417.
[46] Shan X , Cao J , Huo S , Chen L , Sarrigiannis PG , and Zhao Y , “Spatial-temporal graph convolutional network for alzheimer classification based on brain functional connectivity imaging of electroencephalogram,” Human Brain Mapping, vol. 43 , no. 17 , pp. 5194–5209, 2022.35751844
[47] Kim M , Kim J , Qu J , Huang H , Long Q , Sohn K-A , Kim D , and Shen L , “Interpretable temporal graph neural network for prognostic prediction of alzheimer’s disease using longitudinal neuroimaging data,” in 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM). IEEE, 2021, pp. 1381–1384.
[48] Kipf TN and Welling M , “Semi-supervised classification with graph convolutional networks,” in International Conference on Learning Representations, 2017.
[49] Veličković P , Cucurull G , Casanova A , Romero A , Lio P , and Bengio Y , “Graph attention networks,” arXiv preprint arXiv:1710.10903, 2017.
[50] Ma X , Wu G , and Kim WH , “Multi-resolution graph neural network for identifying disease-specific variations in brain connectivity,” arXiv preprint arXiv:1912.01181, 2019.
[51] Fillard P , Descoteaux M , Goh A , Gouttard S , Jeurissen B , Malcolm J , Ramirez-Manzanares A , Reisert M , Sakaie K , Tensaouti F , “Quantitative evaluation of 10 tractography algorithms on a realistic diffusion mr phantom,” Neuroimage, vol. 56 , no. 1 , pp. 220–234, 2011.21256221
[52] Pasqualetti F , Zampieri S , and Bullo F , “Controllability metrics, limitations and algorithms for complex networks,” IEEE Transactions on Control of Network Systems, vol. 1 , no. 1 , pp. 40–52, 2014.
[53] Kailath T , Linear systems. Prentice-Hall Englewood Cliffs, NJ, 1980, vol. 156 .
[54] Petersen RC , Aisen PS , Beckett LA , Donohue MC , Gamst AC , Harvey DJ , Jack CR , Jagust WJ , Shaw LM , Toga AW , Trojanowski JQ , and Weiner MW , “Alzheimer’s disease neuroimaging initiative (adni),” Neurology, vol. 74 , no. 3 , pp. 201–209, 2010.20042704
[55] Marcus DS , Wang TH , Parker J , Csernansky JG , Morris JC , and Buckner RL , “Open access series of imaging studies (oasis): crosssectional mri data in young, middle aged, nondemented, and demented older adults,” Journal of cognitive neuroscience, vol. 19 , no. 9 , pp. 14981507, 2007.
[56] Jenkinson M , Beckmann CF , Behrens TE , Woolrich MW , and Smith SM , “Fsl,” Neuroimage, vol. 62 , no. 2 , pp. 782–790, 2012.21979382
[57] Destrieux C , Fischl B , Dale A , and Halgren E , “Automatic parcellation of human cortical gyri and sulci using standard anatomical nomenclature,” Neuroimage, vol. 53 , no. 1 , pp. 1–15, 2010.20547229
[58] Tristán-Vega A and Aja-Fernández S , “Dwi filtering using joint information for dti and hardi,” Medical image analysis, vol. 14 , no. 2 , pp. 205–218, 2010.20005152
[59] Duchi J , Hazan E , and Singer Y , “Adaptive subgradient methods for online learning and stochastic optimization.” Journal of machine learning research, vol. 12 , no. 7 , 2011.
[60] Kipf TN and Welling M , “Semi-supervised classification with graph convolutional networks,” arXiv preprint arXiv:1609.02907, 2016.
[61] Chen M , Wei Z , Huang Z , Ding B , and Li Y , “Simple and deep graph convolutional networks,” in International conference on machine learning. PMLR, 2020, pp. 1725–1735.
[62] Li G , Muller M , Thabet A , and Ghanem B , “Deepgens: Can gens go as deep as cnns?” in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 9267–9276.
[63] Lee WJ , Brown JA , Kim HR , La Joie R , Cho H , Lyoo CH , Rabinovici GD , Seong J-K , Seeley WW , Initiative ADN , “Regional a β-tau interactions promote onset and acceleration of alzheimer’s disease tau spreading,” Neuron, 2022.
[64] LaMontagne PJ , Benzinger TL , Morris JC , Keefe S , Hornbeck R , Xiong C , Grant E , Hassenstab J , Moulder K , Vlassenko AG , “Oasis-3: longitudinal neuroimaging, clinical, and cognitive dataset for normal aging and alzheimer disease,” MedRxiv, pp. 2019–12, 2019.
[65] Jovicich J , Barkhof F , Babiloni C , Herholz K , Mulert C , van Berckel BN , Frisoni GB , and Group S-NJW , “Harmonization of neuroimaging biomarkers for neurodegenerative diseases: A survey in the imaging community of perceived barriers and suggested actions,” Alzheimer’s &amp; Dementia: Diagnosis, Assessment &amp; Disease Monitoring, vol. 11 , no. 1 , pp. 69–73, 2019.
[66] Byers R , “Solving the algebraic riccati equation with the matrix sign function,” Linear Algebra and its Applications, vol. 85 , pp. 267–279, 1987.
