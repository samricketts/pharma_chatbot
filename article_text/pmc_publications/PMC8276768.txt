LICENSE: This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.


101661793
44036
Stat (Int Stat Inst)
Stat (Int Stat Inst)
Stat (International Statistical Institute)
2049-1573

34262756
8276768
10.1002/sta4.245
NIHMS1041480
Article
Fast Covariance Estimation for Multivariate Sparse Functional Data
Li Cai *1
Xiao Luo 1
Luo Sheng 2
1 Department of Statistics, North Carolina State Univerisy, NC, USA
2 Department of Biostatistics and Bioinformatics, Duke Universitye, NC, USA
Present Address

North Carolina State Univerisy

* Correspondence: Cai Li, North Carolina State Univerisy. cli9@ncsu.edu
23 7 2019
17 6 2020
2020
13 7 2021
9 1 e245This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.

Covariance estimation is essential yet underdeveloped for analyzing multivariate functional data. We propose a fast covariance estimation method for multivariate sparse functional data using bivariate penalized splines. The tensor-product B-spline formulation of the proposed method enables a simple spectral decomposition of the associated covariance operator and explicit expressions of the resulting eigenfunctions as linear combinations of B-spline bases, thereby dramatically facilitating subsequent principal component analysis. We derive a fast algorithm for selecting the smoothing parameters in covariance smoothing using leave-one-subject-out cross-validation. The method is evaluated with extensive numerical studies and applied to an Alzheimer’s disease study with multiple longitudinal outcomes.

Bivariate smoothing
Covariance function
Functional principal component analysis
Longitudinal data
Multivariate functional data
Prediction

1 INTRODUCTION

Functional data analysis (FDA) has been enjoying great successes in many applied fields, e.g., neuroimaging (Goldsmith, Crainiceanu, Caffo, &amp; Reich 2012; Lindquist 2012; Reiss &amp; Ogden 2010; Zhu, Li, &amp; Kong 2012), genetics (Leng &amp; Müller 2006; Reimherr &amp; Nicolae 2014 2016), and wearable computing (Morris et al. 2006; Xiao et al. 2015). Functional principal component analysis (FPCA) conducts dimension reduction on the inherently infinite-dimensional functional data, and thus facilitates subsequent modeling and analysis. Traditionally, functional data are densely observed on a common grid and can be easily connected to multivariate data, although the notion of smoothness distinguishes the former from the latter. In recent years, covariance-based FPCA (Yao, Müller, &amp; Wang 2005) has become a standard approach and has greatly expanded the applicability of functional data methods to irregularly spaced data such as longitudinal data. Various nonparametric methods have now been proposed to estimate the smooth covariance function, e.g., Peng and Paul (2009), Cai and Yuan (2010), Goldsmith et al. (2012), Xiao, Li, Checkley, and Crainiceanu (2018) and Wong and Zhang (2019).

There has been growing interest in multivariate functional data where multiple functions are observed for each subject. For dense functional data, Ramsay and Silverman (2005, Chapter 8.5) proposed to concatenate multivariate functional data as a single vector and conduct multivariate PCA on the long vectors and Berrendero, Justel, and Svarc (2011) repeatedly applied point-wise univariate PCA. For sparse and paired functional data, L. Zhou, Huang, and Carroll (2008) extended the low-rank mixed effects model in James, Hastie, and Sugar (2000). Chiou, Chen, and Yang (2014) considered normalized multivariate FPCA through standardizing the covariance operator. Petersen and Müller (2016) proposed various metrics for studying cross-covariance between multivariate functional data. More recently, Happ and Greven (2018) introduced a FPCA framework for multivariate functional data defined on different domains.

The interest of the paper is functional principal component analysis for multivariate sparse functional data, where multiple responses are observed at time points that vary from subjects to subjects and may even vary between responses within subjects. There are much fewer works to handle such data. The approach in L. Zhou et al. (2008) focuses on bivariate functional data and can be extended to more than two-dimensional functional data, although model selection (e.g., selection of smoothing parameters) can be computationally difficult and convergence of the expectation-maximization estimation algorithm could also be an issue. The local polynomial method in Chiou et al. (2014) can be applied to multivariate sparse functional data, although a major drawback is the selection of multiple bandwidths. Moreover, because the local polynomial method is a local approach, there is no guarantee that the resulting estimates of covariance functions will lead to a properly defined covariance operator. The approach in Happ and Greven (2018) (denoted by mFPCA hereafter) estimates cross-covariances via scores from univariate FPCA and hence can be applied to multivariate sparse functional data. While mFPCA is theoretically sound for dense functional data, it may not capture cross-correlations between functions because scores from univariate FPCA for sparse functional data are shrunk towards zero.

We propose a novel and fast covariance-based FPCA method for multivariate sparse functional data. Note that multiple auto-covariance functions for within-function correlations and cross-covariance functions for between-function correlations have to be estimated. Tensor-product B-splines are employed to approximate the covariance functions and a smoothness penalty as in bivariate penalized splines (Eilers &amp; Marx 2003) is adopted to avoid overfit. Then the individual estimates of covariance functions will be pooled and refined. The advantages of the new method are multifold. First, the tensor-product B-spline formulation is computationally efficient to handle multivariate sparse functional data. Second, a fast fitting algorithm for selecting the smoothing parameters will be derived, which alleviates the computational burden of conducting leave-one-subject-out cross-validation. Third, the tensor-product B-spline representation of the covariance functions enables a straightforward spectral decomposition of the covariance operator for the multivariate functional data; see Proposition 1. In particular, the eigenfunctions associated with the covariance operator are explicit functions of the B-spline bases. Last but not the least, via a simple truncation step, the refined estimates of the covariance functions lead to a properly defined covariance operator.

Compared to mFPCA, the proposed method does not rely on scores from univariate FPCA, which could be a severe problem for sparse functional data, and hence could better capture the correlations between functions. And an improved correlation estimation will lead to improved subsequent FPCA analysis and curve prediction. The proposed method also compares favorably with the local polynomial method in Chiou et al. (2014) because of the computationally efficient tensor-product spline formulation of the covariance functions and the derived fast algorithm for selecting the smoothing parameters. Moreover, as mentioned above, there exists an explicit and easy-to-calculate relationship between the tensor-product spline representation of covariance functions and the associated eigenfunctions/eigenvalues, which greatly facilitates subsequent FPCA analysis.

In addition to FPCA, there are also abundant literatures on models for multivariate functional data with most focusing on dense functional data. For clustering of multivariate functional data, see Huang, Li, and Guan (2014); Jacques and Preda (2014); Zhu, Brown, and Morris (2012) and Park and Ahn (2017). For regression with multivariate functional responses, see Kowal, Matteson, and Ruppert (2017); J. Li, Huang, Zhu, and Initiative (2017); Luo and Qi (2017); Wong, Li, and Zhu (2019); Zhu, Li, and Kong (2012); Zhu, Morris, Wei, and Cox (2017) and Qi and Luo (2018). Graphical models for multivariate functional data are studied in Zhu, Strawn, and Dunson (2016) and Qiao, Guo, and James (2019). Works on multivariate functional data include also Chiou and Müller (2014 2016).

The remainder of the paper proceeds as follows. In Section 2, we present our proposed method. We conduct extensive simulation studies in Section 3 and apply the proposed method to an Alzheimer’s disease study in Section 4. A discussion is given in Section 5. All technical details are enclosed in the Appendix.

2 METHODS

2.1 Fundamentals of Multivariate Functional Principal Component Analysis

Let p be a positive integer and denote by T a continuous and bounded domain in the real line ℝ. Consider the Hilbert space H:L2(T)×…×L2(T)︸p: equipped with the inner product &lt;⋅,⋅&gt;H and norm ‖⋅‖H such that for arbitrary functions f = (f(1), …, f(p))⊤ and g = (g(1), …, g(p))⊤ in H with each element in L2(T),&lt;f, g&gt;H=∑k=1p∫f(k)(t)g(k)(t)dt and ‖f‖H=&lt;f,f&gt;H1/2. Let {x(k)}k=1,…,p be a set of p random functions with each function in L2(T). Assume that the p-dimensional vector x(t)=(x(1),…,x(p))⊤∈ℝp has a p-dimensional smooth mean function, μ(t)=E{x(t)}=(E{x(1)(t)},…,E{x(p)(t)})⊤=(μ(1)(t),…,μ(p)(t))⊤. Define the covariance function as C(s,t)=E{(x(s)−μ(s))(x(t)−μ(t))⊤}=[Ckk′(s,t)]1≤k,k′≤p and Ckk′(s,t)=Cov{x(k)(s),x(k′)(t)}. Then the covariance operator Γ:H→H associated with the kernel C(s, t) can be defined such that for any f∈H, the kth element of Γf is given by (Γf)(k)(s)=&lt;Ck(s,⋅),f&gt;H=∑k′=1p∫Ckk′(s,t)f(k′)(t)dt,

where Ck(s, t) = (Ck1(s, t), …, Ckp(s, t))⊤. Note that Γ is a linear, self-adjoint, compact and non-negative integral operator. By the Hilbert-Schmidt theorem, there exists a set of orthonormal bases {Ψl}l≥1∈H,Ψl=(Ψl(1),…,Ψl(p))⊤, and &lt;Ψl,Ψl′&gt;H=∑k=1p∫Ψl(k)(t)Ψl′(k)(t)dt=1{l=l′}, that (1) (Γψl)(k)(s)=∑k′=1p∫Ckk′(s,t)ψl(k′)(t)dt=dlψl(k)(s),

where dℓ is the ℓth largest eigenvalue corresponding to ψℓ. Then the multivariate Mercer’s theorem gives (2) C(s,t)=∑l∞dlψl(s)ψl⊤(t),

where Ckk′(s,t)=∑l=1∞∑l=1∞dlΨl(k)(s)Ψl(k′)(t). As shown in Saporta (1981), x(t) has the multivariate Karhunen-Loève representation, x(t)=μ(t)+∑l=1∞ξlψl(t), where ξl=&lt;x−μ,Ψl&gt;H are the scores with E(ξl)=0 and E(ξlξl′)=dl1{l=l′}. The covariance operator Γ has the positive semi-definiteness property, i.e, for any a=(a1,…,ap)⊤∈ℝp, the covariance function of a⊤x, denoted by Ca(s, t), satisfies that for any sets of time points (t1,…,tq)⊂T with an arbitrary positive integer q, the square matrix [Ca(ti,tj)]{1≤i,j≤q}∈ℝq×q is positive semi-definite.

2.2 Covariance Estimation by Bivariate Penalized Splines

Suppose that the observed data take the form {(yij(k),tij(k)):i=1,…,n;k=1,…,p;j=1,…,mik}, where tij(k)∈T is the observed time point, yij(k) is the observed kth response, n is the number of subjects, and mik is the number of observations for subject i’s kth response. The model is (3) yij(k)=xi(k)(tij(k))+ϵij(k)=μ(k)(tij(k))+∑l=1∞ξilψl(k)(tij(k))+ϵij(k),

where xi(t)=(xi(1)(t),…,xi(p)(t))⊤∈H,ϵij(k) are random noises with zero means and variances σk2 and are independent across i, j and k.

The goal is to estimate the covariance functions Ckk′. We adopt a three-step procedure. In the first step, empirical estimates of the covariance functions are constructed. Let rij(k)=yij(k)-μ(k)(tij(k)) be the residuals and Cij1j2(kk′)=rij1(k)rij2(k′) be the auxiliary variables. Note that E(Cij1j2(kk′))=Ckk′(tij1(k),tij2(k′))+σk21{k=k′,j1=j2} for 1 ≤ j1 ≤ mik, 1 ≤ j2 ≤ mik′. Thus, Cij1j2(kk′) is an unbiased estimate of Ckk′(tij1(k),tij2(k′)) whenever k ≠ k′ or j1 ≠ j2. In the second step, the noisy auxiliary variables are smoothed to obtain smooth estimates of the covariance functions. For smoothing, we use bivariate P-splines (Eilers &amp; Marx 2003) because it is an automatic smoother and is computationally simple. In the final step, we pool all estimates of the individual covariance functions and use an extra step of eigendecomposition to obtain refined estimates of covariance functions. The refined estimates lead to a covariance operator that is properly defined, i.e., positive semi-definite. In practice, the mean functions μ(k)s are unknown and we estimate them using P-splines (Eilers &amp; Marx 1996) with the smoothing parameters selected by leave-one-subject-out cross validation; see the supplement for details. Denote the estimates by μ^(k). Let r^ij(k)=yij(k)-μ^(k)(tij(k)) and C^ij1j2(kk′)=r^ij1(k)r^ij2(k′), the actual auxiliary variables.

The bivariate P-splines model Ckk′(s, t) uses tensor-product splines Gkk′(s, t) for 1 ≤ k, k′ ≤ p. Specifically, Gkk′(s,t)=∑1≤γ1,γ2≤cθγ1γ2(kk′)Bγ1(s)Bγ2(t), where Θkk′=[θγ1γ2(kk′)]1≤γ1,γ2≤c∈ℝc×c is a coefficient matrix, {B1(·), …, Bc(·)} is the collection of B-spline basis functions in T, and c is the number of equally-spaced interior knots plus the order (degree plus 1) of the B-splines. Because Ckk′(s,t)=Ck′k(t,s)= Cov {x(k)(s),x(k′)(t)}, it is reasonable to impose the assumption that Θkk′=Θk′k⊤

so that Gkk′(s; t) = Gk′k(t; s). Therefore, in the rest of the section, we consider only k ≤ k′.

Let D∈ℝ(c−2)×c denote a second-order differencing matrix such that for a vector a=(a1,…,ac)⊤∈ℝc, Da=(a3−2a2+a1,a4−2a3+a2,…,ac−2ac−1+ac−2)⊤∈ℝc−2. Also let ǁ · ǁF be the Frobenius norm. For the cross-covariance function Ckk′ with k &lt; k′, the bivariate P-splines estimate the coefficient matrix Θkk’ by Θkk’ which minimizes the penalized least squares (4) ∑i=1n∑1≤j1≤mik∑1≤j2≤mik′{Gkk′(tij1(k),tij2(k′))−C^ij1j2(kk′)}2+λkk′1‖DΘkk′‖F2+λkk′2‖DΘkk′⊤‖F2,

where λkk′1 and λkk′2 are two nonnegative smoothing parameters that balance the model fit and smoothness of the estimate and will be determined later. Indeed, the column penalty ‖DΘkk′‖F2 penalizes the 2nd order consecutive differences of the columns of Θkk’ and similarly, the row penalty ‖DΘkk′⊤‖F2 penalizes the 2nd order consecutive differences of the rows of Θkk’. The two penalty terms are essentially penalizing the 2nd order partial derivatives of Gkk′(s; t) along the s and t directions, respectively. The two smoothing parameters are allowed to differ to accommodate different levels of smoothing along the two directions.

For the auto-covariance functions Ckk(s; t) with k = 1; …; p, we conduct bivariate covariance smoothing by enforcing the following constraint on the coefficient matrix Θkk (Xiao et al. 2018), (5) Θkk=Θkk⊤.

It follows that Gkk(s; t) is a symmetric function. Then the coefficient matrix Θkk and the error variance σk2 are jointly estimated by Θ^kk and σ^k2, which minimize the penalized least squares (6) ∑i=1n∑1≤j1,≤j2≤mik{Gkk(tij1(k),tij2(k))+σk21{j1=j2}−C^ij1j2(kk)}2+λk‖DΘkk‖F2,

over all symmetric Θkk and λk is a smoothing parameter. Note that the two penalty terms in (4) become the same when Θkk is symmetric and thus only one smoothing parameter is needed for auto-covariance estimation.

2.2.1 Estimation

We first introduce the notation. Let vec(·) be an operator that stacks the columns of a matrix into a column vector and denote by ⊗ the Kronecker product. Fix k and k′ with k ≤ k′. Let θkk′=vec(Θkk′)∈ℝc2 be a vector of the coefficients and b(t)={B1(t),…,Bc(t)}⊤∈ℝc denotes the B-spline base. Then Gkk′(s,t)=b(s)⊤Θkk′b(t)={b(t)⊗b(s)}⊤θkk′.

We now organize the auxiliary responses C^ij1j2(kk′) for each pair of k and k′. Let ri(k)=(ri1(k),…,rimik(k))⊤∈ℝmik,C^i(kk′)=ri(k)⊗ri(k′)∈ℝmikmik′ and C^(kk′)=(C^1(kk′),⊤,…,C^n(k′),T)⊤∈ℝNkk', where Nkk′=∑i=1nmikmik′ is the total number of auxiliary responses for the pair of k and k′. As for the B-splines, let bi(k)=[b(ti1(k)),…,b(timik(k))]∈ℝc×mik, Bi(kk′)=(bi(k′)⊗bi(k))⊤∈ℝ(mikmik')×c2, and B(kk′)=[B1(kk′),⊤,…,Bn(kk′),⊤]⊤∈ℝNkk×c2.

For estimation of the cross-covariance functions Ckk′ with k &lt; k′, the penalized least squares in (4) can be rewritten as (7) (C^(kk′)−B(kk′)θkk′)⊤(C^(kk′)−B(kk′)θkk′)+λkk′1θkk′⊤P1θkk′+λkk′2θkk′⊤P2θkk′,

where P1 = Ic ⊗ D⊤D and P2 = D⊤D ⊗ Ic. The expression in (7) is a quadratic function of the coefficient vector θkk′. Therefore, we derive that θ^kk′=(B(kk′),⊤B(kk′)+λkk′1P1+λkk′2P2)−1B(kk′),⊤C^(kk′)

and the estimate of the cross-covariance function Ckk′(s, t) is C^kk′(s,t)={b(t)⊗b(s)}⊤θ^kk′.

For estimation of the auto-covariance functions, because of the constraint on the coefficient matrix in (5), let ηk∈ℝc(c+1)/2 be a vector obtained by stacking the columns of the lower triangle of Θkk and let GC∈ℝc2×c(c+1)/2 be a duplication matrix such that θkk = Gcηk (Page 246, Seber 2007). Let Zi(k)=vec(Imik)∈ℝmik2 and Z(k)=(Z1(k),⊤,…,Zn(k),⊤)⊤∈ℝNkk. Finally let βk=(ηk⊤,σk2)⊤∈ℝc˜ with c˜=c(c+1)/2+1. It follows that the penalized least squares in (6) can be rewritten as (C^(kk)−X(k)βk)⊤(C^(kk)−X(k)βk)+λkβk⊤Qβk,

where X(k)=[B(kk),Z(k)]∈ℝNkk×c˜ and Q= blockdiag {Gc⊤(Ic⊗DD⊤)Gc⊤,0}∈ℝc˜×c˜. Therefore, we obtain β^k=(η^k⊤,σ^k2)⊤=(X(k),⊤X(k)+λkQ)−1X(k),⊤C^(kk).

It follows that θ^kk=Gcη^k and the estimate of the auto-covariance function Ckk (s, t) is C^kk(s,t)={b(t)⊗b(s)}⊤θ^kk.

The above estimates of covariance functions may not lead to a positive semi-definite covariance operator and thus have to be refined. We pool all estimates together and we shall use the following proposition.

Proposition 1. Assume that Ckk′ (s, t) = b(s)⊤Θkk′b(t). Let G=∫b(t)b(t)⊤dt∈ℝc×c and assume that G is positive definite (S. Zhou, Shen, Wolfe, et al. 1998). Then [G12Θkk′G12]1≤k,k′≤p∈ℝpc×pc admits the spectral decomposition, ∑l=1∞dlulul⊤ where dℓ is the ℓth largest eigenvalue of the covariance operator Γ, and ul={ul(1),⊤,…,ul(p),⊤}⊤∈ℝpc is the associated eigenvector with ul(k)∈ℝc and such that Ψl(k)(t)=b(t)⊤G−12ul(k).

The proof is provided in Appendix A. Proposition 1 implies that, with the tensor-product B-spline representation of the covariance functions, one spectral decomposition gives us the eigenvalues and eigenfunctions. In particular, the eigenfunctions Ψl(k)(t) are linear combinations of the B-spline basis functions, which means that they can be straightforwardly evaluated, an advantage of spline-based methods compared to other smoothing methods for which eigenfunctions are approximated by spectral decompositions of the covariance functions evaluated at a grid of time points.

Once we have Θ^kk′, the estimate of the coefficient matrix Θkk′ the spectral decomposition of [G12Θ^kk′G12]kk′ gives us estimates d^l and u^l={u^l(1),⊤,…,u^l(p),⊤}⊤. We discard negative d^l to ensure that the multivariate covariance operator is positive semi-definite and this leads to a refined estimate of the coefficient matrix Θkk′, Θ˜kk′=G−12{∑l:d^l&gt;0d^lu^l(k)u^l(k′),⊤}G−12. Then the refined estimate of the covariance functions is C˜kk′(s,t)=b(s)⊤ Θ˜kk′b(t). Proposition 1 also suggests that the eigenfunctions can be estimated by Ψ˜l(k)(t)=b(t)⊤G−12u^l(k).

For principal component analysis or curve prediction in practice, one may select further the number of principal components by either the proportion of variance explained (PVE) (Greven, Crainiceanu, Caffo, &amp; Reich 2010) or an AIC-type criterion (Y. Li, Wang, &amp; Carroll 2013). Here, we follow Greven et al. (2010) using PVE with a value of 0.99.

2.2.2 | Selection of Smoothing Parameters

We select the smoothing parameters in each auto-covariance/cross-covariance estimation using leave-one-subject-out cross-validation; see, e.g., Yao et al. (2005) and Xiao et al. (2018). A fast approximate algorithm for the auto-covariance has been derived in Xiao et al. (2018). So we focus on the cross-covariance and use the notation in (7). Note that there are two smoothing parameters for each cross-covariance estimation.

For simplicity, we suppress the superscript and subscript kk′ in (7) for both Ĉ and B. Let C˜i[i] be the prediction of the auxiliary responses C^i from the estimate using data without the ith subject. Let ǁ · ǁ be the Euclidean norm and the cross-validation error is (8) iCV=∑i=1n‖C^i− C˜i[i]‖2.

We shall also now suppress the subscript k from mik and kk′ from Nkk′ Let S=B(B⊤B+λ1P1+λ2P2)−1B⊤∈ℝN×N, Si=Bi(B⊤B+λ1P1+λ2P2)−1B⊤∈ℝmi2×N, and Sii=Bi(B⊤B+λ1P1+λ2P2)−1Bi⊤∈ℝmi2×mi2. Then a short-cut formula for (8) is iCV=∑i=1n‖(Imi2−Sii)−1(SiC^−C^i)‖2.

Similar to Xu and Huang (2012) and Xiao et al. (2018), the iCV can be further simplified by adopting the approximation (Imi2−Sii)−2=Imi2+2Sii, which results in the generalized cross validation, denoted by iGCV, (9) iGCV=‖C^−SC^‖2+2∑i=1n(SiC^−C^i)⊤Sii(SiC^−C^i).

While iGCV is much easier to compute than iCV, the formula in (9) is still computationally expensive to compute. Indeed, the smoother matrix S is of dimension 2500 × 2500 if n = 100 and mi = m = 5 for all i. Thus, we need to further simplify the formula.

Let Gn=B⊤B, B˜=BGn−1/2∈ℝN×c2, B˜i=BiGn−1/2∈ℝmi2×c2, f= B˜⊤C^∈ℝc2, fi=B˜i⊤C^i∈ℝc2, and Li= B˜i⊤ B˜i∈ℝc2×c2. Also let P˜1=Gn−1/2P1Gn−1/2∈ℝc2×c2, P˜2=Gn−1/2P2Gn−1/2∈ℝc2×c2, and Σ=Ic2+λ1 P˜1+λ2 P˜2. Then (9) can be simplified as (10) iGCV=‖C^‖2−2f⊤Σ−1f+f⊤Σ−2f+2∑i=1n(LiΣ−1f−fi)⊤Σ−1(LiΣ−1f−fi).

Note that Σ has two smoothing parameters. Following Wood (2000), we use an equivalent parameterization Σ=I+ρ{w P˜1+(1−w) P˜2}, where ρ = λ1 + λ2 represents the overall smoothing level and w = λ1ρ−1 ∈ [0; 1] is the relative weight of λ1. We conduct a two-dimensional grid search of (ρ, w) as follows. For a given w, let Udiag(s)U⊤ be the eigendecompsition of w P˜1+(1−w) P˜2, where U∈ℝc2×c is an orthonormal matrix and s=(s1,…,sc2)∈ℝc2 is the vector of eigenvalues. Then Σ−1=Udiag(d˜)U⊤ with d˜=1/(1+ρs)∈ℝc2.

Proposition 2. Let ⊙ stand for the point-wise multiplication. Then, iGCV=‖C^‖2+(f˜⊙ d˜)⊤(f˜⊙ d˜)−2d˜⊤g−4d˜⊤Fd˜+2d˜⊤[∑i=1n{L˜i(f˜⊙ d˜)}⊙{L˜i(f˜⊙ d˜)}]

where fi˜=U⊤fi∈ℝc2, f˜=U⊤f∈ℝc2, g=f˜⊙f˜−∑i=1nf˜i⊙ f˜i∈ℝc2, L˜i=U⊤LiU∈ℝc2×c2, and F=∑i=1n( f˜if˜⊤)⊙L˜i∈ℝc2×c2. The proof is provided in Appendix A. For each w, note that only d˜ depends on ρ and needs to be calculated repeatedly, and all other terms need to be calculated only once. The entire algorithm is presented in Algorithm 1. We give an evaluation of the complexity of the proposed algorithm. Assume that mi = m for all i. The first initialization (step 1) requires O(nm2c2 + nc4 + c6) computations. For each w, the second initialization (step 2) also requires O{nc4 min(m2; c2) + c6} computations. For each ρ, steps 3–8 requires O(nc4) computations. Therefore, the formula in Proposition 2 is most efficient to calculate for sparse data with small numbers of observations per subject, i.e., mis are small.

2.3 Prediction

For prediction, assume that the smooth curve xi(t) is generated from a multivariate Gaussian process. Suppose that we want to predict the ith multivariate response xi(t) at {si1, …, sim} for m ≥ 1. Let yi(k)=(yi1(k),…,yimik(k))⊤ be the vector of observations at {ti1(k),…,timik(k)} for the kth response. Let μi(k),∘=(μ(k)(ti1(k)),…,μ(k)(timik(k)))⊤ be the vector of the kth mean function at the observed time points. Let yi=(yi(1),⊤,…,yi(p),⊤)⊤ and μi∘=(μi(1),∘,⊤,…,μi(p),∘,⊤)⊤. Let μin=(μ(1)(si1),…,μ(1)(sim),⋯,μ(p)(si1),…,μ(p)(sim))⊤ be the vector of mean functions at the time points for prediction. It follows that (yixi)~N{(μi°μin),(Cov(yi)Cov(xi,yi)⊤Cov(xi,yi)Cov(xi))}.

Thus, we obtain E(xi|yi)=Cov(xi,yi)Cov(yi)−1(yi−μio)+μin,

Cov(xi|yi)=Cov(xi)−Cov(xi,yi)Cov(yi)−1Cov(xi,yi)⊤,

Let bi(k),∘=[b(ti1(k)),…,b(timik(k))]⊤ and bin=[b(si1),…,b(sim)]⊤. Next let Bi°= blockdiag (bi(1),∘,…,bi(p),∘) and Bin=Ip⊗bin. Then Cov(yi) given by Bi°ΘBi°,⊤+blockdiag(σ12Imi1,…,σp2Imip), and Cov(xi) and Cov(yi, xi) are given by BinΘBin,⊤ and Bi∘ΘBin,⊤, respectively. Let Θ˜=[Θ˜kk′]1≤k,k′≤p∈ℝpc×pc. Plugging in the estimates, we predict xi by x^i=(x^i(1)(si1),…,x^i(1)(sim),⋯,x^i(p)(si1),…,x^i(p)(sim))⊤      =(BinΘ˜Bi°,⊤)V^i−1(yi−μ^io)+μ^in,

where μ^i0=((μ^(1)(ti1(1)),…,μ^(1)(timii(1)),⋯,μ^(p)(ti1(p)),…,μ^(p))(timip(p)))⊤ is the estimate of μi°, μ^in=(μ^(1)(si1),…,μ^(1)(sim),⋯,μ^(p)(si1),…,μ^(p)(sim))⊤ is the estimate of μin,V^i=Bi°Θ˜Bi∘+blockdiag (σ^12Imii1,…,σ^p2Imip). An approximate covariance matrix for x^i is Cov^(xi|yi)=BinΘ˜Bin,⊤−(BinΘ˜Bio,⊤)V^i−1(BinΘ˜Bio,⊤)⊤.

Therefore, a 95%point-wise confidence interval for the kth response is given by x^i(k)(sij)±1.96Var^(xi(k)(sij)|yi),

where Var^(xi(k)(sij)|yi) can be extracted from the diagonal of Cov^(xi|yi).

Finally, we predict the first L ≥ 1 scores ξi = (ξi1, …, ξiL)T for the ith subject. Note that ξiℓ = ∫ ψℓ (t)⊤ {xi(t) − μ(t)}dt. With a similar derivation as above, xi(t) – μ(t) can be predicted by {Ip⊗b(t)}⊤Θ˜Bi°,⊤V^i−1(yi−μi°). By Proposition 1, the eigenfunctions ψl(k)(t) are estimated by b(t)⊤G−12u^l(k) and thus ψl(t)⊤=u^l⊤{Ip⊗G−12b(t)}. it follows that ξ^il=u^l⊤(Ip⊗G12) Θ˜Bio,⊤V^i−1(yi−μ^io).

3 SIMULATIONS

We evaluate the finite sample performance of the proposed method (denoted by mFACEs) against mFPCA via a synthetic simulation study and a simulation study mimicking the ADNI data in the real data example. Here, we report the details and results of the former as the conclusions remain the same for the latter and details are provided in the supplement.

3.1 Simulation Settings and Evaluation Criterias

We generate data by model (3) with p = 3 responses. The mean functions are μ(t) = [5 sin(2πt), 5 cos(2πt), 5(t − 1)2]T. We first specify the auto-covariance functions. Let Φ1(t)=[2sin(2πt),2cos(4πt),2sin(4πt)]⊤, Φ2(t)=[2cos(πt),2cos(2πt),2cos(3πt)]⊤, and Φ3(t)=[2sin(πt),2sin(2πt),2sin(3πt)]⊤. Also let Λ11=(30001.50000.75),Λ22=(3.50001.750000.5),Λ33=(2.500020001).

Then the auto-covariance functions are Ckk(s,t) = Φk(s)⊤ΛkkΦk(t),k = 1,2,3. For the cross-covariance functions, let Ckk′(s,t)=ρΦk(s)⊤Λkk12Λk′k′12,Φk′(t), for k ≠ k′, where ρ ∈ [0, 1] is a parameter to be specified. The induced covariance operator from the above specifications is proper; see Lemma 1 in Appendix B. It is easy to derive that the absolute value of cross-correlation ρkk′(s,t)=Ckk′(s,t)/Ckk(s,s)Ck′k′(t,t) is bounded by ρ. Hence, ρ controls the overall level of correlation between responses: if ρ = 0, then the responses are uncorrelated from each other. The eigendecomposition of the multivariate covariance function gives 9 non-zero eigenvalues with associated multivariate eigenfunctions, hence, for ℓ = 1, … 9, we simulate the scores ξiℓ from N(0,dl), where dl are the induced eigenvalues. Next, we simulate the white noises ϵij(k) from N(0,σϵ2), where σϵ2 is determined according to the signal-to-noise ratio SNR=∑ldl/(pσϵ2). Here, we let SNR = 2. For each response, the sampling time points are drawn from a uniform distribution in the unit interval and the number of observations for each subject, mik, is generated from a uniform discrete distribution on {3, 4, 5, 6, 7}. Thus, the sampling points not only vary from subject to subject but also vary across responses within each subject.

We use a factorial design with two factors: the number of subjects n and the correlation parameter ρ. We let n = 100; 200 or 400. We let ρ = 0:5, which corresponds to a weak correlation between responses as the average absolute correlations between responses is only 0:36. Another value of ρ is 0.9, which corresponds to a moderate correlation between responses as the average absolute correlations between responses is about 0:50. In total, we have 6 model conditions and for each model condition we generate 200 datasets. To evaluate the prediction accuracy of the various methods, we draw 200 additional subjects as testing data. The true correlation functions and a sample of the simulated data are shown in the supplement.

We compare mFACEs and mFPCA in terms of estimation accuracy of the covariance functions, the eigenfunctions and eigenvalues, and prediction of new subjects. For covariance function estimation, we use the relative integrated square errors (RISE). Let C^kk′(s,t) be an estimate of Ckk′ (s; t), then RISE are given by ∑k=1p∑k′=1p∫01∫01{Ckk′(s,t)−C^kk′(s,t)}2dsdt∑k=1p∑k′=1p∫01∫01{Ckk′(s,t)}2dsdt.

For estimating the ℓth eigenfunction, we use the integrated square errors (ISE), which are defined as min[∑k=1p∫01{Ψl(k)(t)−Ψ^l(k)(t)}2dt,∑k=1p∫01{Ψl(k)(t)+Ψ^l(k)(t)}2dt].

Note that the range of ISE is [0; 2]. For estimating the eigenvalues, we use the ratio of the estimate against the truth, i.e., d^l/dl. For predicting new curves, we use the mean integrated square errors (MISE), which are given by 1200p∑k=1p∑i=1200[∫01{xi(k)(t)−x^i(k)(t)}2dt].

For the curve prediction using mFPCA, we truncate the number of principal components using a PVE of 0.99. It is worth noting that if no truncation is adopted, then the curve prediction using mFPCA reduces to curve prediction using univariate FPCA. We shall also consider the conditional expectation method based on the estimates of covariance functions from mFPCA. The method is denoted by mFPCA(CE) and its difference with mFACEs is that different estimates of covariance functions are used.

3.2 Simulation Results

Figure 1 gives boxplots of RISEs of mFACEs and mFPCA for estimating covariance functions. Under all model conditions, mFACEs outperforms mFPCA and the improvement in RISEs as the sample size increases is much more pronounced for mFACEs. Under the model conditions with moderate correlations (ρ = 0.9), the advantage of mFACEs is substantial even for the small sample size n = 100.

Figures 2 and 3 give boxplots of ISEs and violin plots of mFACEs and mFPCA for estimating the top two eigenfunctions and eigenvalues, respectively. The top two eigenvalues account for about 60% of the total variation in the functional data for ρ = 0.5 and it is 80% for ρ = 0.9. Figure 2 shows that while the two methods are overall comparable for estimating the 1st eigenfunction, mFACEs has a much better accuracy for estimating the second eigenfunction than mFPCA. The violin plots in Figure 3 show that mFACEs outperforms mFPCA substantially for estimating both eigen-values under all model conditions. The mFPCA always underestimates the eigenvalues as the variation of scores from univariate FPCA is smaller than the true variation and hence leads to underestimates of eigenvalues.

Finally, we consider the prediction of new subjects by mFACEs, mFPCA and mFPCA(CE). We define the relative efficiencies of different methods as the ratios of MISEs with respect to that of univariate FPCA; see Figure 4. Univariate FPCA is implemented in the R package face (Xiao, Li, Check-ley, &amp; Crainiceanu 2017). We have the following findings. Under all model conditions, mFACEs has the smallest MISE, mFPCA(CE) has the second best performance, and mFPCA is close to univariate FPCA. Thus, on average mFACEs provides the most accurate curve prediction. These results indicate that: 1) mFACEs has better covariance estimation than mFPCA(CE), and so is the prediction based on it; 2) compared to mFPCA/univariate FPCA, mFPCA(CE) exploits the correlation information and hence results in better predictions.

In summary, mFACEs shows competing performance against alternative methods.

4 APPLICATION TO ALZHEIMER’S DISEASE STUDY

The Alzheimer’s Disease Neuroimaging Initiative (ADNI) is a two-stage longitudinal observational study launched in year 2003 with the primary goal of investigating whether serial neuroimags, biological markers, clinical and neuropsychological assessments can be combined to measure the progression of Alzheimer’s disease (AD) (Weiner et al. 2017). The ADNI-1 data from the first stage contain 379 patients with amnestic mild cognitive impairment (MCI, a risk state for AD) at baseline who had at least one follow-up visit. Participants were assessed at baseline, 6, 12, 18, 24, and 36 months with additional annual follow-ups included in the second stage of the study. At each visit, various neuropsychological assessments, clinical measures, and brain images were collected. The ADNI-2 data include 424 additional patients suffering from MCI and significant memory concern, with at least one follow-up visit and longitudinal data collected over four years. Thus, for the combined data, the total number of subjects is 803, and the average number of visits is 4.72. The data are publicly available at http://ida.loni.ucla.edu/.

We consider five longitudinal markers commonly measured in studies of AD with strong comparative predictive value (K. Li, Chan, Doody, Quinn, &amp; Luo 2017). Among the five markers, Disease Assessment Scale-Cognitive 13 items (ADAS-Cog 13), Rey Auditory Verbal Learning Test immediate recall (RAVLT.imme), Rey Auditory Verbal Learning Test learning curve (RAVLT.learn), and Mini-Mental State Examination (MMSE) are neuropsychological assessments. Functional Assessment Questionnaire (FAQ) is a functional and behavioral assessment. High values of ADAS-Cog 13 and FAQ indicate a high-risk state for AD, whereas low values of RAVLT.imme, RAVLT.learn and MMSE reflect severe cognitive impairment. The longitudinal trajectories in ADNI-1 and ADNI-2 are defined on the same time domain with the largest follow-up time 96 months from the start of ADNI-1 (time 0).

4.1 Multivariate FPCA via mFACEs

We analyze the five longitudinal biomarkers using mFACEs. For better visualization, we plot in Figure 5 the estimated correlation functions ρkk′(s,t)=Ckk′(s,t)/Ckk(s,s)Ck′k′(t,t). The plot indicates of two groups of biomarkers: ADAS-Cog 13 and FAQ in one group whereas RAVLT.imme, RAVLT.learn and MMSE in another group. The biomarkers within the groups are positively correlated and negatively correlated between groups, which make sense as high values of ADAS-Cog 13 and FAQ and low values for the other biomarkers suggest of AD. Next, we display in Figure 6 the two estimated (multivariate) eigenfunctions associated with the top two estimated eigenvalues, which account for 69% and 11% of the total variance in the functional part of the data, respectively. The eigenfunctions reveal how the 5 biomarkers co-variate and how a subject’s trajectories of biomarkers deviate from the population mean. Indeed, we see from Figure 6 that the first eigenfunction (solid curves) is below the zero-line for ADAS-Cog 13 and FAQ and above the zero-line for the other three biomarkers. This means that the score corresponding to the first eigenfunction might be used as an indicator of AD. Indeed, a negative score for the first eigenfunction means higher-than-population-mean values of the former while lower-than-population-mean values of the latter, indicating more severe AD status. The second eigenfunction (dashed curves) for the five biomarkers is below the zero line at first and then above it or the other way around, potentially suggesting of a longitudinal pattern of the AD progression. Specifically, these subjects with a positive score for the second eigenfunction will have higher ADAS-Cog 13/FAQ and lower RAVLT and MMSE over the months, suggesting of AD progression. Finally, we illustrate in Figure 7 the predicted curves along with the associated 95% point-wise confidence bands for three subjects. We focus on predicting the trajectories over the first four years as there are more observations. We can see that the confidence bands are getting wider at the later time points because of fewer observations.

4.2 Comparison of Prediction Performance of Different Methods

We compare the proposed mFACEs with mFPCA and mFPCA(CE) for predicting the five longitudinal biomarkers. The prediction performance is evaluated by the average squared prediction errors (APE), APEk=1n∑i=1n[1mik∑j=1mik{yij(k)−y^ij(k)}2],

where y^ij(k) is the predicted value of the kth biomarker for the ith subject at time tij(k). We conduct two types of validation: an internal validation and an external validation. For the internal validation, we perform a 10-fold cross-validation to the combined data of ADNI-1 and ADNI-2. For the external validation, we fit the model using only the ADNI-1 data and then predict ADNI-2 data. Figure 8 summarizes the results. For simplicity, we present the relative efficiency of APE, which is the ratio of APEs of one method against the mFPCA. In both cases, mFACEs achieves better prediction accuracy than competing methods. Note that mFPCA(CE) outperforms mFPCA for predicting almost all biomarkers. The results suggest that: 1) mFACEs is better than competing methods for analyzing the longitudinal biomarkers. 2) exploiting the correlations between the biomarkers improve prediction.

5 DISCUSSION

The prevalence of multivariate functional data has sparked much research interests in recent years. However, covariance estimation for multivariate sparse functional data remains underdeveloped. We proposed a new method, mFACEs, and its features include: 1) a covariance smoothing framework is proposed to tackle multivariate sparse functional data; 2) an automatic and fast fitting algorithm is adopted to ensure the scalability of the method; 3) eigenfunctions and eigenvalues can be obtained through a one-time spectral decomposition, and eigenfunctions can be easily evaluated at any sampling points; 4) a multivariate extension of the conditional expectation approach (Yao et al. 2005) is derived to exploit correlations between outcomes. The simulation study and the data example showed that mFACEs could better capture between-function correlations and thus give improved principal component analysis and curve prediction.

When the magnitude of functional data are quite different, one may first normalize the functional data, as recommended by Chiou et al. (2014). One method of normalization is to rescale the functional data using the estimated variance function C^kk(t,t)−1/2 as in Chiou et al. (2014) and Jacques and Preda (2014). An alternative method is to use a global rescaling factor like (∫C^kk(t,t)dt)−1/2 as in Happ and Greven (2018). Both methods can be easily incorporated into our proposed method. In our data analysis, we find that the results with normalization are very close to those without normalization, thus we present the results without normalization.

Because multivariate FPCA is more complex than univariate FPCA, weak correlations between the functions and small sample size may offset the benefit of conducting multivariate FPCA, see Section 7.3 in Wong et al. (2019). Thus, it is of future interest to develop practical tests to determine if correlations between multivariate functional data are different from 0.

The mFACEs method has been implemented in an R package mfaces and will be submitted to CRAN for public access.

Supplementary Material

Suppl

APPENDICES

Appendix A: Proofs of Propositions 1 and 2

Proof of Proposition 1. Define b˜(t)=G−12b(t), then ∫b˜(t)b˜(t)⊤dt=I. Define Θ⌣kk′(t)=G12Θkk′G12. According to (1), dlΨl(k)(s)=∑k′=1p∫b(s)⊤Θkk′b(t)Ψl(k′)(t)dt

=b˜(s)⊤∑k′=1pΘˇkk′∫b˜(t)Ψl(k′)(t)dt.

Thus, ψl(k)(s)= b˜(s)⊤ul(k) with ul(k)=dl−1{∑k′=1pΘ⌣kk′∫ b˜(t)ψl(k′)(t)dt}∈ℝc and ul=(ul(1),⊤,…,ul(p),⊤)⊤∈ℝpc. Since ∑k=1p∫Ψl(k)(t)Ψl′(k)(t)dt=1{l=l′}, we derive that (11) ul⊤ul'=∑k=1pul(k),⊤ul'(k)=1{l=l'}.

Ckk'(s,t)=b˜(s)⊤Θˇkk'b˜(t)=b˜(s)⊤{∑l≥1dlul(k)ul(k'),⊤}b˜(t),

which gives Θˇkk'=∑l≥1dlul(k)ul(k').

As 1 ≤ k; k′ ≤ p, the above is equivalent to (Θˇ11…Θˇ1p⋮⋱⋮Θˇp1…Θˇpp)︸:=Θˇ=∑l≥1dlulul⊤.

Because of (11), uls are orthonormal eigenvectors of Θˇ with dls the corresponding eigenvalues. The proof is now complete. Proof of Proposition 2. By (10), (12) iGCV=‖C^‖2−2f⊤Σ−1f+f⊤Σ−2f+2∑i=1n(LiΣ−1f−fi)⊤Σ−1(LiΣ−1f−fi).

Since Σ−1=Udiag(d˜)U⊤, we have (13) f⊤Σ−1f=f˜⊤diag(d˜)f˜=d˜⊤(f˜⊙f˜).

Similarly, (14) f⊤Σ−2f=f˜⊤diag(d˜2)f˜=(f˜⊙d˜)⊤(f˜⊙d˜).

Next we derive that (LiΣ−1f−fi)⊤Σ−1(LiΣ−1f−fi)

=(U⊤LiΣ−1f−U⊤fi)⊤diag(d˜)(U⊤LiΣ−1f−U⊤fi)

=(L˜idiag(d˜)f˜−f˜i)⊤diag(d˜)(L˜idiag(d˜)f˜−f˜i.)

It follows that (15) (LiΣ−1f−fi)⊤Σ−1(LiΣ−1f−fi)=d˜⊤[{L˜i(f˜⊙d˜)}⊙{L˜i(f˜⊙d˜)}+(f˜i⊙ f˜i)]−2d˜⊤{(f˜if˜⊤)⊙L˜i}d˜.

Combining (12), (13), (14) and (15), the proof is complete.

Appendix B: A Lemma

Lemma 1. The covariance operator with the covariance functions defined in Section 4.1 is positive semi-definite.

Proof. Let a=(a1,…,ap)⊤∈ℝp and X˜=a⊤x, then X˜ is a stochastic process with covariance function Cov{X˜(s), X˜(t)}=∑kk′akak′Ckk′(s,t)=∑kk′akak′(ρ+(1−ρ)1{k=k′}) Φ˜k(s)⊤Φ˜k′(t),

where Φ˜k(s)=Φk(s)⊤Λkk12. Let Ψ(s)=∑k=1pakΦ˜k(s). Then Cov{X˜(s), X˜(t)}=ρΨ(s)⊤Ψ(t)+(1−ρ)∑kak2Φ˜k(s)⊤Φ˜k(t).

which is always positive semi-definite and the proof is complete.

FIGURE 1 Boxplots of RISEs of mFACEs and mFPCA for estimating the covariance function.

FIGURE 2 Boxplots of ISEs of mFACEs and mFPCA for estimating the top two eigenfunctions.

FIGURE 3 Violin plots of mFACEs and mFPCA for estimating the top two eigenvalues. The red horizontal lines indicate that the estimates are equal to the truth.

FIGURE 4 Boxplots of relative efficiency of three methods for curve prediction. The gray horizontal lines indicate the MISEs for univariate FPCA.

FIGURE 5 Estimated correlation functions for the longitudinal markers.

FIGURE 6 Estimated top two eigenfunctions for the longitudinal markers.

FIGURE 7 Predicted subject-specific curves (red solid line) of the longitudinal markers and associated 95% point-wise confidence bands (blue dashed line) for three subjects.

FIGURE 8 The internal and external prediction validations for the ADNI longitudinal makers.


References

Berrendero J , Justel A , &amp; Svarc M (2011). Principal components for multivariate functional data. Computational Statistics &amp; Data Analysis, 55 (9 ),2619–2634.
Cai T , &amp; Yuan M (2010). Nonparametric covariance function estimation for functional and longitudinal data. University of Pennsylvania and Georgia inistitute of technology.
Chiou J-M , Chen Y-T , &amp; Yang Y-F (2014). Multivariate functional principal component analysis: A normalization approach. Statistica Sinica,1571–1596.
Chiou J-M , &amp; Müller H-G (2014). Linear manifold modelling of multivariate functional data. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76 (3 ), 605–626. Retrieved from https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12038 doi:10.1111/rssb.12038
Chiou J-M , &amp; Müller H-G (2016). A pairwise interaction model for multivariate functional and longitudinal data. Biometrika, 103 (2 ), 377–396.27279664
Eilers P , &amp; Marx B (1996). Flexible smoothing with B-splines and penalties (with Discussion). Statist. Sci, 11 , 89–121.
Eilers P , &amp; Marx B (2003). Multivariate calibration with temperature interaction using two-dimensional penalized signal regression. Chemometrics and Intelligent Laboratory Systems, 66 , 159–174.
Goldsmith J , Crainiceanu C , Caffo B , &amp; Reich D (2012). Longitudinal penalized functional regression for cognitive outcomes on neuronal tract measurements. Journal of the Royal Statistical Society: Series C (Applied Statistics), 61 , 453–469.
Greven S , Crainiceanu C , Caffo B , &amp; Reich D (2010). Longitudinal functional principal component. Electronic J. Statist, 4 , 1022–1054.
Happ C , &amp; Greven S (2018). Multivariate functional principal component analysis for data observed on different (dimensional) domains. Journal of the American Statistical Association, 113 (522 ), 649–659.
Huang H , Li Y , &amp; Guan Y (2014). Joint modeling and clustering paired generalized longitudinal trajectories with application to cocaine abuse treatment data. Journal of the American Statistical Association, 109 (508 ), 1412–1424.
Jacques J , &amp; Preda C (2014). Model-based clustering for multivariate functional data. Computational Statistics &amp; Data Analysis, 71 , 92–106.
James G , Hastie T , &amp; Sugar C (2000). Principal component models for sparse functional data. Biometrika, 87 , 587–602.
Kowal DR , Matteson DS , &amp; Ruppert D (2017). A bayesian multivariate functional dynamic linear model. Journal of the American StatisticalAssociation, 112 (518 ), 733–744.
Leng X , &amp; Müller H (2006). Classification using functional data analysis for temporal gene expression data. Bioinformatics, 22 , 68–76.16257986
Li J , Huang C , Zhu H , &amp; Initiative ADN (2017). A functional varying-coefficient single-index model for functional response data. Journal of theAmerican Statistical Association, 112 (519 ), 1169–1181.
Li K , Chan W , Doody RS , Quinn J , &amp; Luo S (2017). Prediction of conversion to alzheimerÂÀÂŕs disease with longitudinal measures and time-to-event data. Journal of Alzheimer’s Disease, 58 (2 ), 361–371.
Li Y , Wang N , &amp; Carroll RJ (2013). Selecting the number of principal components in functional data. Journal of the American Statistical Association, 108 (504 ), 1284–1294.
Lindquist M (2012). Functional causal mediation analysis with an application to brain connectivity. Journal of the American Statistical Association,107 (500 ), 1297–1309.25076802
Luo R , &amp; Qi X (2017). Function-on-function linear regression by signal compression. Journal of the American Statistical Association, 112 (518 ),690–705.
Morris J , Arroyo C , Coull B , Ryan L , Herrick R , &amp; Gortmaker S (2006). Using wavelet-based functional mixed models to characterize population heterogeneity in accelerometer profiles: A case study. Journal of the American Statistical Association, 101 (476 ), 1352–1364.19169424
Park J , &amp; Ahn J (2017). Clustering multivariate functional data with phase variation. Biometrics, 73 (1 ), 324–333.27218696
Peng J , &amp; Paul D (2009). A geometric approach to maximum likelihood estimation of functional principal components from sparse longitudinal data. J. Comput. Graph. Stat, 18 , 995–1015.
Petersen A , &amp; Müller H-G (2016). Fréchet integration and adaptive metric selection for interpretable covariances of multivariate functional data. Biometrika, 103 (1 ), 103–120.
Qi X , &amp; Luo R (2018). Function-on-function regression with thousands of predictive curves. Journal of Multivariate Analysis, 163 , 51–66.
Qiao X , Guo S , &amp; James GM (2019). Functional graphical models. Journal of the American Statistical Association, 114 (525 ), 211–222.
Ramsay J , &amp; Silverman B (2005). Functional data analysis. New York: Springer.
Reimherr M , &amp; Nicolae D (2014). A functional data analysis approach for genetic association studies. The Annals of Applied Statistics, 8 , 406–429.
Reimherr M , &amp; Nicolae D (2016). Estimating variance components in functional linear models with applications to genetic heritability. Journal of the American Statistical Association, 111 , 407–422.
Reiss P , &amp; Ogden R (2010). Functional generalized linear models with images as predictors. Biometrics, 66 , 61–69.19432766
Saporta G (1981). Méthodes exploratoires dÂąÂŕanalyse de données temporelles. Cahiers du bureau universitaire de recherche opérationnelle(37–38).
Seber G (2007). A matrix handbook for statisticians. New Jersey: Wiley-Interscience.
Weiner MW , Veitch DP , Aisen PS , Beckett LA , Cairns NJ , Green RC , … others (2017). Recent publications from the alzheimer’s disease neuroimaging initiative: Reviewing progress toward improved ad clinical trials. Alzheimer’s &amp; dementia: the journal of the Alzheimer’s Association, 13 (4 ), e1–e85.
Wong RK , Li Y , &amp; Zhu Z (2019). Partially linear functional additive models for multivariate functional data. Journal of the American Statistical Association, 114 (525 ), 406–418.
Wong RK , &amp; Zhang X (2019). Nonparametric operator-regularized covariance function estimation for functional data. Computational statistics &amp; data analysis, 131 , 131 – 144. Retrieved from http://www.sciencedirect.com/science/article/pii/S0167947318301221 doi: 10.1016/j.csda.2018.05.013
Wood SN (2000). Modelling and smoothing parameter estimation with multiple quadratic penalties. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 62 (2 ), 413–428.
Xiao L , Huang L , Schrack J , Ferrucci L , Zipunnikov V , &amp; Crainiceanu C (2015). Quantifying the life-time circadian rhythm of physical activity: a covariate-dependent functional approach. Biostatistics, 16 , 352–367.25361695
Xiao L , Li C , Checkley W , &amp; Crainiceanu C (2017). R package face: Fast covariance estimation for sparse functional data (version 0.1–4). URL:http://cran.r-project.org/web/packages/face/index.html.
Xiao L , Li C , Checkley W , &amp; Crainiceanu C (2018). Fast covariance estimation for sparse functional data. Statistics and Computing, 28 , 511–522.29449762
Xu G , &amp; Huang J (2012). Asymptotic optimality and efficient computation of the leave-subject-out cross-validation. Ann. Statist, 40 , 3003–3030.
Yao F , Müller H , &amp; Wang J (2005). Functional data analysis for sparse longitudinal data. J. Amer. Statist. Assoc, 100 , 577–590.
Zhou L , Huang JZ , &amp; Carroll RJ (2008). Joint modelling of paired sparse functional data using principal components. Biometrika, 95 (3 ), 601–619.19396364
Zhou S , Shen X , Wolfe D , (1998). Local asymptotics for regression splines and confidence regions. Annals of Statistics, 26 (5 ), 1760–1782.
Zhu H , Brown P , &amp; Morris J (2012). Robust classification of functional and quantitative image data using functional mixed models. Biometrics, 68 , 1260–1268.22670567
Zhu H , Li R , &amp; Kong L (2012). Multivariate varying coefficient model for functional responses. Annals of statistics, 40 (5 ), 2634.23645942
Zhu H , Morris JS , Wei F , &amp; Cox DD (2017). Multivariate functional response regression, with application to fluorescence spectroscopy in a cervical pre-cancer study. Computational statistics &amp; data analysis, 111 , 88–101.29051679
Zhu H , Strawn N , &amp; Dunson DB (2016). Bayesian graphical models for multivariate functional data. Journal of Machine Learning Research, 17 (204 ),1–27.
