LICENSE: This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.


9814863
21942
J Alzheimers Dis
J Alzheimers Dis
Journal of Alzheimer's disease : JAD
1387-2877
1875-8908

36189586
9885492
10.3233/JAD-215606
NIHMS1864936
Article
Spoken word recognition in listeners with mild dementia symptoms
McClannahan Katrina S. a
Mainardi Amelia a
Luor Austin a
Chiu Yi-Fang b
Sommers Mitchell S. c
Peelle Jonathan E. a
a Department of Otolaryngology, Washington University in St. Louis
b Department of Speech, Language and Hearing Sciences, Saint Louis University
c Department of Psychological and Brain Sciences, Washington University in St. Louis
Please address correspondence to: Dr. Katrina (Kate) McClannahan, Department of Otolaryngology, MSC 8042-26-2000, 600 S. Euclid, St. Louis, MO 63110, Phone: (314) 747-0109, k.mcclannahan@wustl.edu
26 1 2023
2022
30 1 2023
90 2 749759
This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.
Background

Difficulty understanding speech is a common complaint of older adults. In quiet, speech perception is often assumed to be relatively automatic. However, higher-level cognitive processes play a key role in successful communication in noise. Limited cognitive resources in adults with dementia may therefore hamper word recognition.

Objective

The goal of this study was to determine the impact of mild dementia on spoken word recognition in quiet and noise.

Methods

Participants were 53–86 years with (n=16) or without (n=32) dementia symptoms as classified by the Clinical Dementia Rating scale. Participants performed a word identification task with two levels of word difficulty (few and many similar sounding words) in quiet and in noise at two signal-to-noise ratios, +6 and +3 dB. Our hypothesis was that listeners with mild dementia symptoms would have more difficulty with speech perception in noise under conditions that tax cognitive resources.

Results

Listeners with mild dementia symptoms had poorer task accuracy in both quiet and noise, which held after accounting for differences in age and hearing level. Notably, even in quiet, adults with dementia symptoms correctly identified words only about 80% of the time. However, word difficulty was not a factor in task performance for either group.

Conclusion

These results affirm the difficulty that listeners with mild dementia may have with spoken word recognition, both in quiet and in background noise, consistent with a role of cognitive resources in spoken word identification.

speech intelligibility
word processing
cognition
hearing
dementia

pmcIntroduction

Communication is a foundation of daily experience and social interaction. Impaired communication skills associated with dementia can be distressing for both people with dementia and their caregivers and increase caregiver burden [1]. Understanding how basic speech perception is affected by dementia will assist health care professionals to better manage the impact of dementia on verbal communication. Speech perception requires intact cognitive abilities to convert acoustic signals into meanings. For older adults with good hearing, the experience of understanding speech in quiet often feels relatively automatic. However, when the speech signal is acoustically challenging due to hearing loss or background noise, successful understanding requires a greater engagement of higher-level cognitive abilities [2–5]. The relative contributions of auditory and cognitive factors to speech perception for people with dementia symptoms remains unclear, and thus, have been at the forefront of hearing science and dementia research.

Prior studies have shown that listeners with dementia have more difficulty with spoken word recognition than controls [6; 7] potentially due to cognitive problems associated with dementia (e.g., executive function deficits). For example, Marshall and colleagues [8] used a gating paradigm to investigate lexical competition in a group of listeners with AD dementia. In a gating paradigm [9], listeners are presented with a portion of the beginning of a word and asked to guess what the word is. If they guess incorrectly, they are given another presentation, with more acoustic information. For example, they might be presented with the first 100 milliseconds of a word and incorrectly guess; but given 200 milliseconds they may correctly identify the word. The number of gates needed for correct identification is then used as an indication of when in the word it is identifiable. Participants with AD dementia performed significantly worse on the word identification task than did controls, including taking longer (i.e., a greater number of gates) to identify a group of potential target words once the first phoneme had been identified.

Cognitive difficulties are not the only contributor to speech recognition difficulty, however. Age-related hearing loss is common, having an estimated prevalence in the United States of 49% in adults 60–69 years [10] and 63.1% for adults 70 years and older [11]. It is well understood that declines in hearing sensitivity negatively impact speech understanding in several ways, including reduced audibility (how well someone can hear a speech signal) and changes to both spectral and temporal processing [12]. Additionally, environmental conditions, such as listening in reverberant settings or background noise)], may further degrade and corrupt the signal received by the central auditory system [13].

Recent research continues to find that these types of acoustic challenges force listeners to engage additional cognitive resources when listening to speech [14]. Expending cognitive resources at this early stage of listening subsequently reduces the availability of those resources for additional processing, such as encoding the spoken information into memory [15–20]. Consistent with these behavioral findings, an increasing number of functional brain imaging studies have associated acoustic challenge with increased activity in the frontoparietal and cingulo-opercular executive attention networks [21–26].

Lexical competition models, which conceptualize word recognition as involving competition between candidates [27–30], provide a useful framework to examine the challenges of spoken word recognition. The Neighborhood Activation Model (NAM), for example, accounts for the ways in which the structure of the mental lexicon, or the representation of words in memory, will affect the way a listener processes and maps the phonological input of the words they hear [28]. Like other activation-competition models, the NAM proposes that as a listener hears a spoken word, activation of the target word occurs simultaneously with activation of competitor words in its similarity neighborhood. Previous studies have shown that target words with dense, high frequency neighborhoods (i.e., frequently-encountered words with many similar sounding competitors) are identified more slowly and less accurately whereas words with sparse, low frequency neighborhoods are identified more quickly and accurately [7; 28; 31–33].

In addition to increased difficulty for specific combinations of word frequency and neighborhood density, neighborhood density (words from dense versus sparse neighborhoods) itself has been shown to affect word identification independent of word frequency [31; 32]. That is, a word that sounds like many other words, for example, the target word “cat” has competitors “cap”, “can”, “hat”, and so on, requires greater involvement in selection (given the many competing alternatives) and/or inhibition of competitors for a listener to correctly select the target. The effects of neighborhood density on spoken word recognition have been shown to be more pronounced for older adults as compared to young adults [26; 28; 29] and for older adults with cognitive decline as compared to healthy aging. Furthermore, Sommers (1998) showed that while overall performance on a speech task in quiet was worse for individuals with dementia than for cognitively normal peers, this effect was driven by performance on words that sound more similar to several other words, described as “lexically hard” words. Therefore, listeners with reduced cognitive capacity may find words with many phonological neighbors to be particularly problematic.

Thus, prior research on spoken word recognition broadly supports an important role for cognition in understanding speech in noise, when the speech signal is degraded (through background noise or hearing loss), and for words with many competitors. Given the cognitive changes frequently associated with many forms of dementia, including Alzheimer disease (AD) dementia, it is perhaps surprising that relatively few studies have investigated simple word recognition in listeners with mild dementia while carefully controlling for hearing ability. This is a critical issue because hearing sensitivity can have a large effect on speech recognition [34–36], and thus failing to adequately account for hearing loss can confound any conclusions drawn based on cognitive status.

In the current study, we use spoken word recognition as a window into the role of cognitive processes engaged during listening, which has implications for both quality-of-life (that is, understanding communication experiences of listeners with dementia symptoms) and clinical usefulness, for example, incorporating speech recognition as a potential diagnostic marker. Therefore, one of our goals in the careful construction of our specific task was to include various types of challenges to word recognition. Importantly, we used well-validated clinical measures to identify hearing status to determine the contribution of hearing to an individual’s ability to correctly identify spoken words in quiet and background noise. We hypothesized that individuals with mild dementia symptoms, and thus reduced cognitive ability compared to cognitively normal peers, would have greater difficulty identifying words in conditions that require increased cognitive resources, particularly understanding speech in noisier listening conditions. Because identification of words with sparse phonological neighborhoods does not require the same degree of cognitive engagement as identifying words from dense neighborhoods (with more competitors), we expected to see similar performance for sparse words between listeners with normal cognitive function and mild dementia symptoms.

Materials and Methods

Raw data and analysis scripts are available at https://osf.io/k72su/.

Participants

The research protocol was approved by the Washington University in Saint Louis Institutional Review Board. Participants were recruited from the Charles F. and Joanne Knight Alzheimer Disease Research Center (ADRC) at Washington University in Saint Louis. Dementia symptoms for each participant was categorized using the Clinical Dementia Rating (CDR©) Dementia Staging Instrument [37; 38] which uses a five-point scale to classify individuals as normal (0), very mild (.5), mild (1), moderate (2), or severe dementia (3). Individuals with a CDR of 0, 0.5, and 1 were recruited for this study. All participants were native speakers of English with no additional diagnosed neurological disorders. A total of 48 participants aged 53–86 years (M=74.7; SD=6.0) took part in this study, 32 (18 women) CDR 0, 12 (6 women) CDR 0.5, 3 (2 women) CDR 1. Participants with CDR 0.5 and 1 were combined into a mild dementia symptoms group. Two participants, one from each group, were excluded because their better ear pure tone average (PTA), an indicator of speech audibility, was above 60 dB HL resulting in insufficient audibility of the task stimuli. See Table 1 for additional demographic information.

Procedure

Hearing Assessment

Participants completed a comprehensive audiometric assessment, including otoscopy and tympanometry to confirm normal outer and middle ear status, pure tone air (octave intervals from 250–8000 Hz) and bone conduction thresholds (octave intervals from 500–4000 Hz), using a validated procedure [39].

Stimuli

Speech materials consisted of 240 monosyllabic words spoken by a single male speaker. The average log target word frequency was 2.93 for sparse and 3.17 for dense words [40].1 We used each word’s neighborhood density from the English Lexicon Project [41] to characterize each item as coming from a phonological neighborhood that is either sparse (few phonological neighbors, easier to recognize) or dense (many phonological neighbors, more difficult to recognize). The sparse words had an average neighborhood frequency2 of 7.21 occurrences per million and an average neighborhood density of 5.35. The dense words had an average neighborhood frequency of 8.63 occurrences per million and an average neighborhood density of 28.6. A total of 40 sparse and 40 dense words, were presented at 75 dBC SPL using a HSC271 Over-Ear Headset and AudioQuest DragonFly v1.5 amplifier, in three noise conditions: in quiet, +6 dB SNR, and +3 dB SNR. Speech-in-noise stimuli were created by adding speech shaped noise to individual sound files. Stimuli included 500 ms of noise before and after each word (using jp_addnoise.m available from http://github.com/jpeelle/jp_matlab); that is, the word occurred in the midst of background noise, but with some additional noise added before and after the word, which helps presentation feel more natural. Words were presented in the same order for all participants.

Task Procedure

The word identification task was presented on a laptop computer using E-prime Version 2 (Psychology Software Tools, Pittsburgh, USA). Instructions for the task were presented both orally and in writing. To initiate each trial, the participants were instructed to press any button on the keyboard, at which time they saw the prompt “LISTEN.” The target word was presented after a 500 ms delay. The participant was then cued to repeat the word, out loud, when they saw a “******” prompt. If the participant was unsure of the word, they were instructed to provide their best guess. Participants were allowed breaks in between trials when needed. The task included five practice trials for each condition. Word identification accuracy was scored manually by the test administrators. Responses were counted as correct only if they were identical to the target word. An audio recording for each session was reviewed to verify scoring accuracy.

Data Analysis

We analyzed data using R version 3.6.0 (R Core Team, 2020).

Data were checked to determine that no statistical assumptions were violated. Hearing sensitivity was quantified as the better ear 4-frequency (.5, 1, 2 and 4 kHz) PTA for each participant. Group level differences in hearing sensitivity and age were assessed using t-tests.

Performance on the spoken word identification task (a dichotomous variable, 0-incorrect, 1-correct) was analyzed using a generalized linear mixed-effects model with a logit link function in the R package lme4 [42] to determine how cognitive status, phonological neighborhood density, and signal-to-noise ratio contributed to word identification accuracy. Fixed effects included in the model were age, hearing sensitivity, neighborhood density (sparse, dense), noise level (quiet, +6 SNR, and +3 SNR), and group (control, mild dementia symptoms), and interaction terms. Individual word items were entered into the model as random intercepts to account for variability in how accurately individual word items were identified. Participants were entered as a random intercept to account for variability in individual response accuracy as well as the neighborhood density by participant random slope, to account for differences in the direction or magnitude of the neighborhood density effect on a participant-by-participant basis.

Results

Model selection

A lower order model was built with five fixed effects of interests: age, hearing threshold, neighborhood density (reference = sparse), SNR (reference = quiet), and group (reference = Control). Two-way and three-way interactions of interests were then added, including neighborhood density ⨉ group, SNR ⨉ group, neighborhood density ⨉ SNR, and neighborhood density ⨉ SNR ⨉ group, which represented our central hypothesis. A likelihood-ratio test indicated that the model including the three-way interaction (neighborhood density ⨉ SNR ⨉ group) did not provide a better fit for the data than a model without it (χ2(2) = 0.12, p = 0.94) and it was removed from the final model.

Likelihood-ratio tests indicated that a model including the SNR ⨉ group interaction term provided a better fit for the data compared to a model without it (χ2(2) = 37.12, p &lt; 0.001), however, neither the neighborhood density ⨉ group nor the neighborhood density ⨉ SNR interaction term improved model fit. Therefore, our final model included the five fixed effects (age, hearing threshold, neighborhood density, SNR, and group) and the SNR ⨉ group interaction term. The final model specification was the following: Task Accuracy ~ Age + Hearing Threshold + Lexical Difficulty + SNR + Group + SNR*Group + (1+Lexical Difficulty|Participant) + (1|Task Item). The lower order model, with all interaction terms removed, was used to interpret the main effects of SNR and group. The lower order model specification was the following: Task Accuracy ~ Age + Hearing Threshold + Lexical Difficulty + SNR + Group + (1+Lexical Difficulty|Participant) + (1|Task Item).

Age and Hearing Sensitivity

Groups did not differ in mean age (t (46) = 0.66, p = 0.51). Participants’ hearing sensitivity ranged from normal hearing to a severe hearing loss in the better ear. Individual and group mean better ear air conduction thresholds are displayed in Figure 1. Groups were significantly different in better ear PTA (t (46) = −2.20, p = .03), with the dementia group (mean = 36.8, SD = 15.0) having poorer hearing sensitivity than the control group (mean = 28.3, SD = 11.3). Because audibility and hearing sensitivity are critically important for speech recognition, we took this group mean difference into consideration in two ways in the subsequent analyses. First, we accounted for better ear PTA statistically in our model design and second, we performed all analyses twice, before and after equating for hearing ability between groups. The first analysis was done with the full groups and for the second, we removed six participants in the control group with better ear PTAs &lt; 20 dB HL, which resolved the group mean difference in hearing sensitivity. There were no appreciable changes in overall results (See Supplemental Materials), therefore, all results moving forward will be from the full groups.

Word Identification Task Performance-Mixed Effects Modeling

As previously described, prior to analysis we removed 2 participants, one from each group, because their better ear PTA was above 60 dB HL, which resulted in insufficient audibility of the task stimuli. This reduced our sample size to 46 for all subsequent analyses. Figure 2 shows percent correct performance on the word identification task. Comparison between full and hearing ability equated groups can be found in Supplemental Materials. Our final mixed-effects model (Table 2), previously described, predicted the probability of a correct response on the word identification task.

Analyses revealed that the effects of age and hearing sensitivity, but not neighborhood density, were significant (Figure 3). Hearing sensitivity significantly contributed to the total variance in word identification accuracy. More specifically, a one decibel increase in better ear PTA decreased the odds of correctly identifying a target word by a multiplicative factor of 0.92 (β= −0.08, SE= 0.01, z= −9.78, p&lt;0.001), with other variables fixed. Age also had a significant effect on word identification accuracy. For every one year increase in age the odds of identifying a target word decreased by a multiplicative factor of 0.94 (β=−0.06, SE= 0.02, z=−4.01, p&lt;0.001), with other variables fixed.

Group and SNR had the largest effects on accuracy. Compared to words presented in quiet, the odds of correctly identifying a target word decreased by a multiplicative factor of 0.12 in the +6 dB SNR (β= −2.09, SE= 0.25, z= −8.52, p&lt;0.001) and a multiplicative factor of 0.08 in +3 dB SNR (β= −2.57, SE= 0.24, z= −10.51, p&lt;0.001) conditions (Table 3), with other variables fixed. Analysis of the group effect revealed that the odds of correctly identifying a target word is decreased by a multiplicative factor of 0.39 for the mild dementia group (β= −0.93, SE= 0.19, z= −4.83, p&lt;0.001) when referenced to the control group (Table 3) and other variables fixed.

The significant SNR ⨉ group interaction (Table 2), indicated that the effect of noise varied by group status. Specifically, and counter to our expectations, the change from quiet to noise resulted in a lesser detriment to task performance for the group with dementia symptoms compared to the control group for both the +6 dB SNR (β= 1.10, SE= 0.18, z= 6.11, p&lt;0.001) and +3 dB SNR conditions (β= 0.79, SE= 0.18, z= 4.42, p&lt;0.001).

Discussion

In this study we examined how mild dementia may affect spoken word recognition. Our results indicated that listeners with mild dementia symptoms had more difficulty with word identification than cognitively normal controls in quiet and at all noise levels. Counter to our predictions, however, we did not find an effect of word difficulty on word identification accuracy, and listeners with mild dementia symptoms were less affected by increased background noise than were controls. We discuss these findings in detail below.

Although word recognition in quiet often feels automatic, listeners still need to select a correct word from among possible alternatives. We found that listeners with mild dementia symptoms showed poorer word identification accuracy in quiet (~81% correct) than controls (~96% correct). Because we accounted for hearing loss in groups through a comprehensive audiological assessment, inclusion in our model, and subgroup analysis, we attribute these group differences to deficits above the level of the auditory periphery. It is possible that differences in performance may still be related to auditory deficits in the form of diminished central auditory processing. Previous work suggests individuals with cognitive deficits also perform worse on measures used to assess central auditory processing [43–45]. Group differences could, alternatively, be related to deficits in non-auditory, cognitive deficits associated with mild dementia, including declines in executive function. Future studies are required to provide more specificity in the mechanisms that may underly these group differences in spoken word recognition.

While we anticipated performance for both groups to be significantly affected by noise, and for the control group to outperform the group with mild dementia symptoms, we hypothesized that noise would have a larger negative effect on performance for the mild dementia group due to the greater cognitive demands of listening to speech in noise. Instead, we found that word recognition accuracy was more affected by noise in our control sample than in listeners with mild dementia symptoms. To some degree, this effect can be explained by the listeners with mild dementia symptoms performing significantly worse in quiet, thus having less “room” to reflect the challenge of noise, compared to the control group. However, the explanation is not fully satisfying, because even in the hardest condition identification accuracy is still around 50% (no floor effect was present). In that sense, the listeners with mild dementia symptoms are performing better-than-expected in the most difficult SNR condition. We do not have a ready explanation for this provocative finding and further investigation is warranted.

Prior studies examining the added impact of noise on speech understanding in listeners with mild dementia are mixed. Mamo and Helfer [46] examined auditory sentence processing in listeners with diagnosed or suspected mild cognitive impairment (MCI) or AD dementia in quiet and background noise. They found poorer performance for listeners with MCI and AD compared to neurologically healthy age- and hearing-matched controls, consistent with our results. Other studies, however, have reported decrements in speech perception for adults with cognitive difficulties emerging only after the introduction of background noise [47; 48], or no differences in speech perception in quiet or noise between groups [49]. Differences across studies are likely due to various methodologies used to assess word recognition, different ways of determining cognitive status, and whether audibility of task stimuli and participant hearing status were taken into account. The emerging consensus, however, is that people with AD have difficulty with speech perception, even when no background noise is present.

In our study, hearing sensitivity played a significant role in spoken word identification accuracy: listeners with poorer hearing performed worse on spoken word identification. This is not surprising, given the known effects of hearing loss on speech perception accuracy (see recent review [50]). It is important to note that once better ear hearing sensitivity was accounted for, the group differences in performance persisted: listeners with mild dementia symptoms had worse word identification scores than listeners in the control group. Furthermore, the effect of hearing sensitivity was smaller than that of noise and group membership. Thus, for our sample of older adults, group differences in accuracy were influenced, but not primarily driven, by the greater hearing loss in the listeners with mild dementia symptoms.

We did not find a significant effect of neighborhood density on spoken word identification, for either group. The lack of a density effect does not necessarily mean words from dense neighborhoods were not more difficult: even when identification accuracy is high, pupillometry shows greater cognitive effort associated with recognizing words from dense neighborhoods compared to sparse neighborhoods [51]. It may be that accuracy effects on spoken word recognition would emerge under more difficult SNR conditions. However, given our current results, it seems that neighborhood density plays, at most, a small role in word recognition accuracy, compared to the relatively large effects of cognitive status, hearing ability, and background noise.

When interpreting our results it is important to consider the degree to which we can characterize our participant populations. Our sample size was limited; therefore, we must be modest in generalizing our results. Additionally, we do not have clinical diagnoses, only CDR scores. For example, one participant with dementia symptoms was in their 50s, which is younger than a typical patient with Alzheimer Disease dementia—suggesting perhaps another form of dementia with an earlier onset. For our current interest in assessing effects of general cognitive difficulties on speech perception, we relied on the CDR status, which allowed us to draw conclusions about cognitive ability without a specific clinical diagnosis.

From a practical perspective, we find it noteworthy that listeners in the mild dementia symptoms group were unable to recognize ~20% of the words they heard in quiet. Single words recorded in isolation do not benefit from acoustic or linguistic context, but they are also typically more clearly produced than when in the context of sentences and do not suffer from the co-articulation effects present in connected speech. We envision an awareness of the speech perception challenges in adults with dementia—which cannot be explained by peripheral hearing difficulty—to be useful in coaching conversation partners, such as family members and clinicians, in effective communication strategies. For example, being mindful to speak clearly, slowly, and distinctly, face the listener to provide eye-contact and facial cues, and schedule important conversations when the listener is at their best (i.e., not at the end of a long day) will improve speech understanding for all listeners, but may be particularly important for listeners with dementia.

The difficulty individuals with dementia symptoms have with spoken word recognition is also critical to consider in the context of assessment. Difficulties with sensory processing have long been appreciated to affect estimates of cognitive function [52–56]. For example, if a patient mis-hears instructions, they may fail to complete a test properly; or, on a verbal memory test, mishearing one of the presented items will prevent it from being accurately recalled when later tested. Ensuring audibility of assessment items is critical but does not fully address the issue: in our data, word recognition deficits persisted in people with mild dementia symptoms, even after controlling for peripheral hearing loss. Thus, higher level auditory or cognitive deficits that hinder speech recognition may lead to inaccurate estimates of cognitive function.

In conclusion, our results support the fact that listeners with mild dementia symptoms are likely to have difficulties understanding spoken words, even in quiet. Paradoxically, noise may not have a differential effect, though this may be due to poorer performance in quiet. A clearer understanding of individual differences in specific auditory and cognitive abilities may help further clarify reasons for the significant individual variability observed in all listeners.

Supplementary Material

Supplemental Material

Acknowledgments

The work here was supported by grants R01 DC014281(JEP), T32 AG000030 (KSM trainee, PI: Balota), K01 DC 017522 (KSM), and P30 AG066444 (Knight ADRC) from the US National Institutes of Health. We are grateful to Noël Dwyer for assistance with data collection and Violet Brown for advice on statistical analyses. Special thanks to the Knight ADRC for allowing us access to their participant population and assistance with recruitment.

Figure 1. Pure tone better ear air conduction thresholds for listeners with mild dementia symptoms and controls.

Individual (thin lines) and group mean (thick lines) audiograms.

Figure 2. Spoken word identification accuracy as a function of noise level.

Quiet is the easiest condition, followed by +6 dB SNR and then +3 dB SNR. Each dot represents an individual participant, with the condition mean shown in the filled bars (± 1 SE). See Supplemental Materials for comparison with performance with hearing loss equated groups.

Figure 3. Correlations of age and better ear pure tone average (PTA).

Global word identification accuracy (collapsed across SNR and lexical difficulty) for controls and participants with mild dementia symptoms as a function of Age (left) and Better Ear PTA (right).

Table 1. Participant Demographic Information

		Age (years)	Sex	Education (highest degree in years)	
	
	N	Mean	SD	Range	Male	Female	Mean	SD	Range	
	
CDR 0	31	74.8	5.15	63–86	13	18	16.58	1.95	12–20	
CDR 0.5	12	73.5	7.93	53–82	6	6	15.17	2.17	12–18	
CDR 1	3	75.0	2.65	72–77	1	2	15.33	3.06	12–18	
										
	
CDR &gt; 0 (CDR 0.5 and 1 combined)	15	74.0	6.90	53–82	7	8	15.25	2.18	12–18	

Table 2. Final Model. Effects of Age, Hearing, and Dementia on Word Identification

Fixed Effects	Estimate (odds)	Standard Error	Z-Value	Pr(&gt;|z|)	Confidence Intervals (odds)	
	
Intercept	11.40	1.15	9.95	&lt; .001 ***	9.16 13.65	
Age	−0.06 (0.94)	0.01	−4.02	&lt; .001 ***	−.09 −.03
(.91 .97)	
Hearing Threshold	−0.08 (0.92)	0.01	−9.78	&lt; .001 ***	−.10 −.06
(.91 .94)	
Neighborhood Density – Dense	−0.24 (0.79)	0.20	−1.19	0.23	−.63 .15
(.53 1.17)	
+3 SNR	−3.07 (0.05)	0.27	−11.39	&lt; .001 ***	−3.60 −2.54
(.03 .08)	
+6 SNR	−2.72 (0.07)	0.27	−10.05	&lt; .001 ***	−3.25 −2.19
(.04 .11)	
Dementia Group	−1.72 (0.18)	0.24	−7.17	&lt; .001 ***	−2.19 −1.25
(.11 .29)	
+3 SNR: Dementia Group	0.79 (2.21)	0.18	4.42	&lt; .001 ***
	.44 1.14
(1.55 3.13)	
+6 SNR: Dementia Group	1.10 (3.01)	0.18	6.11	&lt; .001 ***
	.75 1.46
(2.11 4.29)	

Table 3. Lower Order Model. Effects of Age, Hearing, and Dementia on Word Identification

Fixed Effects	Estimate (odds)	Standard Error	Z-Value	Pr(&gt;|z|)	Confidence Interval (odds)	
	
Intercept	10.88	1.14	9.55	&lt; .001 ***	8.65 13.12	
Age	−0.06 (0.94)	0.01	−4.00	&lt; .001 ***	−.09 −.03
(.92 .97)	
Hearing Threshold	−0.08 (0.92)	0.01	−9.86	&lt; .001 ***	−.10 −.06
(.91 .94)	
Neighborhood Density – Dense	−0.24 (0.79)	0.20	−1.20	0.23	−.63 .15
(.53 1.16)	
+3 SNR	−2.57 (0.08)	0.24	−10.51	&lt; .001 ***	−3.05 −2.09
(.05 .12)	
+6 SNR	−2.09 (0.12)	0.25	−8.52	&lt; .001 ***	−2.58 −1.61
(.08 .20)	
Dementia Group	−0.93 (0.39)	0.19	−4.83	&lt; .001 ***	−1.31 −.55
(.27 .57)	

1 Words differ in how often they occur in language, and this frequency of occurrence affects how easily listeners are able to understand words. It is therefore important to control for (and report) word frequency values for studies of speech recognition.

2 Each target word also has a neighborhood—that is, the words that sound similar to a target word. “Cat” has neighbors including “cap”, “can”, “car”, and so on. Each of those words has a frequency of occurrence. The “average neighborhood frequency” refers to the average of the frequency of the neighbors of a target word, typically expressed in untransformed (i.e., non-log) units.

Conflict of Interest

The authors have no conflict of interest to report.


References

[1] Feast A , Orrell M , Charlesworth G , Melunsky N , Poland F , Moniz-Cook E (2016) Behavioural and psychological symptoms in dementia and the challenges for family carers: systematic review. Br J Psychiatry, 208 , 429–434.26989095
[2] Dryden A , Allen HA , Henshaw H , Heinrich A (2017) The Association Between Cognitive Performance and Speech-in-Noise Perception for Adult Listeners: A Systematic Literature Review and Meta-Analysis. Trends Hear, 21 , 2331216517744675.29237334
[3] Rönnberg J , Lunner T , Zekveld A , Sörqvist P , Danielsson H , Lyxell B , Dahlström O , Signoret C , Stenfelt S , Pichora-Fuller MK , Rudner M (2013) The Ease of Language Understanding (ELU) model: theoretical, empirical, and clinical advances. Front Syst Neurosci, 7 , 31.23874273
[4] Tun PA , Williams VA , Small BJ , Hafter ER (2012). The effects of aging on auditory processing and cognition. Am J Audiol, 21 , 344–350.23233520
[5] Wingfield A , Tun PA , McCoy SL (2005) Hearing loss in older adulthood: What it is and how it interacts with cognitive performance. Current Directions in Psychological Science, 14 , 144–148.
[6] Chauvin A , Baum S , &amp; Phillips NA (2021) Individuals With Mild Cognitive Impairment and Alzheimer’s Disease Benefit from Audiovisual Speech Cues and Supportive Sentence Context. J Speech Lang Hear Res, 64 , 1550–1559.33861623
[7] Sommers MS (1998). Spoken word recognition in individuals with dementia of the Alzheimer’s type: changes in talker normalization and lexical discrimination. Psychol Aging, 13 , 631–646.9883463
[8] Marshall NB , Duke LW , Walley AC (1996) Effects of age and Alzheimer’s disease on recognition of gated spoken words. J Speech Hear Res, 39 , 724–733.8844553
[9] Grosjean F (1980) Spoken word recognition processes and the gating paradigm. Percept Psychophys, 28 , 267–283.7465310
[10] Agrawal Y , Platz EA , Niparko JK (2008) Prevalence of hearing loss and differences by demographic characteristics among US adults: data from the National Health and Nutrition Examination Survey, 1999–2004. Arch Intern Med, 168 , 1522–1530.18663164
[11] Lin FR , Metter EJ , O’Brien RJ , Resnick SM , Zonderman AB , Ferrucci L (2011) Hearing loss and incident dementia. Arch Neurol, 68 , 214–220.21320988
[12] Schneider BA , Daneman M , Pichora-Fuller MK (2002) Listening in aging adults: from discourse comprehension to psychoacoustics. Can J Exp Psychol, 56 , 139–152.12271745
[13] Humes LE (1996) Speech understanding in the elderly. J Am Acad Audiol, 7 , 161–167.8780988
[14] Peelle JE (2018) Listening Effort: How the Cognitive Consequences of Acoustic Challenge Are Reflected in Brain and Behavior. Ear Hear, 39 , 204–214.28938250
[15] Heinrich A , Schneider BA (2011) The effect of presentation level on memory performance. Ear Hear, 32 , 524–532.21278574
[16] Koeritzer MA , Rogers CS , Van Engen KJ , Peelle JE (2018) The Impact of Age, Background Noise, Semantic Ambiguity, and Hearing Loss on Recognition Memory for Spoken Sentences. J Speech Lang Hear Res, 61 , 740–751.29450493
[17] McCoy SL , Tun PA , Cox LC , Colangelo M , Stewart RA , Wingfield A (2005) Hearing loss and perceptual effort: downstream effects on older adults’ memory for speech. Q J Exp Psychol A, 58 , 22–33.15881289
[18] Piquado T , Cousins KA , Wingfield A , Miller P (2010) Effects of degraded sensory input on memory for speech: behavioral data and a test of biologically constrained computational models. Brain Res, 1365 , 48–65.20875801
[19] Rabbitt PM (1968) Channel-capacity, intelligibility and immediate memory. Q J Exp Psychol, 20 , 241–248.5683763
[20] Ward CM , Rogers CS , Van Engen KJ , Peelle JE (2016) Effects of Age, Acoustic Challenge, and Verbal Working Memory on Recall of Narrative Speech. Exp Aging Res, 42 , 97–111.26683044
[21] Davis MH , Johnsrude IS (2003) Hierarchical processing in spoken language comprehension. J Neurosci, 23 , 3423–3431.12716950
[22] Lee YS , Wingfield A , Min NE , Kotloff E , Grossman M , Peelle JE (2018) Differences in Hearing Acuity among “Normal-Hearing” Young Adults Modulate the Neural Basis for Speech Comprehension. eNeuro, 5 , ENEURO.0263-17.2018.
[23] Vaden KI , Kuchinsky SE , Cute SL , Ahlstrom JB , Dubno JR , Eckert MA (2013) The cingulo-opercular network provides word-recognition benefit. J Neurosci, 33 , 18979–18986.24285902
[24] Vaden KI , Kuchinsky SE , Ahlstrom JB , Teubner-Rhodes SE , Dubno JR , Eckert MA (2016) Cingulo-Opercular Function During Word Recognition in Noise for Older Adults with Hearing Loss. Exp Aging Res, 42 , 67–82.26683042
[25] Vaden KI , Matthews LJ , Eckert MA , &amp; Dubno JR (2017) Longitudinal Changes in Audiometric Phenotypes of Age-Related Hearing Loss. J Assoc Res Otolaryngol, 18 , 371–385.27830350
[26] Wild CJ , Yusuf A , Wilson DE , Peelle JE , Davis MH , Johnsrude IS (2012) Effortful listening: the processing of degraded speech depends critically on attention. J Neurosci, 32 , 14010–14021.23035108
[27] Gaskell MG , Marslen-Wilson WD (2002) Representation and competition in the perception of spoken words. Cogn Psychol, 45 , 220–266.12528902
[28] Luce PA , Pisoni DB (1998) Recognizing spoken words: the neighborhood activation model. Ear Hear, 19 , 1–36.9504270
[29] Marslen-Wilson W , Welsh A (1978) Processing interactions. and lexical access during word recognition in continuous speech. Cogn Psychol, 10 , 29–63.
[30] Norris D , McQueen JM (2008) Shortlist B: a Bayesian model of continuous speech recognition. Psychol Rev, 115 , 357–395.18426294
[31] Sommers MS (1996) The structural organization of the mental lexicon and its contribution to age-related declines in spoken-word recognition. Psychol Aging, 11 , 333–341.8795062
[32] Sommers MS , Danielson SM (1999) Inhibitory processes and spoken word recognition in young and older adults: the interaction of lexical competition and semantic context. Psychol Aging, 14 , 458–472.10509700
[33] Taler V , Aaron GP , Steinmetz LG , Pisoni DB (2010) Lexical neighborhood density effects on spoken word recognition and production in healthy aging. J Gerontol B Psychol Sci Soc Sci, 65 , 551–560.20542997
[34] Benichov J , Cox LC , Tun PA , &amp; Wingfield A (2012) Word recognition within a linguistic context: effects of age, hearing acuity, verbal ability, and cognitive function. Ear Hear, 33 , 250–256.21918453
[35] Coren S , Hakstian AR (1994) Predicting speech recognition thresholds from pure tone hearing thresholds. Percept Mot Skills, 79 , 1003–1008.7870486
[36] Townsend TH , Bess FH (1980) Effects of age and sensorineural hearing loss on word recognition. Scand Audiol, 9 , 245–248.7466286
[37] Hughes CP , Berg L , Danziger WL , Coben LA , Martin RL (1982) A new clinical scale for the staging of dementia. Br J Psychiatry, 140 , 566–572.7104545
[38] Morris JC , Storandt M , Miller JP , McKeel DW , Price JL , Rubin EH , Berg L (2001) Mild cognitive impairment represents early-stage Alzheimer disease. Arch Neurol, 58 , 397–405.11255443
[39] McClannahan KS , Chiu YF , Sommers MS , Peelle JE (2021) Test-Retest Reliability of Audiometric Assessment in Individuals With Mild Dementia. JAMA Otolaryngol Head Neck Surg, 147 , 442–449.33662120
[40] Brysbaert M , New B (2009) Moving beyond Kucera and Francis: a critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for American English. Behav Res Methods, 41 , 977–990.19897807
[41] Balota DA , Yap MJ , Cortese MJ , Hutchison KA , Kessler B , Loftis B , Neely JH , Nelson DL , Simpson GB , Treiman R (2007) The English Lexicon Project. Behav Res Methods, 39 , 445–459.17958156
[42] Bates D , Mächler M , Bolker B , Walker S (2015) Fitting linear mixed-effects models using lme4. Journal of Statistical Software, 67 ,1–48.
[43] Fischer ME , Cruickshanks KJ , Nondahl DM , Klein BEK , Klein R , Pankow JS , Tweed TS , Dalton DS , Paulsen AJ (2017) Dichotic Digits Test Performance Across the Ages: Results From Two Large Epidemiologic Cohort Studies. Ear Hear, 38 , 314–320.27941404
[44] Gates GA , Gibbons LE , McCurry SM , McCusrry SM , Crane PK , Feeney MP , &amp; Larson EB (2010). Executive dysfunction and presbycusis in older persons with and without memory loss and dementia. Cogn Behav Neurol, 23 (4 ), 218–223.21150347
[45] Mohammed A , Gibbons LE , Gates G , Anderson ML , McCurry SM , McCormick W , Bowen JD , Grabowski TJ , Crane PK , Larson EB (2022) Association of Performance on Dichotic Auditory Tests With Risk for Incident Dementia and Alzheimer Dementia. JAMA Otolaryngol Head Neck Surg, 148 , 20–27.34647974
[46] Mamo SK , Helfer KS (2021) Speech Understanding in Modulated Noise and Speech Maskers as a Function of Cognitive Status in Older Adults. Am J Audiol, 30 , 642–654.34314238
[47] Jalaei B , Valadbeigi A , Panahi R , Nahrani MH , Arefi HN , Zia M , Ranjbar N (2019) Central Auditory Processing Tests as Diagnostic Tools for the Early Identification of Elderly Individuals with Mild Cognitive Impairment. J Audiol Otol, 23 , 83–88.30727718
[48] Lee SJ , Park KW , Kim LS , Kim H (2018) Association between Frontal-Executive Dysfunction and Speech-in-Noise Perception Deficits in Mild Cognitive Impairment. J Clin Neurol, 14 , 513–522.30198228
[49] Idrizbegovic E , Hederstierna C , Dahlquist M , Kämpfe Nordström ,., Jelic V , Rosenhall U (2011) Central auditory function in early Alzheimer’s disease and in mild cognitive impairment. Age Ageing, 40 , 249–254.21233090
[50] Slade K , Plack CJ , Nuttall HE (2020) The Effects of Age-Related Hearing Loss on the Brain and Cognitive Function. Trends Neurosci, 43 , 810–821.32826080
[51] McLaughlin DJ , Zink ME , Gaunt L , Brent S , Van Engen KJ , Sommers MS , Peelle JE (2021) Pupillometry reveals cognitive demands of lexical competition during spoken word recognition in young and older adults. Psychon Bull Rev, 29 , 268–280.34405386
[52] Dupuis K , Pichora-Fuller MK , Chasteen AL , Marchuk V , Singh G , &amp; Smith SL (2015). Effects of hearing and vision impairments on the Montreal Cognitive Assessment. Neuropsychol Dev Cogn B Aging Neuropsychol Cogn, 22 , 413–437.25325767
[53] Füllgrabe C (2020) On the Possible Overestimation of Cognitive Decline: The Impact of Age-Related Hearing Loss on Cognitive-Test Performance. Front Neurosci, 14 , 454.32581666
[54] Parada JC , Hillyer J , Parbery-Clark A (2020) Performance on the standard and hearing-impaired Montreal Cognitive Assessment in cochlear implant users. Int J Geriatr Psychiatry, 35 , 338–347.31989675
[55] Toner CK , Reese BE , Neargarder S , Riedel TM , Gilmore GC , Cronin-Golomb A (2012) Vision-fair neuropsychological assessment in normal aging, Parkinson’s disease and Alzheimer’s disease. Psychol Aging, 27 , 785–790.22201330
[56] Whitson HE , Cronin-Golomb A , Cruickshanks KJ , Gilmore GC , Owsley C , Peelle JE , Recanzone G , Sharma A , Swenor B , Yaffe K , &amp; Lin FR (2018) American Geriatrics Society and National Institute on Aging Bench-to-Bedside Conference: Sensory Impairment and Cognitive Decline in Older Adults. J Am Geriatr Soc, 66 , 2052–2058.30248173
