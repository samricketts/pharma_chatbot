LICENSE: This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.


7708048
28154
Biom J
Biom J
Biometrical journal. Biometrische Zeitschrift
0323-3847
1521-4036

36642803
10006346
10.1002/bimj.202200021
NIHMS1859085
Article
Bayesian and Influence Function-Based Empirical Likelihoods for Inference of Sensitivity to the Early Diseased Stage in Diagnostic Tests
Hai Yan
Shi Shuangfei
Qin Gengsheng
Alzheimer’s Disease Neuroimaging*
Department of Mathematics and Statistics, Georgia State University, Atlanta GA 30303, U.S.A.
* Gengsheng Qin: gqin@gsu.edu.
17 2 2023
3 2023
15 1 2023
01 3 2024
65 3 e2200021e2200021
This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.
In practice, a disease process might involve three ordinal diagnostic stages: the normal healthy stage, the early stage of the disease, and the stage of full development of the disease. Early detection is critical for some diseases since it often means an optimal time window for therapeutic treatments of the diseases. In this study, we propose a new influence function-based empirical likelihood method and Bayesian empirical likelihood methods to construct confidence/credible intervals for the sensitivity of a test to patients in the early diseased stage given a specificity and a sensitivity of the test to patients in the fully diseased stage. Numerical studies are performed to compare the finite sample performances of the proposed approaches with existing methods. The proposed methods are shown to outperform existing methods in terms of coverage probability. A real data set from the Alzheimer’s Disease Neuroimaging Initiative (ANDI) is used to illustrate the proposed methods.

Bayesian Inference
Confidence Interval
Empirical Likelihood
Influence Function
Sensitivity of the Early Stage

pmc1 Introduction

Medical diagnostic tests are usually used to classify subjects into two stages, the non-diseased stage and the diseased stage. The accuracy of a diagnostic test can be evaluated based on its sensitivity and specificity, i.e., the probabilities of correct diagnoses for diseased subjects and for non-diseased subjects, respectively. Both sensitivity and specificity are not fixed and they are functions of a cut-off value of test results when the diagnostic test result is continuous. The receiver operating characteristic (ROC) curve, a plot of sensitivity vs. (1-specificity), is an important tool in measuring the accuracy of a diagnostic test in a two-class classification problem. It shows the trade-off between sensitivity and specificity as the cut-off point varies through all possible values of the diagnostic test. However, a disease process might be more complicated and involves three diagnostic stages: the non-diseased (healthy) stage, the early diseased stage, and the fully diseased stage. In practice, the sensitivity to early diseased stage is more significant since early detection means optimal therapeutic treatments for some diseases. For example, mild cognitive impairment (MCI) is a transitional stage between the normal levels of cognition and dementia in Parkinson Disease (PD) (Petersen (2004)). And it is important to diagnose patients with MCI because they are more likely to progress to dementia (Aarsland and Kurz (2010)).

To be more specific, let Y1, Y2 and Y3 be the continuous test results of a diagnostic test from the non-diseased, the early diseased, and the fully diseased groups, respectively, and F1, F2 and F3 be the corresponding cumulative distribution functions of the test results. Assume that the higher values of the test results indicate greater severity of the disease. Given a pair of cut-off values c1 and c2 (c1 &lt; c2), a subject is identified as non-diseased if the test result is smaller than c1, as fully diseased if the test result is larger than c2, and as early diseased if the test result is between c1 and c2. The specificity P1 of the test is the correct classification rate of subjects in the non-diseased stage. The sensitivity P2 of the test to the early diseased stage and the sensitivity P3 of the test to the fully diseased stage are the correct classification rates of subjects in the early and fully diseased stages, respectively. P1, P2, and P3 are defined as P1=F1(c1),

P2=F2(c2)−F2(c1)=F2[F3−1(1−P3)]−F2[F1−1(P1)],

P3=1−F3(c2).

Given P1 and P3, c1 and c2 can be determined if F1 and F3 are known distribution functions. Given the specificity P1 and the sensitivity P3 to the fully diseased stage, P2, the sensitivity to the early diseased stage can be formulated as a function of P1 and P3, i.e., P2=P2(P1,P3). The surface in the three-dimensional space (P1, P3, P2) with 0≤P1, P3≤1 is called the ROC surface of the test. Scurfield (1996) proposed a ROC surface analysis. A few years later, Mossman (1999) independently proposed a similar analysis which was implemented using mathematica by Heckerling (2001). A non-parametric estimation of ROC surface was proposed by Nakas and Yiannoutsos (2004) and reshaped later by Xiong et al. (2006), and Li and Zhou (2009). For a disease such as PD, early detection of the disease is usually difficult and crucial because it usually means that patients can receive earlier treatments. Therefore, the probability associated with the early detection of the disease is a very important accuracy measure for the diagnostic test of the disease with three stages. Dong et al. (2011) first provided parametric and nonparametric confidence intervals for P2, the sensitivity to the early diseased stage. However, their approaches fail if the normal assumption for test results cannot be satisfied. Dong and Tian (2015) proposed two empirical likelihood-based (EL) confidence intervals (ELP and ELB) for P2 which can overcome the normal assumption, but their EL ratio statistic for P2 follows a scaled chi-square distribution asymptotically. Thus, an extra step is required to estimate the unknown scale constant for inference on P2 by using density estimation or bootstrap method. Yu et al. (2012) proposed EL methods based on influence functions of parameters of interest. Hai et al. (2020) proposed influence function-based EL method for inference of sensitivity of two-stage diagnostic tests. Motivated by Yu et al. (2012) and Hai et al. (2020), we propose an influence function and EL-based confidence interval for P2 at a given value of the pair (P1, P3), i.e., the sensitivity of the test to the early diseased stage at a specificity and a sensitivity of the test to the fully diseased stage. The idea is to replace the estimating function in the EL with an influence function of the sensitivity to the early stage. The corresponding empirical log-likelihood ratio statistic is shown to converge to a standard chi-square variable, which makes inference on P2 more convenient. Despite wide applications in many areas, EL has only recently been used in Bayesian analysis. Lazar (2003) observed that properties of EL are, in many respects, similar to those of parametric likelihoods, and explained why EL can be used in Bayesian framework like parametric likelihoods. Motivated by Lazar’s work, Bayesian empirical likelihood (BEL) approaches are also developed to construct credible intervals for P2 in this article.

The article is organized as follows. In Section 2, we review Dong and Tian (2015)’s plug-in EL methods for interval estimation of sensitivity to the early stage. In Section 3, we introduce a new EL ratio statistic for sensitivity to the early stage based on influence function. In Section 4, we propose Bayesian EL methods based on influence function. In Section 5, we conduct simulation studies to compare the finite sample performance of the proposed methods with existing methods. In Section 6, a real data set from the Alzheimer’s Disease Neuroimaging Initiative (ANDI) is used to illustrate the proposed methods. In Section 7, we conduct a discussion on estimation of sensitivity to the early stage. The R codes and dataset used in this article are provided as supplementary information.

2 Plug-in empirical likelihood method

Without any underlying distribution assumptions, Dong and Tian (2015) proposed two empirical likelihood-based confidence intervals for P2. They defined an indicator function Φ: Φ(X,Y,Z)={1,X&lt;Y&lt;Z,12,X=Y&lt;ZorX&lt;Y=Z,16,X=Y=Z,0,otherwise,

and a random variable U at given P1 and P3: U(Y)=Φ[F1−1(P1),Y,F3−1(1−P3)],

where Y is a test result of a subject from the early diseased group.

For a given test result Y2 of a test from the early diseased group, the value of U(Y2) can be interpreted as the placement value of Y2 in the non-diseased and fully diseased populations. Let {Y1,j:j=1,2,⋯,n1}, {Y2,j:j=1,2,⋯,n2}, and {Y3,j:j=1,2,⋯,n3} denote the n1, n2, and n3 test results from the non-diseased, early stage, and fully diseased groups, respectively. From the following relationship between U(Y2) and P2, E(U(Y2))=E{Φ[F1−1(P1),Y2,F3−1(1−P3)]}=P[F1−1(P1)&lt;Y2&lt;F3−1(1−P3)]=P[F1−1(P1)&lt;Y2≤F3−1(1−P3)]=P2,

a plug-in empirical likelihood for P2 can be defined as (1) L(P2)=supp{∏j=1n2pj:∑j=1n2pj=1,∑jn2pj(U^j−P2)=0},

where U^j=Φ[F^1−1(P1),Y2,j,F^3−1(1−P3)], j=1,2,…,n2, are plug-in estimates for U(Y2,j)’s, F^1 and F^3 are the empirical distributions of F1 and F3, respectively. Using Lagrange multiplier method, we can easily obtain the expression of pj: p˜j=1n2{1+λ˜(P2)(U^j−P2)}−1

where λ˜(P2) is the solution to 1n2∑j=1n2U^j−P21+λ˜(P2)(U^j−P2)=0.

The corresponding plug-in empirical log-likelihood ratio for P2 is: l(P2)=2∑j=1n2log{1+λ˜(P2)(U^j−P2)}.

Dong and Tian (2015) have shown that the asymptotic distribution of l(P2) is a scaled (with unknown scale constant rP1,P2,P3) chi-square distribution with one degree of freedom. Thus, a 100(1 − α)% level plug-in empirical likelihood-based confidence interval (ELP) for P2 can be constructed as follows: CI1(P2)={P2:r^P1,P2,P3l(P2)≤χ12(1−α)},

where χ12(1−α) is the (1−α)th quantile of χ12, and r^P1,P2,P3 is an estimate for the unknown scale constant: (2) r^P1,P2,P3=P¯^2(1−P¯^2)n2σ^P¯^22

where (3) P¯^2=∑j=1n2I[F^1−1(P1)&lt;Y2,j≤F^3−1(1−P3)]n2,

(4) σ^P¯^22=P¯^2(1−P¯^2)n2+P1(1−P1)n1f^22[F^1−1(P1)]f^12[F^1−1(P1)]+P3(1−P3)n3f^22[F^3−1(1−P3)]f^32[F^3−1(1−P3)],

and f^i is a kernel density estimate for the probability density function fi of Yi, i=1,2,3.

To estimate the density function fi, Dong and Tian (2015) used the “over-smoothed bandwidth selector” by Wand and Jones (1995) to select the bandwidth with a Gaussian kernel function. The performance of this method highly depends on the kernel density estimates whose bandwidths are chosen without a well-recognized standard. Therefore, they proposed the following bootstrap procedure to obtain a bootstrap estimate σ^P¯^22∗ for the variance instead of σ^P¯^22 in Equation (2):

Step 1:	Draw bootstrap resamples of sizes n1, n2, and n3 with replacement from the non-diseased sample Y1j’s, the early diseased sample Y2j’s, and the fully diseased sample Y3j’s, respectively. Denote the bootstrap samples as {Yijb}, i=1,2,3, j=1,2,…,ni.	
Step 2:	Calculate the bootstrap version P¯^2b of P¯^2 according to Equation (3).	
Step 3:	Repeat the first two steps B times to obtain the bootstrap variance estimate of P¯^2 which is defined as	
	σ^P¯^22∗=1B−1∑b=1B(P¯^2b−P¯^¯2∗)2,	
	where P¯^¯2∗=1B∑b=1BP¯^2b.	

This leads to the second 100(1 − α)% level empirical likelihood confidence interval (ELB) for P2: CI2(P2)={P2:rP1,P2,P3∗l(P2)≤χ12(1−α)},

where (5) rP1,P2,P3∗=P¯^¯2∗(1−P¯^¯2∗)n2σ^P¯^22∗.

3 Influence function-based empirical likelihood (IFEL) method

The application of the existing plug-in empirical likelihood-based ELP and ELB intervals for P2 is computationally expensive due to the need for estimation of an unknown scale constant by density estimation and bootstrap process. Moreover, the finite sample performance of ELP and ELB intervals depends on estimation accuracy of the estimators for the scale constant. Hai et al. (2020) initially provided influence function-based EL methods for sensitivity of two-stage diagnostic tests. However, the extension of the results for two-stage in Hai et al. (2020) to three-stage tests is not trivial. Herein, we disclose a new influence function-based EL method, allowing construction of confidence intervals for sensitivity to the early diseased stage to remove the estimation of the unknown scale constant in Dong and Tian (2015). We combine the samples {Yi,j:i=1,2,3;j=1,2,⋯,ni} as: Zl={Y1,l,l=1,…,n1,Y2,l−n1,l=1+n1,…,n1+n2,Y3,l−n1−n2,l=1+n1+n2,…,N,

where N=n1+n2+n3.

Let c1=F1−1(P1), and c2=F3−1(1−P3). Define F^2 as the empirical distribution of F2, c^1=F^1−1(P1) (i.e., the P1-th sample quantile of Y1,j’s), and c^2=F^3−1(1−P3) (i.e., the (1 − P3)-th sample quantile of Y3,j’s). Then the sensitivity P2 of the test to the early diseased stage can be consistently estimated by P˜2=F^2(c^2)−F^2(c^1)=F^2[F^3−1(1−P3)]−F^2[F^1−1(P1)]≡P^2(P1,P3).

We have the following decomposition: (6) P˜2−P2=[F^2(c^2)−F^2(c^1)]−[F2(c2)−F2(c1)]=[F^2(c^2)−F^2(c2)]−[F^2(c^1)−F^2(c1)]+{[F^2(c2)−F^2(c1)]−[F2(c2)−F2(c1)]}≡I1−I2+I3.

The third term of Equation (6) can be written as (7) I3=[F^2(c2)−F^2(c1)]−[F2(c2)−F2(c1)]=1n2∑j=1n2[I(c1&lt;Y2,j≤c2)−P2]=1N∑l=n1+1n1+n2Nn2[I(c1&lt;Zl≤c2)−P2].

From the Bahadur representation for the sample quantiles c^1 and c^2 (Ghosh (1971)), c^1−c1=P1−1n1∑j=1n1I(Y1,j≤c1)f1(c1)+op(n1−12),

c^2−c2=1n3∑j=1n3I(Y3,j&gt;c2)−P3f3(c2)+op(n3−12),

it follows that (8) I2=F^2(c^1)−F^2(c1)=∫[I(y≤c^1)−I(y≤c1)]dF^2(y)=∫[I(y≤c^1)−I(y≤c1)]dF2(y)+op(n2−1/2)=f2(c1)(c^1−c1)+op(n2−1/2+n1−1/2)=−1n1f2(c1)f1(c1)∑j=1n1[I(Y1,j≤c1)−P1]+op(n2−1/2+n1−1/2)=−1N∑l=1n1Nn1f2(c1)f1(c1)[I(Zl≤c1)−P1]+op(N−1/2),

(9) I1=F^2(c^2)−F^2(c2)=∫[I(y≤c^2)−I(y≤c2)]dF^2(y)=∫[I(y≤c^2)−I(y≤c2)]dF2(y)+op(n2−1/2)=f2(c2)(c^2−c2)+op(n2−1/2+n3−1/2)=1n3f2(c2)f3(c2)∑j=1n3[I(Y3,j&gt;c2)−P3]+op(n2−1/2+n3−1/2)=1N∑l=n1+n2+1NNn3f2(c2)f3(c2)[I(Zl&gt;c2)−P3]+op(N−1/2).

Therefore, (10) P˜2−P2=1N∑l=1NWl(P1,P2,P3)+op(N−1/2),

where (11) Wl(P1,P2,P3)={Nn1f2(c1)f1(c1)[I(Zl≤c1)−P1],l=1,…,n1,Nn2[I(c1&lt;Zl≤c2)−P2],l=n1+1,…,n1+n2,Nn3f2(c2)f3(c2)[I(Zl&gt;c2)−P3],l=n1+n2+1,…,N,

is called the influence function of P˜2.

From Equation (10), we can get the following asymptotic distribution of the empirical estimator P˜2 for P2.

Proposition 1:

Assume that F1, F2, and F3 are continuous distribution functions with density functions f1, f2 and f3, respectively, f2′(x) is bounded in neighborhoods of c1=F1−1(P1) and c2=F3−1(1−P3), f1(c1) and f3(c2) are strictly positive, f2(x)f1(x) is bounded in a neighborhood of c1, and f2(x)f3(x) is bounded in a neighborhood of c2. If limn1n2=ρ1(0&lt;ρ1&lt;∞), limn3n2=ρ2(0&lt;ρ2&lt;∞) and limn1n3=ρ3(0&lt;ρ3&lt;∞), then (12) N(P˜2−P2)→dN(0,σ2),

where N=n1+n2+n3, and σ2=(1+ρ1−1+ρ3−1)P1(1−P1)f22[F1−1(P1)]f12[F1−1(P1)]+(1+ρ1+ρ2)P2(1−P2)+(1+ρ2−1+ρ3)P3(1−P3)f22[F3−1(1−P3)]f32[F3−1(1−P3)].

Dong and Tian (2015) derived a similar result to Proposition 1 using a heuristic approach. To derive the influence function of P˜2, we provide a rigorous proof of Proposition 1 in Appendix. Proposition 1 can be used to construct a normal approximation-based confidence interval for P2 if we can get a good estimate for σ2. But estimating σ2 involves estimation of unknown densities and quantiles. To avoid the complex variance estimation, we propose the following influence function-based EL method for inference on P2.

Based on the influence function in (11), an EL for the sensitivity P2 to the early diseased stage at a given pair of (P1, P3) can be defined as follows: (13) LIF(P2)=supp{∏l=1Npl:∑l=1Npl=1,∑l=1NplW^l(P1,P2,P3)=0},

where p=(p1,⋯,pN) is a probability vector, and W^l(P2) is the estimated influence function of P˜2 which is given as follows (14) W^l(P1,P2,P3)={Nn1f^2(c^1)f^1(c^1)[I(Zl≤c^1)−P1],l=1,…,n1,Nn2[I(c^1&lt;Zl≤c^2)−P2],l=n1+1,…,n1+n2,Nn3f^2(c^2)f^3(c^2)[I(Zl&gt;c^2)−P3],l=n1+n2+1,…,N,

where f^ is a density estimator for fi, i=1,2,3. We use the over-smoothed bandwidth selector to select the bandwidth for the Gaussian kernel function for fi as described in Dong and Tian (2015). By Lagrange multiplier, the maximization of Equation (13) is achieved at pl=1N[1+λW^l(P1,P2,P3)]−1,l=1,…,N,

where λ≡λ(P2) is the solution to (15) 1N∑l=1NW^l(P1,P2,P3)1+λW^l(P1,P2,P3)=0.

The corresponding empirical log-likelihood ratio statistic is (16) lIF(P2)=−∑l=1Nlog{1+λW^l(P1,P2,P3)}.

The following theorem establishes the asymptotic distribution of lIF(P2), and a proof is given in Appendix.

Theorem 1:

Assume that F1, F2, and F3 are continuous distribution functions with density functions f1, f2 and f3, respectively, f2′(x) is bounded in neighborhoods of c1=F1−1(P1) and c2=F3−1(1−P3), f1(c1) and f3(c2) are strictly positive, f2(x)f1(x) is bounded in a neighborhood of c1 and f2(x)f3(x) is bounded in a neighborhood of c2. If n1n2=ρ1(0&lt;ρ1&lt;∞), limn3n2=ρ2(0&lt;ρ2&lt;∞) and limn1n3=ρ3(0&lt;ρ3&lt;∞), and P20 is the true value of sensitivity P2 to the early diseased stage at a fixed level P1 of specificity and P3 of sensitivity to the fully diseased stage, then the asymptotic distribution of −2lIF(P20) is a standard chi-squared distribution with one degree of freedom as n1, n2, n3→∞.

From Theorem 1, a 100(1 − α)% level influence function-based (IF) EL confidence interval for P2 can be constructed as follows: CIIF(P2)={P2:−2lIF(P2)≤χ12(1−α)}.

This IF-based EL interval for P2 can be easily obtained using the algorithm for the standard EL method (see Hall and La Scala (1990)) without estimation of the variance in Proposition 1 and the scale constant in the plug-in EL-based method of Dong and Tian (2015).

Remark:

In practice, it is possible that 1+λW^l(P1,P2,P3)&lt;0 for an l(l=1,…,N) with a sample of real test results. Under this situation, the adjusted empirical likelihood and the numerical algorithm by Chen et al. (2008) can be used for the calculation of the empirical log-likelihood ratio statistic of the sensitivity P2.

4 Bayesian Empirical Likelihood (BEL) Method

Bayesian empirical likelihood can be used as the basis for Bayesian inference. As Lazar (2003) pointed out that EL has many asymptotic properties similar to those of parametric likelihoods, Bayesian EL methods are naturally used to quantify uncertainty and can have good small sample properties. With similar motivation to Bayesian EL inference for sensitivity of two-stage diagnostic tests in Hai et al. (2020), we propose two types of Bayesian EL methods for better interval estimation of sensitivity of a three-stage diagnostic test to early stage disease in this section.

4.1 Bayesian empirical likelihood based on sensitivity

We follow Lazar (2003)’s idea to combine empirical likelihood L(P2) with prior π(P2) on P2 by Bayesian theorem to obtain a posterior: π(P2∣data)∝L(P2)π(P2).

We consider reference priors, originally introduced by Bernardo (1979), and further developed in Berger and Bernardo (1992), on P2 in this study. Reference priors only depend on the assumed model and the available data. In our problem, we do not have a parametric model. Therefore, we follow Clarke and Yuan (2010) to derive reference priors for EL. The following proposition gives the reference priors for the Bayesian EL method where [L(P2)]r^P1,P2,P3 in the plug-in EL is used as the likelihood.

Proposition 2:

The reference prior based on the relative entropy for the plug-in EL is πEL,1(P2)=β(32,32),

and the reference prior based on Hellinger distance is πEL,2(P2)=β(12,12),

where β(a,b) is the beta distribution with parameters a and b.

The corresponding posterior is πEL(P2∣Y)∝∏j=1n2[1+λ˜(P2)(U^j−P2)]−r^P1,P2,P3πEL(P2),

where πEL(P2)=πEL,1(P2), or πEL,2(P2), and U^j=Φ[F^1−1(P1),Y2,j,F^3−1(1−P3)], j=1,2,…,n2. Based on these posteriors, we can calculate equal-tail credible intervals for P2. These two new methods are called as Bayesian Empirical Likelihood 1 (BEL1) and Bayesian Empirical Likelihood 2 (BEL2).

Similarly, to construct Bayesian credible intervals for P2 based on influence function, we propose the following reference priors on P2 for the Bayesian EL based on the influence function Wl(P1,P2,P3) in Equation (11): πIF,1(P2)∝[1n1P1(1−P1)f22[F1−1(P1)]f12[F1−1(P1)]+1n2P2(1−P2)+1n3P3(1−P3)f22[F3−1(1−P3)]f32[F3−1(1−P3)]]12,

and πIF,2(P2)∝[1n1P1(1−P1)f22[F1−1(P1)]f12[F1−1(P1)]+1n2P2(1−P2)+1n3P3(1−P3)f22[F3−1(1−P3)]f32[F3−1(1−P3)]]−12.

Both priors are proper since πIF,1(P2) is bounded by a constant and πIF,2(P2) is bounded by a beta distribution. In practice, we use W^l(P1,P2,P3) to estimate the influence function Wl(P1,P2,P3), and replace f1, f2, f3, c1 and c2 with their estimates since they are generally unknown. The posterior based on this approach is then πIF(P2∣Z)∝∏l=1N[1+λ(P2)W^l(P1,P2,P3)]−1πIF(P2),

where πIF(P2)=πIF,1(P2), or πIF,2(P2). Based on these posteriors, we can calculate equal-tail credible intervals for P2. Therefore, we have two more new intervals for P2: the first one is called Bayesian Influence Function-based Empirical Likelihood 1 (BIF1) interval, and the second one is called Bayesian Influence Function-based Empirical Likelihood 2 (BIF2) interval.

4.2 Bayesian pseudo empirical likelihood (BpEL) based on probability vector

Rao and Wu (2010) proposed Bayesian pseudo-EL methods for complex surveys. Inspired by their methods, in this section, we develop Bayesian EL based on probability vector (p1,…,pl) instead of P2. We treat (p1,…,pl) as unknown parameters and the EL function is define as LEL(p1,…,pl)=∏i=1lpi,

where l = n2 for the plug-in EL, and l = N for the influence function-based EL. Consider Dirichlet prior D(α1,…,αl) on (p1,…,pl): π(p1,…,pl)=c(α1,…,αl)∏i=1lpiαi−1,

where c(α1,…,αl)=Γ(∑i=1lαi)/∏i=1lΓ(αi). The posterior distribution of (p1,…,pl) given the data is Dirichlet D(1+α1,…,1+αl) and is given by: π(p1,…,pl∣data)=c(1+α1,…,1+αn)∏i=1lpiαi.

The posterior of the early stage sensitivity P2 satisfies the following equation: (17) ∑i=1lpiQ^i(P2)=0,

where Q^i(P2) is an estimating/influence function and (p1,…,pl) follows the Dirichlet distribution D(1+α1,…,1+αl). In practice, we can generate samples of (p1,…,pl) from D(1+α1,…,1+αl), and by solving Equation (17), we get the posterior samples of P2. Based on these posterior samples, we can calculate the equal-tail credible intervals for sensitivity P2.

Similar to Section 4.1, we consider two types of EL: EL in Equation (1) as in the plug-in EL, and the influence function EL in Equation (13). We call them Bayesian pseudo EL (BpEL) and Bayesian pseudo Influence Function-based EL (BpIF), respectively. For BpEL, we use W^ELP(P1,P2,P3)≡U^j−P2 to replace Q^i(P2) in Equation (17), and consider D(r∗,…,r∗) and D(r∗+1n2,…,r∗+1n2) as the priors (labeled as BpEL1 and BpEL2, respectively), where r∗=r^P1,P2,P3 is the estimate defined in Equation (2) in section 2 for the scale constant. For BpIF, we use W^l(P1,P2,P3) to replace Q^i(P2) in Equation (17), and consider D(1,…,1) and D(1+1N,…,1+1N) as the priors (labeled as BpIF1 and BpIF2, respectively).

5 Simulation Study

Simulation studies are conducted to examine the finite sample performance of the proposed intervals for sensitivity of a test to early stage disease: influence function-based EL (IF) interval, Bayesian influence function EL (BIF1 and BIF2) intervals with reference priors πIF,1(P2) and πIF,2(P2), Bayesian EL (BEL1 and BEL2) intervals with reference priors πEL,1(P2) and πEL,2(P2), Bayesian pseudo influence function EL (BpIF1 and BpIF2) intervals, and Bayesian pseudo EL (BpEL1 and BpEL2) intervals. We compare them to the existing ELP and ELB intervals proposed by Dong and Tian (2015). For the purpose of comparison, we also include the normal approximation (NA) interval based on Proposition 1 by using P˜2=P¯^2 as the point estimate and σ^P¯^22 in Equation (4) as the variance estimate.

We evaluate these approaches under scenarios where the underlying distributions are normal distributions, beta distributions, and the combined scenario where the normality assumptions cannot be met, that is, a gamma distribution for the non-diseased, a log-normal distribution for the early diseased and a Weibull distribution for the fully diseased groups. Sample sizes (n1, n2, n3) are set as (10, 10, 10), (30, 30, 30), (50, 30, 30), (50, 50, 50), (100, 100, 100), (100, 50, 50) and (100, 100, 50). With a fixed 80% or 90% specificity and a fixed 80% or 90% sensitivity to the fully diseased stage, the parameters for the distributions are chosen accordingly so that P2 equals 80% or 90%. Under each distribution scenario, there are four settings corresponding to different levels of P1 and P3, and true value of P2: (i) P1=P3=0.8 and P2=0.8, (ii) P1=P3=0.9 and P2=0.8, (iii) P1=P3=0.8 and P2=0.9, (iv) P1=P3=0.9 and P2=0.9. Under each setting, 5000 random samples are generated. With a simulated sample, it is possible that 1+λW^l(P1,P2,P3)&lt;0 for an l(l=1,…,N), which can occur with not insignificant rate (possibly due to (1) the poor density estimates and (2) the poor estimates for c1 and c2) in the simulation runs when sample sizes are small (i.e., (10,10,10), (30,30,30), (50,50,50)), and it can have some impacts in practical use of the IF-based EL intervals (under this situation, the adjusted empirical likelihood by Chen et al. (2008) can be used for the calculation of the empirical log-likelihood ratio statistic of the sensitivity P2. Please see the Remark at the end of Section 3). We also notice that the new IF-based EL method as well as the existing EL-based ELP and ELB methods all present problems of slow convergence rate (see Appendix for some QQ-plots of the empirical log-likelihood ratio statistics). In our simulation study, −2lIF(P2) is set to be ∞ when one of the terms {1+λW^l(P1,P2,P3)}’s is negative. The simulation results are presented in Tables 2–7 and Figures 1–3.

Under the scenario with normal distributions (see Table 2–3 and Figure 1), we observe that the IF related confidence intervals (IF, BIF1, BIF2, BpIF1 and BpIF2) generally under-cover P2 (the sensitivity of a test to early stage disease). The existing confidence intervals (NA, ELP and ELB) also under-cover P2 except the small sample size (10, 10, 10). New methods always have better performance compared to NA, ELP and ELB methods in all settings considered here. In particular, BEL1 has the best overall performance in terms of coverage probability close to 95%. Bayesian and Bayesian pseudo approaches generally have similar or improved performance over ELP, ELB or IF. IF function related methods have poor performance when sample size is small. The possible reason is that small sample sizes result in the poor density estimation involved in the IF related methods. Comparing the results from the normal distribution settings (i) and (iii) with those from the normal distribution settings (ii) and (iv) which have higher specificity(P1) and sensitivity(P3) to fully diseased group, we can see that the performance of NA, ELB, ELP and IF related methods all depends on the degree of separation of test outcomes in the fully diseased, early diseased and non-diseased groups. Under the higher specificity and sensitivity to fully diseased group, they have lower coverage probabilities. However, the performance of BEL1, BEL2, BpEL1 and BpEL2 does not obviously change with different P1 and P3. Comparing the results from the normal distribution setting (i) with (iii) or normal distribution setting (ii) with (iv) which have fixed P1 and P3 but higher true value of P2, we observe that ELP, ELB, BEL2, BpEL1, IF, BIF1 and BIF2 generally have similar or poorer finite sample performance with higher true value of P2 and other methods perform similarly or slightly better. For example, when sample size is (100,100,100) with P1=P3=0.8, the coverage probability of ELB dropped from 0.944 to 0.906 when the true value of P2 is changed from 0.8 to 0.9.

The simulation results under Beta distribution setting are reported in Table 4 – 5 and Figure 2. Similar as normal distribution settings, we observe that NA, ELP, ELB intervals and FI related intervals (IF, BIF1, BIF2, BpIF1 and BpIF2) generally have under-coverage problems. New intervals always have better performance and BEL1 interval has the best overall performance in terms of coverage probability closed to 95% except the first setting where IF related intervals work well. The performance of NA, ELB, ELP, BpEL1, BpEL2 and IF related intervals all depend on the degree of separation of test outcomes in the fully diseased, early diseased and non-diseased groups. Under higher specificity and sensitivity to the fully diseased group, they have slightly lower coverage probabilities. However, performances of BEL1 and BEL2 intervals do not obviously change when P1 and P3 increase. Comparing the results from the beta distribution setting (i) with (iii) or beta distribution setting (ii) with (iv) which have fixed P1 and P3 but higher true value of P2, we note that BEL1, BpEL1 and BpEL2 intervals generally have similar or better finite sample performance with higher true value of P2, and other intervals perform similarly or slightly worse except BpIF1 and BpIF2 which have no obvious trend. Specifically, BpIF1 and BpIF2 intervals perform very well when P1=P3=0.8 and true P2 = 0.8 and have similar or slightly lower coverage probabilities than that with P2 = 0.9. However, BpIF1 and BpIF2 intervals perform better when the true value of P2 is changed from 0.8 to 0.9.

The simulation results under the combined distribution settings are reported in Table 6–7 and Figure 3. Clearly, the new methods always have better performance compared to ELB and ELP. However, BEL1 does not always have the best overall performance. Bayesian pseudo empirical likelihood methods(BpEL1, BpEL2, BpIF1 and BpIF2) also perform well in most of the settings considered here. IF related methods also work well when true P2 = 0.8. The performance of ELB, ELP and IF related methods all depend on the degree of separation of test outcomes in the fully diseased, early diseased and non-diseased groups. Under higher specificity and sensitivity to the fully diseased group, they have slightly lower coverage probabilities. The performances of NA, BEL1, BEL2, BpEL1 and BpEL2 intervals do not obviously change when P1 and P3 increase. Comparing the results from the combined distribution setting (i) with (iii) or combined distribution setting (ii) with (iv) which have fixed P1 and P3 but higher true value of P2, we can see that BEL1, BEL2, BpEL1, BpEL2, BpIF1 and BpIF2 intervals generally have similar or better finite sample performance with higher true value of P2, and performances of other intervals are obviously worse when the true value of P2 is changed from 0.8 to 0.9, especially the IF and Bayesian IF intervals. The possible reason might be it is more difficult to obtain a better density estimate when the true value of P2 is higher (which means higher degree of separation of early diseased test outcomes from other groups).

In summary, new Bayesian and Bayesian pseudo empirical likelihood intervals, especially BEL1 interval, are consistent and have coverage probabilities closer to the nominal confidence level than other intervals in all settings. The performance of IF and Bayesian IF methods is acceptable in some settings.

6 A Real Example in the Detection of Alzheimer’s Disease

In this section, we illustrate the application of the proposed methods to assess the diagnostic accuracy of biomarkers in the detection of Alzheimer’s disease (AD). The data used in this section was obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). The goal of the ADNI study is to track the progression of the diseases, mild cognitive impairment (MCI) and AD, using biomarkers and clinical measures.

We apply the proposed methods to a small subset of a data-freeze named “QT-PAD Project Data” which was downloaded on June the 29th, 2017. It is available in the “Test Data/Data for Challenges” section of the LONI website (ADNI database). Here, we only consider non-missing records based on the commonly used biomarker: Alzheimer’s Disease Assessment Scale 11 (ADAS11). The dataset we used consists of 203 control subjects (CN), 389 MCI and 237 AD patients. We consider MCI as the early stage of AD. Figure 4 presents the estimated density curves of ADAS11 values from these three groups, respectively. We note that the results of MCI patients are similar to those of control groups. Therefore, we can expect that the sensitivities of the this biomarker to MCI patients cannot be high enough.

In Table 8, we report point estimates and 95% level BEL1, BEL2 intervals for the sensitivity P2 of this biomarker to MCI patients when P1 = P3 = 0.3, P1 = P3 = 0.4, and P1 = P3 = 0.8. As expected, the estimated sensitivity of ADS11 to MCI (the early stage of AD) patients is low when P1 = P3 = 0.8. We observe that ADAS11 has relatively high sensitivities (0.864–0.923) to the MCI patients when P1 and P3 are low (0.4–0.3).

7 Discussion

In this article, we propose an influence function-based EL method, several Bayesian and Bayesian Pseudo EL methods for inference on sensitivity of a test to early stage disease. Our simulation results show that the proposed intervals have better coverage accuracy than the existing intervals. The proposed BEL and BpEL intervals have the best performance among all intervals discussed in this paper. The influence function-based intervals perform slightly worse than the BEL and BpEL intervals.

In practice, clinicians sometimes need to compare the sensitivities of two tests to early stage disease at the same specificity and sensitivity to the late stage of disease, denoted as P21 and P22. Similar to the proposed Bayesian approach, we can generate posterior samples of P21 and P22 separately to obtain posterior samples of (P21 − P22). Based on these posterior samples, Bayesian credible intervals for the difference of the sensitivities of two tests to early stage disease can be easily constructed. In addition, the influence function techniques can be extended immediately to making inference on the difference between the sensitivities of two tests since the influence function of the difference is the difference between the influence functions of two sensitivities to early stage disease.

Supplementary Material

Supp info

Acknowledgements

We thank the associate editor and two reviewers for their helpful comments. Data collection and sharing for this project was funded by the Alzheimer’s Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12-2-0012). ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from the following: AbbVie, Alzheimers Association; Alzheimers Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its affiliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research &amp; Development, LLC.; Johnson &amp; Johnson Pharmaceutical Research &amp; Development LLC.; Lumosity; Lundbeck; Merck &amp; Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Neurotrack Technologies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda Pharmaceutical Company; and Transition Therapeutics. The Canadian Institutes of Health Research is providing funds to support ADNI clinical sites in Canada. Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org). The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimer’s Therapeutic Research Institute at the University of Southern California. ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California.

Appendix: I. Proof of Propositions and Theorem

Proof of Proposition 1.

From (10), we only need to prove that (18) 1σN∑l=1NWl(P1,P2,P3)→dN(0,1).

From (11), we have that 1N∑l=1NWl(P1,P2,P3)=N{1n1f2(c1)f1(c1)∑j=1n1[I(Y1,j≤c1)−P1]+1n2∑j=1n2[I(c1&lt;Y2,j≤c2)−P2]+1n3f2(c2)f3(c2)∑j=1n3[I(Y3,j&gt;c2)−P3]}.

Since I(Y1,j≤c1)‘s∼iidBinomial(1,P1), I(c1&lt;Y2,j≤c2)‘s∼iidBinomial(1,P2), and I(Y3,j&gt;c2)‘s∼iidBinomial(1,P3), by Central Limit Theorem, we have that 1n1∑j=1n1[I(Y1,j≤c1)−P1]→dN(0,P1(1−P1)),

1n2∑j=1n2[I(c1&lt;Y2,j≤c2)−P2]→dN(0,P2(1−P2)),

1n3∑j=1n3[I(Y3,j&gt;c2)−P3]→dN(0,P3(1−P3)).

Hence, (18) and Proposition 1 follows immediately from (10) and the independence of Y1,i’s, Y2,j’s and Y3,k’s.

We need the following lemma for the proof of Theorem 1.

Lemma 1.

Under the conditions in Theorem 1, we have that

(i) 1σN∑l=1NW^l(P1,P2,P3)→dN(0,1).

(ii) 1N∑l=1NW^l2(P1,P2,P3)→pσ2.

Proof.

(i) From (18), we only need to prove that 1N∑l=1NW^l(P1,P2,P3)=1N∑l=1NWl(P1,P2,P3)+op(1).

We have the following decomposition: 1N∑l=1NW^l(P1,P2,P3)=1N∑l=1NWl(P1,P2,P3)+N{1n2∑j=1n2[I(c^1&lt;Y2,j≤c^2)−I(c1&lt;Y2,j≤c2)]}+N{1n1f^2(c^1)f^1(c^1)∑j=1n1[I(Y1,j≤c^1)−P1]−1n1f2(c1)f1(c1)∑j=1n1[I(Y1,j≤c1)−P1]}+N{1n3f^2(c^2)f^3(c^2)∑j=1n3[I(Y3,j&gt;c^2)−P3]−1n3f2(c2)f3(c2)∑j=1n3[I(Y3,j&gt;c2)−P3]}.

As we mentioned in Section 3, using the Bahadur representation of the sample quantiles c^1 and c^2 (Ghosh (1971)), c^1−c1=P1−1n1∑j=1n1I(Y1,j≤c1)f1(c1)+op(n1−12),

c^2−c2=1n3∑j=1n3I(Y3,j&gt;c2)−P3f3(c2)+op(n3−12),

and Equations 8 and 9, we get that (19) 1n2∑j=1n2[I(c^1&lt;Y2,j≤c^2)−I(c1&lt;Y2,j≤c2)]=1n2∑j=1n2[I(Y2,j≤c^2)−I(Y2,j≤c^1)]−1n2∑j=1n2[I(Y2,j≤c2)−I(Y2,j≤c1)]=1n2∑j=1n2[I(Y2,j≤c^2)−I(Y2,j≤c2)]−1n2∑j=1n2[I(Y2,j≤c^1)−I(Y2,j≤c1)]=[F^2(c^2)−F^2(c2)]−[F^2(c^1)−F^2(c1)]=1n3f2(c2)f3(c2)∑j=1n3[I(Y3,j&gt;c2)−P3]+1n1f2(c1)f1(c1)∑j=1n1[I(Y1,j≤c1)−P1]+op(N−1/2).

In addition, we have that (20) 1n1∑j=1n1[I(Y1,j≤c^1)−P1]=1n1∑j=1n1[I(Y1,j≤c^1)−I(Y1,j≤c1)]+1n1∑j=1n1[I(Y1,j≤c1)−P1]=∫[I(y≤c^1)−I(y≤c1)]dF^1(y)+1n1∑j=1n1[I(Y1,j≤c1)−P1]=∫[I(y≤c^1)−I(y≤c1)]dF1(y)+n1−1/2∫[I(y≤c^1)−I(y≤c1)]d(n11/2(F^1(y)−F1(y)))+1n1∑j=1n1[I(Y1,j≤c1)−P1].≡D1+D2+D3.

From n11/2(F^1(y)−F1(y))→dB(y) which is a Gaussian process, and I(y≤c^1)−I(y≤c1)→0 a.s., it follows that (21) D2=n1−1/2∫[I(y≤c^1)−I(y≤c1)]d(n11/2(F^1(y)−F1(y)))=op(n1−1/2).

Using the Bahadur representation for the sample quantile c^1, c^1−c1=Op(n1−1/2), and o(c^1−c1)=op(n1−1/2), we get that (22) 1n1∑j=1n1[I(Y1,j≤c^1)−P1]=D1+op(n1−1/2)+D3=∫[I(y≤c^1)−I(y≤c1)]dF1(y)+op(n1−1/2)+1n1∑j=1n1[I(Y1,j≤c1)−P1]=f1(c1)(c^1−c1)+o(c^1−c1)+1n1∑j=1n1[I(Y1,j≤c1)−P1]+op(n1−1/2)=f1(c1)(P1−1n1∑j=1n1I(Y1,j≤c1)f1(c1)+op(n1−12))+1n1∑j=1n1[I(Y1,j≤c1)−P1]+op(n1−1/2),=f1(c1)⋅op(n1−1/2)+op(n1−1/2)=op(n1−1/2).

Similarly, we have that (23) 1n3∑j=1n3[I(Y3,j&gt;c^2)−P3]=1n3∑j=1n3[I(Y3,j&gt;c^2)−I(Y3,j&gt;c2)]+1n3∑j=1n3[I(Y3,j&gt;c2)−P3]=−∫[I(y≤c^2)−I(y≤c2)]dF^3(y)+1n3∑j=1n3[I(Y3,j&gt;c2)−P3]=−∫[I(y≤c^2)−I(y≤c2)]dF3(y)+op(n3−1/2)+1n3∑j=1n3[I(Y3,j&gt;c2)−P3]=−f3(c2)(c^2−c2)+1n3∑j=1n3[I(Y3,j&gt;c2)−P3]+op(n3−1/2)=−f3(c2)(1n3∑j=1n3I(Y3,j&gt;c2)−P3f3(c2)+op(n3−12))+1n3∑j=1n3[I(Y3,j&gt;c2)−P3]+op(n3−1/2)=−f3(c2)⋅op(n3−1/2)+op(n3−1/2)=op(n3−1/2),

Therefore, 1N∑l=1NW^l(1,P2,P3)=1N∑l=1NWl(P1,P2,P3)+Nn1f^2(c^1)f^1(c^1)∑j=1n1[I(Y1,j≤c^1)−P1]+Nn3f^2(c^2)f^3(c^2)∑j=1n3[I(Y3,j&gt;c^2)−P3]+op(1)=1N∑l=1NWl(P1,P2,P3)+op(1).

Under the assumptions in Theorem 1, kernel density estimators for f^i‘s are almost surely and uniformly consistent (see Silverman (1978)). The last equality holds by the uniform consistency of the density estimates f^1, f^2 and f^, and f2(c1)f1(c1)=O(1), f2(c2)f3(c2)=O(1). Lemma 1(i) is thus proved.

(ii) Since 1N∑l=1NWl2(P1,P2,P3)=Nn12f22(c1)f12(c1)∑j=1n1[I(Y1,j≤c1)−P1]2+Nn22∑j=1n2[I(c1&lt;Y2,j≤c2)−P2]2+Nn32f22(c2)f32(c2)∑j=1n3[I(Y3,j&gt;c2)−P3]2=(1+ρ1−1+ρ3−1)f22(c1)f12(c1)E[I(Y1,j≤c1)−P1]2+(1+ρ1+ρ2)E[I(c1&lt;Y2,j≤c2)−P2]2+(1+ρ2−1+ρ3)f22(c2)f32(c2)E[I(Y3,j&gt;c2)−P3]2+op(1)=(1+ρ1−1+ρ3−1)P1(1−P1)f22(c1)f12(c1)+(1+ρ1+ρ2)P2(1−P2)+(1+ρ2−1+ρ3)f22(c2)f32(c2)P3(1−P3)+op(1)=σ2+op(1),

we only need to prove that 1N∑l=1NW^l2(P1,P2,P3)=1N∑l=1NWl2(P1,P2,P3)+op(1).

Under the assumptions in Theorem 1, using the uniform consistency of the density estimate f^ (Silverman (1978)) and the strong consistency of the sample quantile c^1, we get that |f^1(c^1)−f1(c1)|≤|f^1(c^1)−f1(c^1)|+|f1(c^1)−f1(c1)|≤supx|f^1(x)−f1(x)|+op(1)=op(1).

So, f^1(c^1)=f1(c1)+op(1). Similarly, we have f^3(c^2)=f3(c2)+op(1), f^2(c^1)=f2(c1)+op(1), and f^2(c^2)=f2(c2)+op(1). By Slutsky’s Theorem, we have that f^22(c1)f^12(c1)=f22(c1)f12(c1)+op(1) and f^22(c2)f^32(c2)=f22(c2)f32(c2)+op(1).

From Equation (19) and CLT, it follows that 1n2∑j=1n2[I(c1&lt;Y2,j≤c2)−I(c^1&lt;Y2,j≤c^2)]=−1n3f2(c2)f3(c2)∑j=1n3[I(Y3,j&gt;c2)−P3]−1n1f2(c1)f1(c1)∑j=1n1[I(Y1,j≤c1)−P1]+op(N−1/2)=Op(n1−1/2)+Op(n3−1/2)+op(N−1/2)=Op(N−1/2).

Similarly, from Equation (8) and Bahadur representation of the sample quantile, we have that 1n1∑j=1n1[I(Y1,j≤c1)−I(Y1,j≤c^1)]=f1(c1)(c1−c^1)+op(n1−1/2)=1n1∑j=1n1[I(Y1,j≤c1)−P1]+op(n1−1/2)=Op(n1−1/2)

and 1n3∑j=1n3[I(Y3,j&gt;c2)−I(Y3,j&gt;c^2)]=Op(n3−1/2).

Therefore, |1N∑l=1NWl2(P1,P2,P3)−1N∑l=1NW^l2(P1,P2,P3)|=(N)∣1−2P2n22∑j=1n2[I(c1&lt;Y2,j≤c2)−I(c^1&lt;Y2,j≤c^2)]+P12n2[f22(c1)f12(c1)−f^22(c1)f^12(c1)]+1−2P1n12∑j=1n1{(f22(c1)f12(c1)−f^22(c1)f^12(c1))I(Y1,j≤c1)+f^22(c1)f^12(c1)[I(Y1,j≤c1)−I(Y1,j≤c^1)]}+1−2P3n32∑j=1n3{(f22(c2)f32(c2)−f^22(c2)f^32(c2))I(Y3,j&gt;c2)+f^22(c2)f^32(c2)[I(Y3,j&gt;c2)−I(Y3,j&gt;c^2)]}+P32n3[f22(c2)f32(c2)−f^22(c2)f^32(c2)]|≤op(1),

and Lemma 1(ii) is proved.

Proof of Theorem 1:

From (11), we have that E(Wl(P1,P2,P3))=0, and (24) Var(Wl(P1,P2,P3))={N2n22f22(c1)f12(c1)P1(1−P1),l=1,…,n1,N2n22P2(1−P2),l=n1+1,…,n1+n2,N2n32f22(c2)f32(c2)P3(1−P3),l=n1+n2+1,…,N.

From (24) and the assumptions in Theorem 1, it follows that Var(Wl(P1,P2,P3))&lt;∞. Then, using the similar techniques in the proofs of Lemma 11.1 and Theorem 3.2 in Owen (2001), we get that zero is inside the convex hull of Wl(P1,P20,P3)‘s with probability tending to 1. From the strong consistency of the density estimates f^i’s and the strong consistency of the sample quantile c^i’s, it follows that W^l(P1,P20,P3)=Wl(P1,P20,P3)+o(1), a.s.. Hence, zero is inside the convex hull of W^l(P1,P20,P3)’s with probability tending to 1.

Using Proposition 1 and Lemma 1, Theorem 1 can be proved by using the similar techniques in the proof of Theorem 2 in Gong et al. (2010), or an application of Theorem 1 and subsequent corollaries in Adimari and Guolo (2010). The detail for the proof is omitted here.

Proof of Proposition 2:

We first briefly introduce the approach of Clarke and Yuan (2010). Define the outer product matrix Ω=E[g(Zj,P2)g′(Zj,P2)], Jacobian matrix D(P2)=E[∂g(Zj,P2)/∂P2] and the matrix Λ(P2)=D′(P2)Ω−1(P2)D(P2), where g(Zj,P2) is an estimating function.

For our Bayesian EL approach, g(Zj,P2)=U(Y2,j)−P2, where U(Y) is defined in Section 2, and Ω(P2)=E[g(Zj,P2)]2=E[U(Y2,j)−P2]2=P2(1−P2). Thus we have Λ(P2)=D′(P2)Ω−1(P2)D(P2)=1P2(1−P2).

So the reference prior for the EL under the relative entropy is πEL,1(P2)∝|Λ−1(P2)|1/2=P2(1−P2), i.e., πEL,1(P2)=β(32,32), and the reference prior for the hybrid EL under Hellinger distance is πEL,2(P2)∝|Λ(P2)|1/2=1P2(1−P2),

i.e., πEL,2(P2)=β(12,12).

II. QQ Plots for IF, ELB and ELP based empirical log-likelihood ratio statistics with normal distributions as the underlying distributions

Figure 5 Four scenarios for (P1, P2, P3): 1. P1=P3=0.8 and P2=0.8; 2. P1=P3=0.9 and P2=0.8; 3. P1=P3=0.8 and P2=0.9; 4. P1=P3=0.9 and P2=0.9.

Figure 6 Four scenarios for (P1, P2, P3): 1. P1=P3=0.8 and P2=0.8; 2. P1=P3=0.9 and P2=0.8; 3. P1=P3=0.8 and P2=0.9; 4. P1=P3=0.9 and P2=0.9.

Figure 7 Four scenarios for (P1, P2, P3): 1. P1=P3=0.8 and P2=0.8; 2. P1=P3=0.9 and P2=0.8; 3. P1=P3=0.8 and P2=0.9; 4. P1=P3=0.9 and P2=0.9.

Table 1 List of Acronyms

Acronyms Explanation	
EL	Empirical Likelihood	
ANDI	Alzheimers Disease Neuroimaging Initiative	
ROC	Receiver Operating Characteristic	
AUC	Area Under the ROC Curve	
ELP	Empirical Likelihood	
ELB	Empirical Likelihood based on bootstrap	
IF	Influence Function-based Empirical Likelihood	
BEL1	Bayesian Empirical Likelihood 1	
BEL2	Bayesian Empirical Likelihood 2	
BIF1	Bayesian Influence Function-based Empirical Likelihood 1	
BIF2	Bayesian Influence Function-based Empirical Likelihood 2	
BpEL1	Bayesian Pseudo Empirical Likelihood 1	
BpEL2	Bayesian Pseudo Empirical Likelihood 2	
BpIF1	Bayesian Pseudo Influence Function Empirical Likelihood 1	
BpIF2	Bayesian Pseudo Influence Function Empirical Likelihood 2	

Table 2: Coverage probabilities of various 95% level intervals for P2 under Normal distributions with P2 = 0.8. NA, ELB, ELP and IF intervals are confidence intervals, the other intervals are credible intervals.

(μ1,σ1)=(0,1),(μ2,σ2)=(3,1.2),(μ3,σ3)=(5.858,2),P2=0.8,P1=P3=0.8	
	
Coverage Probability	
Sample Size	NA	ELB	ELP	BEL1	BEL2	BpEL1	BpEL2	IF	BIF1	BIF2	BpIF1	BpIF2	
(10,10,10)	0.798	0.990	0.991	0.982	0.988	0.983	0.966	0.872	0.927	0.919	0.843	0.838	
(30,30,30)	0.896	0.853	0.845	0.970	0.985	0.960	0.949	0.935	0.939	0.939	0.926	0.928	
(50,30,30)	0.872	0.827	0.827	0.957	0.960	0.960	0.947	0.914	0.923	0.925	0.918	0.918	
(50,50,50)	0.909	0.870	0.862	0.964	0.963	0.939	0.935	0.933	0.953	0.943	0.925	0.927	
(100,100,100)	0.939	0.944	0.944	0.950	0.953	0.948	0.944	0.947	0.948	0.946	0.926	0.922	
(100,50,50)	0.914	0.867	0.867	0.959	0.971	0.937	0.932	0.942	0.952	0.945	0.919	0.917	
(100,100,50)	0.912	0.911	0.905	0.960	0.962	0.932	0.926	0.936	0.956	0.944	0.919	0.919	
(μ1,σ1)=(0,1),(μ2,σ2)=(4,1.2),(μ3,σ3)=(7.625,2),P2=0.8,P1=P3=0.9	
	
Coverage Probability	
Sample Size	NA	ELB	ELP	BEL1	BEL2	BpEL1	BpEL2	IF	BIF1	BIF2	BpIF1	BpIF2	
	
(10,10,10)	0.668	0.997	0.994	0.986	0.997	0.989	0.967	0.643	0.839	0.677	0.730	0.723	
(30,30,30)	0.828	0.776	0.783	0.974	0.975	0.930	0.913	0.883	0.852	0.850	0.877	0.874	
(50,30,30)	0.806	0.781	0.759	0.965	0.961	0.920	0.903	0.858	0.836	0.825	0.856	0.854	
(50,50,50)	0.852	0.799	0.796	0.960	0.948	0.914	0.904	0.885	0.908	0.891	0.889	0.887	
(100,100,100)	0.881	0.906	0.906	0.939	0.930	0.925	0.914	0.902	0.920	0.912	0.923	0.923	
(100,50,50)	0.857	0.812	0.791	0.952	0.953	0.892	0.884	0.889	0.899	0.891	0.892	0.891	
(100,100,50)	0.857	0.865	0.842	0.948	0.934	0.878	0.870	0.894	0.901	0.893	0.867	0.866	

Table 3: Coverage probabilities of various 95% level intervals for P2 under Normal distributions with P2 = 0.9. NA, ELB, ELP and IF are confidence intervals, the other intervals are credible intervals.

(μ1,σ1)=(0,1),(μ2,σ2)=(3,1.2),(μ3,σ3)=(6.515,2),P2=0.9,P1=P3=0.8	
	
Coverage Probability	
Sample Size	NA	ELB	ELP	BEL1	BEL2	BpEL1	BpEL2	IF	BIF1	BIF2	BpIF1	BpIF2	
(10,10,10)	0.762	0.980	0.982	0.931	0.979	0.975	0.958	0.866	0.915	0.929	0.829	0.824	
(30,30,30)	0.914	0.827	0.795	0.950	0.985	0.985	0.977	0.906	0.918	0.908	0.932	0.932	
(50,30,30)	0.912	0.805	0.781	0.931	0.977	0.985	0.981	0.871	0.868	0.867	0.936	0.935	
(50,50,50)	0.918	0.816	0.809	0.952	0.982	0.966	0.963	0.931	0.915	0.914	0.944	0.943	
(100,100,100)	0.942	0.908	0.905	0.950	0.970	0.963	0.961	0.961	0.966	0.963	0.955	0.954	
(100,50,50)	0.926	0.812	0.806	0.946	0.980	0.970	0.964	0.931	0.942	0.934	0.942	0.944	
(100,100,50)	0.924	0.882	0.876	0.956	0.977	0.954	0.953	0.956	0.919	0.917	0.945	0.945	
(μ1,σ1)=(0,1),(μ2,σ2)=(4,1.2),(μ3,σ3)=(8.189,2),P2=0.9,P1=P3=0.9	
	
Coverage Probability	
Sample Size	NA	ELB	ELP	BEL1	BEL2	BpEL1	BpEL2	IF	BIF1	BIF2	BpIF1	BpIF2	
	
(10,10,10)	0.619	0.973	0.988	0.944	0.987	0.977	0.960	0.591	0.880	0.670	0.714	0.713	
(30,30,30)	0.849	0.769	0.778	0.948	0.983	0.989	0.986	0.804	0.692	0.708	0.887	0.886	
(50,30,30)	0.845	0.774	0.781	0.937	0.982	0.986	0.979	0.793	0.691	0.713	0.864	0.864	
(50,50,50)	0.864	0.734	0.757	0.952	0.984	0.963	0.955	0.888	0.802	0.808	0.901	0.905	
(100,100,100)	0.899	0.845	0.867	0.944	0.953	0.945	0.940	0.929	0.879	0.875	0.938	0.936	
(100,50,50)	0.882	0.763	0.753	0.935	0.973	0.958	0.953	0.887	0.870	0.866	0.906	0.905	
(100,100,50)	0.878	0.804	0.789	0.957	0.963	0.911	0.905	0.916	0.794	0.797	0.891	0.891	

Table 4: Coverage probabilities of various 95% level intervals for P2 under Beta distributions with P2 = 0.8. NA, ELB, ELP and IF are confidence intervals, the other intervals are credible intervals.

(α1,β1)=(2,6),(α2,β2)=(8,6),(α3,β3)=(21.3,6),P2=0.8,P1=P3=0.8	
	
Coverage Probability	
Sample Size	NA	ELB	ELP	BEL1	BEL2	BpEL1	BpEL2	IF	BIF1	BIF2	BpIF1	BpIF2	
(10,10,10)	0.865	0.990	0.986	0.973	0.987	0.988	0.980	0.885	0.938	0.912	0.855	0.847	
(30,30,30)	0.931	0.907	0.894	0.968	0.976	0.968	0.960	0.961	0.956	0.953	0.950	0.949	
(50,30,30)	0.925	0.904	0.887	0.959	0.975	0.970	0.960	0.962	0.956	0.953	0.942	0.943	
(50,50,50)	0.930	0.926	0.934	0.964	0.961	0.953	0.946	0.948	0.961	0.951	0.941	0.942	
(100,100,100)	0.944	0.972	0.962	0.963	0.957	0.955	0.954	0.954	0.962	0.957	0.963	0.964	
(100,50,50)	0.936	0.946	0.938	0.963	0.969	0.964	0.957	0.955	0.957	0.949	0.954	0.953	
(100,100,50)	0.940	0.970	0.961	0.964	0.957	0.946	0.944	0.948	0.973	0.957	0.961	0.961	
(α1,β1)=(1,6),(α2,β2)=(6,6),(α3,β3)=(15.2,6),P2=0.8,P1=P3=0.9	
	
Coverage Probability	
Sample Size	NA	ELB	ELP	BEL1	BEL2	BpEL1	BpEL2	IF	BIF1	BIF2	BpIF1	BpIF2	
	
(10,10,10)	0.791	0.995	0.992	0.990	0.996	0.994	0.984	0.706	0.772	0.753	0.717	0.714	
(30,30,30)	0.900	0.859	0.808	0.981	0.965	0.934	0.918	0.894	0.894	0.879	0.851	0.848	
(50,30,30)	0.916	0.877	0.852	0.979	0.965	0.931	0.917	0.914	0.915	0.897	0.876	0.878	
(50,50,50)	0.918	0.882	0.844	0.965	0.938	0.910	0.899	0.882	0.918	0.884	0.875	0.868	
(100,100,100)	0.940	0.963	0.924	0.953	0.934	0.921	0.915	0.911	0.917	0.904	0.907	0.901	
(100,50,50)	0.925	0.924	0.902	0.962	0.960	0.937	0.926	0.931	0.913	0.906	0.918	0.916	
(100,100,50)	0.919	0.962	0.928	0.959	0.940	0.916	0.913	0.908	0.964	0.933	0.907	0.904	

Table 5: Coverage probabilities of various 95% level intervals for P2 under Beta distributions with P2 = 0.9. NA, ELB, ELP and IF are confidence intervals, the other intervals are credible intervals.

(α1,β1)=(2,6),(α2,β2)=(8,6),(α3,β3)=(31.8,6),P2=0.9,P1=P3=0.8	
	
Coverage Probability	
Sample Size	NA	ELB	ELP	BEL1	BEL2	BpEL1	BpEL2	IF	BIF1	BIF2	BpIF1	BpIF2	
(10,10,10)	0.728	0.985	0.981	0.929	0.988	0.988	0.983	0.813	0.921	0.893	0.777	0.772	
(30,30,30)	0.920	0.817	0.822	0.955	0.984	0.987	0.987	0.850	0.850	0.850	0.921	0.920	
(50,30,30)	0.923	0.824	0.805	0.947	0.974	0.983	0.981	0.860	0.848	0.849	0.914	0.916	
(50,50,50)	0.914	0.815	0.815	0.951	0.982	0.969	0.965	0.918	0.909	0.902	0.919	0.918	
(100,100,100)	0.940	0.922	0.919	0.955	0.969	0.959	0.956	0.956	0.962	0.954	0.963	0.963	
(100,50,50)	0.940	0.843	0.836	0.954	0.980	0.961	0.956	0.936	0.967	0.950	0.947	0.948	
(100,100,50)	0.937	0.910	0.917	0.953	0.975	0.964	0.964	0.952	0.923	0.921	0.963	0.961	
(α1,β1)=(1,6),(α2,β2)=(6,6),(α3,β3)=(20.4,6),P2=0.9,P1=P3=0.9	
	
Coverage Probability	
Sample Size	NA	ELB	ELP	BEL1	BEL2	BpEL1	BpEL2	IF	BIF1	BIF2	BpIF1	BpIF2	
	
(10,10,10)	0.787	0.966	0.989	0.956	0.986	0.987	0.972	0.636	0.896	0.721	0.686	0.679	
(30,30,30)	0.889	0.786	0.800	0.960	0.986	0.990	0.989	0.816	0.808	0.801	0.901	0.901	
(50,30,30)	0.906	0.789	0.782	0.945	0.974	0.981	0.977	0.820	0.807	0.811	0.901	0.902	
(50,50,50)	0.903	0.745	0.803	0.958	0.980	0.950	0.944	0.909	0.894	0.885	0.918	0.916	
(100,100,100)	0.931	0.877	0.924	0.961	0.973	0.962	0.958	0.954	0.969	0.956	0.951	0.950	
(100,50,50)	0.911	0.816	0.827	0.959	0.979	0.957	0.954	0.929	0.929	0.910	0.929	0.929	
(100,100,50)	0.929	0.874	0.913	0.966	0.968	0.954	0.955	0.944	0.917	0.915	0.941	0.939	

Table 6: Coverage probabilities of various 95% level intervals for P2 under combined distributions with P2 = 0.8. NA, ELB, ELP and IF are confidence intervals, the other intervals are credible intervals.

Gamma(α,β)=(4,10),LN(μ,σ)=(1,0.5),Weibull(a,b)=(4.07,6),P2=0.8,P1=P3=0.8	
	
Coverage Probability	
Sample Size	NA	ELB	ELP	BEL1	BEL2	BpEL1	BpEL2	IF	BIF1	BIF2	BpIF1	BpIF2	
(10,10,10)	0.852	0.983	0.982	0.957	0.979	0.986	0.970	0.876	0.914	0.895	0.869	0.871	
(30,30,30)	0.920	0.902	0.911	0.968	0.977	0.952	0.940	0.944	0.936	0.934	0.936	0.934	
(50,30,30)	0.921	0.903	0.910	0.970	0.970	0.947	0.938	0.940	0.929	0.934	0.938	0.932	
(50,50,50)	0.926	0.930	0.938	0.951	0.959	0.948	0.942	0.948	0.958	0.948	0.945	0.944	
(100,100,100)	0.938	0.957	0.960	0.961	0.957	0.957	0.953	0.956	0.959	0.958	0.950	0.948	
(100,50,50)	0.928	0.924	0.941	0.964	0.952	0.941	0.939	0.939	0.959	0.955	0.938	0.937	
(100,100,50)	0.914	0.946	0.956	0.964	0.959	0.954	0.957	0.953	0.950	0.941	0.948	0.952	
Gamma(α,β)=(4,10),LN(μ,σ)=(0.5,0.5),Weibull(a,b)=(2.8,6),P2=0.8,P1=P3=0.9	
	
Coverage Probability	
Sample Size	NA	ELB	ELP	BEL1	BEL2	BpEL1	BpEL2	IF	BIF1	BIF2	BpIF1	BpIF2	
	
(10,10,10)	0.754	0.996	0.997	0.991	0.995	0.993	0.989	0.719	0.795	0.721	0.788	0.787	
(30,30,30)	0.886	0.850	0.852	0.991	0.981	0.955	0.949	0.928	0.922	0.916	0.908	0.910	
(50,30,30)	0.870	0.840	0.849	0.982	0.973	0.946	0.934	0.910	0.897	0.896	0.901	0.897	
(50,50,50)	0.907	0.886	0.905	0.971	0.967	0.945	0.939	0.934	0.949	0.940	0.928	0.928	
(100,100,100)	0.917	0.954	0.952	0.970	0.962	0.949	0.946	0.944	0.955	0.949	0.937	0.938	
(100,50,50)	0.891	0.871	0.890	0.968	0.953	0.931	0.926	0.928	0.939	0.930	0.921	0.920	
(100,100,50)	0.887	0.926	0.916	0.970	0.947	0.932	0.929	0.930	0.938	0.925	0.924	0.924	

Table 7: Coverage probabilities of various 95% level intervals for P2 under combined distributions with P2 = 0.9. NA, ELB, ELP and IF are confidence intervals, the other intervals are credible intervals.

Gamma(α,β)=(4,10),LN(μ,σ)=(1,0.5),Weibull(a,b)=(4.07,7.49),P2=0.9,P1=P3=0.8	
	
Coverage Probability	
Sample Size	NA	ELB	ELP	BEL1	BEL2	BpEL1	BpEL2	IF	BIF1	BIF2	BpIF1	BpIF2	
(10,10,10)	0.689	0.977	0.990	0.923	0.978	0.976	0.970	0.764	0.836	0.805	0.745	0.736	
(30,30,30)	0.911	0.820	0.810	0.974	0.990	0.995	0.994	0.838	0.831	0.827	0.932	0.933	
(50,30,30)	0.910	0.793	0.829	0.952	0.982	0.986	0.984	0.821	0.805	0.804	0.927	0.928	
(50,50,50)	0.923	0.833	0.853	0.946	0.968	0.950	0.949	0.914	0.892	0.897	0.936	0.939	
(100,100,100)	0.940	0.946	0.922	0.967	0.974	0.965	0.960	0.965	0.964	0.962	0.960	0.959	
(100,50,50)	0.928	0.818	0.849	0.956	0.982	0.952	0.948	0.903	0.925	0.924	0.933	0.932	
(100,100,50)	0.900	0.917	0.917	0.961	0.972	0.957	0.956	0.955	0.884	0.884	0.948	0.948	
Gamma(α,β)=(4,10),LN(μ,σ)=(1,0.5),Weibull(a,b)=(4.25,6),P2=0.9,P1=P3=0.9	
	
Coverage Probability	
Sample Size	NA	ELB	ELP	BEL1	BEL2	BpEL1	BpEL2	IF	BIF1	BIF2	BpIF1	BpIF2	
	
(10,10,10)	0.629	0.987	0.989	0.937	0.990	0.985	0.981	0.583	0.765	0.605	0.670	0.668	
(30,30,30)	0.906	0.798	0.816	0.959	0.983	0.990	0.988	0.805	0.745	0.751	0.910	0.910	
(50,30,30)	0.900	0.812	0.805	0.958	0.984	0.991	0.988	0.801	0.740	0.751	0.914	0.914	
(50,50,50)	0.904	0.822	0.807	0.958	0.979	0.957	0.952	0.904	0.852	0.855	0.925	0.924	
(100,100,100)	0.926	0.924	0.929	0.958	0.961	0.948	0.942	0.944	0.932	0.930	0.940	0.939	
(100,50,50)	0.910	0.822	0.819	0.960	0.977	0.949	0.949	0.884	0.842	0.849	0.929	0.929	
(100,100,50)	0.895	0.905	0.918	0.973	0.965	0.947	0.947	0.946	0.834	0.839	0.937	0.939	

Table 8 Point estimates and 95% level credible intervals for the sensitivity P2 of ADAS11 to MCI patients

	Point estimate	CI	
	
		P1 = P3 = 0.3	
BEL1	0.923	(0.846, 0.931)	
BEL2	0.923	(0.851, 0.934)	
	
		P1 = P3 = 0.4	
	
BEL1	0.864	(0.759, 0.871)	
BEL2	0.864	(0.763, 0.874)	
	
		P1 = P3 = 0.8	
	
BEL1	0.280	(0.120, 0.299)	
BEL2	0.280	(0.113, 0.290)	

Figure 1 Boxplots of coverage probabilities under normal distribution setting.

Figure 2 Boxplots of 95% coverage probabilities under Beta distribution setting.

Figure 3 Boxplots of 95% coverage probabilities under the combined distribution setting.

Figure 4 Estimated densities of ADAS11 values in the ADNI data.

Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf

Conflict of Interest

The authors have declared no conflict of interest.


References

Aarsland D , and Kurz MW (2010) The epidemiology of dementia associated with Parkinson’s disease. Brain Pathology 20 , 633–639.20522088
Adimari G , and Guolo A (2010) A note on the asymptotic behaviour of empirical likelihood statistics. Statistical Methods &amp; Applications, 19 , 463–476.
Berger JO , and Bernardo JM (1992) On the development of reference priors. Bayesian statistics 4 , 35–60.
Berger JO , Bernardo JM , and Sun D (2009) The formal definition of reference priors. The Annals of Statistics 37 , 905–938.
Bernardo JM (1979) Reference posterior distributions for Bayesian inference. Journal of the Royal Statistical Society. Series B (Methodological) 41 ,113–147.
Chen J , Variyath AM , and Abraham B (2008) Adjusted empirical likelihood and its properties. Journal of Computational and Graphical Statistics 17 , 426–443.
Clarke B , and Yuan A (2010) Reference priors for empirical likelihoods. In Chen MH , Mueller P , Sun D , Ye K &amp; Dey DK (eds.). Frontiers of Statistical Decision Making and Bayesian Analysis: In honor of James O. Berger, pages 56–68. New York: Springer; 2010.
Dong T , Tian L , Hutson A , and Xiong C (2011) Parametric and non-parametric confidence intervals of the probability of identifying early disease stage given sensitivity to full disease and specificity with three ordinal diagnostic groups. Statistics in Medicine 30 , 3532–3545.22139763
Dong T , and Tian L (2015) Confidence interval estimation for sensitivity to the early diseased stage based on empirical likelihood. J Biopharm Stat. 25 , 1215–1233.25372999
Ghosh JK (1971) A new proof of the Bahadur representation of quantiles and an application. The Annals of Mathematical Statistics 42 , 1957–1961.
Gong Y , Peng L , and Qi Y (2010) Smoothed jackknife empirical likelihood method for ROC curve. Journal of Multivariate Analysis 6 , 1520–1531.
Hai Y , Min X , and Qin G (2020) Bayesian and influence function-based empirical likelihoods for inference of sensitivity in diagnostic tests. Statistical Methods in Medical Research 29 , 3457–3491.32552342
Hall P , La Scala B (1990) Methodology and algorithm of empirical likelihood. International Statistical Review 58 , 109–127.
Heckerling PS (2001) Parametric three-way receiver operating characteristic surface analysis using Mathematica. Medical Decision Making 21 , 409–417 11575490
Lazar NA (2003) Bayesian empirical likelihood. Biometrika 90 ,319–326.
Li J , and Zhou XH (2009) Nonparametric and semiparametric estimation of the three way receiver operating characteristic surface. Journal of Statistical Planning and Inference 139 , 4133–4142.
Mossman D (1999) Three-way ROCs. Medical Decision Making 1999; 19 , 78–79.
Nakas CT , and Yiannoutsos CT (2004) Ordered multiple-class ROC analysis with continuous measurements. Statistics in Medicine 23 , 3437–3449.15505886
Owen A (1990) Empirical likelihood ratio confidence regions. The Annals of Statistics 18 , 90–120.
Owen A (2001) Empirical likelihood. Boca Raton: Chapman &amp; Hall/CRC
Petersen RC (2004) Mild Cognitive Impairment as a Diagnostic Entity. Journal of Internal Medicine 256 , 183–194.15324362
Rao JNK , and Wu C (2010) Bayesian pseudo-empirical-likelihood intervals for complex surveys. Journal of the Royal Statistical Society. Series B (Methodological) 72 , 533–544.
Scurfield BK (1996) Multiple-event forced-choice tasks in the theory of signal detectability. Journal of Mathematical Psychology 40 , 253–269.8979976
Silverman BW (1978) Weak and strong uniform consistency of the kernel estimate of a density and its derivatives. The Annals of Statistics 6 (1 ), 177–184.
Wand MP , and Jones MC (1995) Kernel smoothing. New York: Chapman &amp; Hall/CRC
Xiong C , Belle G van , Philip M , John CM (2006) Measuring and estimating diagnostic accuracy when there are three ordinal diagnostic groups. Statistics in Medicine 25 , 1251–1273.16345029
Yu W , Zhao Z , and Zheng M (2012) Empirical likelihood methods based on influence functions. Statistics and Its Interface 5 , 355–366.
