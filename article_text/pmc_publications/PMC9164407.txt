LICENSE: This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.


101317307
33873
Stat Probab Lett
Stat Probab Lett
Statistics &amp; probability letters
0167-7152

35665309
9164407
10.1016/j.spl.2021.109100
NIHMS1755478
Article
A goodness-of-fit test based on neural network sieve estimators
Shen Xiaoxi a
Jiang Chang a
Sakhanenko Lyudmila b
Lu Qing ab*
a Department of Biostatistics, University of Florida, Gainesville, FL, USA
b Department of Statistics and Probability, Michigan State University, East Lansing, MI, USA
* Corresponding author at: Department of Biostatistics, University of Florida, Gainesville, FL, USA. lucienq@ufl.edu (Q. Lu).
10 11 2021
7 2021
26 3 2021
04 6 2022
174 109100This file is available for text mining. It may also be used consistent with the principles of fair use under the copyright law.
Neural networks have become increasingly popular in the field of machine learning and have been successfully used in many applied fields (e.g., imaging recognition). With more and more research has been conducted on neural networks, we have a better understanding of the statistical proprieties of neural networks. While many studies focus on bounding the prediction error of neural network estimators, limited research has been done on the statistical inference of neural networks. From a statistical point of view, it is of great interest to investigate the statistical inference of neural networks as it could facilitate hypothesis testing in many fields (e.g., genetics, epidemiology, and medical science). In this paper, we propose a goodness-of-fit test statistic based on neural network sieve estimators. The test statistic follows an asymptotic distribution, which makes it easy to use in practice. We have also verified the theoretical asymptotic results via simulation studies and a real data application.

Donsker class
Nonparametric least squares

pmc1. Introduction

Deep neural networks have become one of the most popularly used methods in artificial intelligence. Despite its attractive performance in various applications, as a statistical model, it is important to investigate its statistical inference. Due to the unidentifiability of the parameters in the neural network model, which were mentioned in Fukumizu (1996, 2003), classical tests such as Wald test and likelihood ratio test may not work since unidentifiability leads to inconsistency for the parameter estimators (Wu, 1981).

Most existing literature on asymptotic properties of neural networks are based on the nonparametric regression. For example, Chen and Shen (1998) and Shen et al. (2019) developed the rate of convergence of neural network estimators under the random design and the fixed design, respectively. Compared with other commonly used nonparametric estimation methods, such as the Nadaraya–Watson estimator and spline regression, neural networks have advantages in terms of rate of convergence. For instance, it has been shown in Györfi et al. (2006) that the mean integrated squared error (MISE) for Nadaraya–Watson estimator is E[‖f^n−f0‖2]=O(n−22+d) when the true function f0 is L-Lipschitz and the bandwidth h ≍ n−1/(2+d). Györfi et al. (2006) also showed that when the underlying function f0 has continuous pth order derivative and the degree of B-splines is chosen to be p − 1, the MISE for spline estimator is E[‖f^n−f0‖2]=O((logn/n)2p2p+d). From the order of rate of convergence, both methods suffer from the curse of dimensionality. On the other hand, it has been shown in Chen and Shen (1998) that for sufficiently smooth true function f0, neural network estimators have ‖f^n−f0‖=Op((n/logn)−1+1/d4(1+1/(2d))). Therefore, neural network estimators can, in some sense, avoid the curse of dimensionality. In the supplementary material, we provide a simple comparison of three types of estimators in estimating a trigonometric function.

While the rate of convergence of neural networks has been studied in previous literature, limited research has been done on the hypothesis testing of neural networks. In Shen et al. (2019), asymptotic normality has been derived for neural network sieve estimators, which can be used to test whether the underlying function has a certain specific form. However, in real data applications, we generally do not know the underlying function, which makes the results hard to apply. Moreover, researchers are often more interested in testing the significant association of multiple covariates with the response of interest. Recently, Horel and Giesecke (2020) proposed a significance test based on neural networks. However, the asymptotic distribution is complicated, which may hinder its use in real data analysis. These issues motive us to develop a new nonparametric significance testing procedure based on neural network sieve estimators. As we demonstrated in the real data application, the new goodness-of-fit test can be used in practical research, such as genetic research. Therefore, one of the importance of our work is to bridge the gap between the theoretical work on neural networks and practical research.

We consider a similar setup to the one used in Shen et al. (2019) with the exception of using the random design. Suppose that (X1, Y1), …, (Xn, Yn) are n pairs of i.i.d. samples generated from the following true nonparametric regression model: Yi=f0(Xi)+ϵ, i=1,…,n,

where Xi∈X⊂ℝd, i = 1, …, n and X is a compact subset in ℝd; ϵ1, …, ϵn are i.i.d. random errors independent of X1, …, Xn with E[ϵ]=0  and E[ϵ2]=σ2&lt;∞. It is clear that under the quadratic error loss, f0(x)=E[Y∣X=x]=argminf∈FE[(Y−f(X))2∣X=x],

and f0 is a minimizer of the population criterion function Q(f)=E[(Y−f(X))2]=σ2+E[(f(X)−f0(X))2].

Suppose that F is some function space containing f0. Throughout the paper, we assume that F⊂C(X)∩L2(X,μ) and the pseudo-metric considered on F is the classical L2-metric, that is, for any f∈F, ∥f∥2=∫Xf2(x)dμ(x). Under the framework of empirical risk minimization (ERM) (Vapnik, 1998; Devroye et al., 2013), an estimator for f0 is the one that minimizes the empirical loss function ℚn(f)=1n∑i=1n(Yi−f0(Xi))2, that is, f^n=argminf∈Fℚn(f)=argminf∈F1n∑i=1n(Yi−f(Xi))2.

As for the sieve extremum estimator based on a neural network, we define (1) Frn={α0+∑j=1rnαjσ(γjTx+γ0,j):γj∈ℝd,αj,γ0,j∈ℝ,∑j=0rn|αj|≤Vn for some Vn&gt;4 and max1≤j≤rn∑i=0d|γi,j|≤Mn for some Mn&gt;0},

where rn, Vn, Mn ↑ ∞ as n → ∞ and σ(·) is the standard sigmoid function (σ(X)=(1+e−X)−1). The requirement of Vn &gt; 4 is necessary for computing the covering number for Frn, and 4 is the reciprocal of the Lipschitz constant of σ. Due to the Universal Approximation Theorem (Hornik et al., 1989), Frn is nondecreasing and ∪rn=1∞Frn is dense in F under the sup-norm. With some abuse of notation, the sieve extremum estimator f^n is defined as (2) ℚn(f^n)≤inff∈Frnℚn(f)+op(n−1),

where ηn → 0 as n → ∞.

The goal of this paper is to derive a goodness-of-fit statistic to test hypothesis on whether a subset of the covariates is significant, that is, for a given p &gt; 0, we test (3) H0:f0∈C(X′)∩L2(X′,μ) vs H1:f0∈C(X)∩L2(X,μ),

where X′ is a compact subset in ℝd−p and C(A) is the space of continuous function on A. In other words, the statistic is proposed to test is whether the p covariates absent in X′ are significantly associated with the response of interest.

The rest of the paper is organized as follows. Section 2 reviews the necessary concepts and results from the empirical process theory. The main theoretical results and the process of constructing the test statistic are discussed in Section 3. A simulation study is conducted in Section 4 to verify the conditions proposed in the main results followed by a real data application to Alzheimer disease. Additional simulation results and the proofs of the results in the main text are given in the supplementary materials.

Notations:

Throughout the paper, bold font alphabetic letters and Greek letters are vectors. We use ∥ · ∥sup to denote the sup-norm, that is ‖f‖sup= supx|f(x)|. For a pseudo-metric space (T, d), N(ϵ, T, d), D(ϵ, T, d) and N[](ϵ, T, d) represent the covering number, packing number, and bracketing number, respectively. The natural logarithm of the covering number is denoted by H(ϵ, T, d), which is also known as the entropy number.

2. Preliminaries

The main tool used in proofs is the Donsker class from the empirical process theory. In this section, we review the definition of Donsker class as well as a way to check whether a function class is Donsker. More details on Donsker class can be found in Dudley (1984) and van der Vaart and Wellner (1996).

Definition 1 (Donsker Class).

Let (X, A, P) be a probability space and GP be a Gaussian process indexed by L2 (X, A, P) with zero mean and covariance E[GP(f)GP(g)]=P(fg)−Pf⋅Pg for all f,g∈L2(X,A,P).

Define ρP(f,g)=(E[(GP(f)−GP(g))2])1/2.

A class F⊂L2(X,A,P) is called a GPBUC class (or pre-Gaussian) if and only if the process GP(f, ω) can be chosen so that for all ω, the sample functions f ↦ GP(f, ω), restricted to f∈F, are bounded and uniformly continuous for ρP.

A class F⊂L2(X,A,P) is called a Donsker class (for P) if and only if it is a GPBUC class and there are processes Yj(f, ω), f∈F, ω ∈ Ω, where Yj are independent copies of GP with f ↦ Yj(f, ω) bounded and ρP-uniformly continuous on F for each j, such that for every ϵ &gt; 0, ℙ*(n−1/2maxm≤nsupf∈F|∑j=1mf(Xj)−Pf−Yj(f)|&gt;ϵ)→0 as n→∞.

A common approach to check whether a class F is a Donsker class is to use the Dudley integral based on the bracketing number.

Theorem 1 (Theorem 3.1 in Ossiander (1987)).

If F is a class of measurable functions with (4) ∫0∞(logN[](ϵ,F,∥⋅∥L2(P)))1/2dϵ&lt;∞,

then F is a Donsker class.

Dudley (1984) provides a relationship between the bracketing number and packing number: N[](2ϵ,F,∥⋅∥∞)≤2D(ϵ,F,∥⋅∥sup ).

It then follows from the duality of packing number and covering number (see Lemma 5.5 in Wainwright (2019)) that (5) N[](2ϵ,F,∥⋅∥∞)≤2D(ϵ,F,∥⋅∥sup )≤2N(ϵ2,F,∥⋅∥sup ).

Based on these facts, we verify in the following proposition that Frn, Grn={y−f(x):f∈Frn} and Hrn={(y−f(x)2:f∈Frn)} are all Donsker classes for each fixed n.

Proposition 2.

For each fixed n, the following classes of functions are Donsker classes: Frn,

Grn={y−f(x):f∈Frn}, and

Hrn={(y−f(x)2:f∈Frn)}.

3. Main results

The theory that derives the asymptotic distribution of a goodness-of-fit test statistic depends heavily on Lemma 2 in Yatchew (1992). The lemma, however, requires strong uniform consistency for nonparametric least squares estimators. Therefore, we modify the lemma under a weaker consistency assumption.

Lemma 3.

Let H be a Donsker class. Let h0 and h^n, n = 1, 2, … be in H, where ‖ h^n−h0‖L2(P)→P0, then n1/2[1n∑i=1nh^n(Zi)−Ph^n(Z)]−n1/2[1n∑i=1nh0(Zi)−Ph0(Z)]→P0.

As Hrn is a Donsker class from Proposition 2. Based on Lemma 3, we can construct a goodness-of-fit statistic.

Theorem 4.

Let E[ϵ4]&lt;∞, then if ‖πrnf0−f0‖=o(n−1/4) and [rn(d+2)+1]Vn4log(Vn[rn(d+2)+1])=o(n),

we have n1/2κ1/2[1n∑i=1n(Yi−f^n(Xi))2−σ2]→dN(0,1),

where κ = Var[ϵ2].

Remark 1.

If the underlying function f0 is real analytic, it follows from a combination of the results in Goulaouic (1971) and Lemma 3.2 in Mhaskar (1996) that the approximation rate can decay exponentially so that the condition ‖πrnf0−f0‖=o(n−1/4) satisfies easily.

Theorem 4 provides a theoretical justification for constructing the following goodness-of-fit test statistic for hypothesis testing. The test statistic is formed based on the following steps.

Step 1. Partition the sample (X1, Y1), …, (Xn, Yn) into two equal parts. For simplicity, we assume that n = 2m for some m &gt; 0.

Step 2. From Theorem 4, under the null hypothesis H0:f0∈C(X′)∩L2(X′,μ), we have T0=m1/2κ1/2[1m∑i=1m(Yi−f^n,H0(Xi))2−σ2]→dN(0,1),

T1=m1/2κ1/2[1m∑i=m+1n(Yi−f^n,H1(Xi))2−σ2]→dN(0,1),

where f^n,H0 is the neural network sieve extremum estimator obtained by using the first m samples and the covariates excluding the p covariates, and fn,H1 is the neural network sieve extremum estimator calculated based on the remaining samples and all of the covariates.

Step 3. Since (X1, Y1), …, (Xn, Yn) are independent, T0 and T1 are also independent. Then T2=T0−T1=m1/2κ1/2[1m∑i=1m(Yi−f^n,H0(Xi))2−1m∑i=m+1n(Yi−f^n,H1(Xi))2]→dN(0,2).

Step 4. For any consistent estimator κ^n of κ, it then follows from the Slutsky’s Theorem that T=m1/2κ^n1/2[1m∑i=1m(Yi−f^n,H0(Xi))2−1m∑i=m+1m(Yi−f^n,H1(Xi))2]→dN(0,2).

As mentioned in Yatchew (1992), a possible choice for κ^n is κ^n=1n∑i=1n(Yi−f^n,H0(Xi))4−σ^4,

where σ^2=n−1∑i=1n(Yi−f^n,H0(Xi))2.

4. Simulation

We have conducted a simulation study to verify the main result established in the previous section. Suppose that the response variables Y1, …, Yn are generated from the following model, Yi=f0(Xi)+ϵi, i=1,…,n,

where ϵ1,…,ϵn~ i.i.d. N(0,1). The covariates X1,…,Xn∈ℝ2 are i.i.d. samples from N2(0,I2), where I2 is the 2-dimensional identity matrix. The hypothesis of interest is whether X(2), the second element in the vector X, is significant. Under the null hypothesis H0, the true function is chosen to be a trigonometric function: f0(x)=sin(π3x(1))+13cos(π4x(1)+1).

Based on the testing procedure we established the previous section, the test statistic can be written as Tn=n/2(2κ^)1/2[2n∑i=1n/2(Yi−f^n,H0(Xi))2−2n∑i=n/2+1n/2(Yi−f^n,H1(Xi))2],

where κ^=Var^[ϵ2]=1n∑i=1n(Yi−f^n,H0(Xi))4−[1n∑i=1n(Yi−f^n,H0(Xi))2]2.

Due to the constraints in the neural network sieve Frn, we used a subgradient method discussed in Section 7 in Boyd and Mutapcic (2008) to estimate the parameters and obtain the fitted function. As mentioned in Boyd and Mutapcic (2008), the algorithm converges when the step size δk is diminishing nonsummable (δk ↓ 0 and ∑k=1∞δk=∞). We thus chose the step size for the kth iteration in the subgradient to be δk = 0.1/log(e + k) and the number of iterations was set as 3e4. We specified rm = ⌊m1/6⌋ and Vm = 20m1/6, where m = n/2 so that the assumption in Theorem 4 is fulfilled. For each sample size, 500 Monte Carlo iterations were performed to obtained the normal QQ-plot, which is shown in Fig. 1.

The QQ-plots indicate that Tn does not deviate from the standard normal distribution, which is consistent with the results from the Shapiro–Wilks test and the Anderson–Darling test as summarized in Table 1.

Empirical type I error rates were also calculated based on the test statistics obtained from the 500 Monte Carlo iterations. The results are shown in Table 2.

In the supplementary materials, we evaluated the method’s performance for different choices of Vn and rn. The results are consistent with the findings provided above. Moreover, additional simulations were conducted on multiple covariates. Based on these empirical studies, we found that as long as the choice of rn and Vn satisfied the required condition [rn(d+2)+1]Vn4log(Vn[rn(d+2)+1])=o(n), the asymptotic distribution and the type I error were guaranteed. Moreover, we conducted a simulation study when the null model contains multiple covariates. The empirical type I error can also be controlled when the sample size is large. Details of the simulations can be found in the supplementary materials.

5. A real data application

We applied our method to the sequencing data from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) and performed a genetic association analysis. The best known gene related to the Alzheimer’s disease (AD) is the APOE on chromosome 19 (Strittmatter et al., 1993). The covariates in our analysis are therefore the single-nucleotide polymorphisms (SNPs) in APOE. After quality control, a total sample of 780 individuals with 169 SNPs remained for the analysis. Studies have shown that for patients with AD, the whole brain volume decreases significantly (Thambisetty et al., 2011). We therefore chose the logarithm of the whole brain volume as the response.

For each SNP in APOE gene, we conducted the goodness-of-fit test as we discussed in Section 3. Table 3 summarizes the top 10 associated SNPs in the APOE gene detected by our method. SNPs with P-value less than 0.05 are shown in bold font.

6. Conclusion

Deep neural networks have been increasingly used in areas such as computer vision and speech recognition. While numerous studies have shown that neural networks attained high performance in terms of prediction accuracy, few studies have investigated the statistical inference of neural networks. Many biomedical studies are hypothesis-driven studies. For instance, in a typical genetic study, investigators are interested in testing the association of genetic variants with a disease of interest. While neural networks hold great promise to reveal the complex relationship between genetic variants and the disease, the lack of established statistical inference limits the use of neural networks in genetic research and other biomedical research.

To fill this gap, we proposed a goodness-of-fit test statistic based on neural network sieve estimators. The proposed test statistic has a simple limiting distribution, which facilitates its use in practice. The idea is to split the sample into two portions, one of which is used to fit the reduced model while the remaining is used to fit the full model. The test statistic is then built on the difference between the mean squared error of the reduced model and that of the full model. As pointed out in Yatchew (1992), sample splitting is necessary. Otherwise, the asymptotic distribution of the test statistic may be degenerate when the mean squared error of the reduced model and that of the full model are computed from the same sample. In fact, the idea of sample splitting has also been applied to construct important quantities in other theoretical studies. For example, Bartlett et al. (2002) defined the maximum discrepancy of a function class F as D^n(F)=supf∈F(2n∑i=1n/2f(Xi)−2n∑i=n/2+1nf(Xi)),

which quantifies how much the behavior on half of the sample can be unrepresentative of the behavior on the other half. Due to the independence between the two sample portions, the asymptotic normality of the test statistic can be easily established. Nevertheless, the tradeoff is that we have to split the samples to calculate the test statistic, which may result in power loss due to the reduced sample size.

We have conducted a simulation study to confirm the theoretical results. By using the QQ-plots and normality tests, we showed that the type I error of the proposed test was well controlled. This paper can be viewed as our initial effort on building test statistics based on neural networks. Additional topics, such as developing a neural-network-based test using the entire sample, could also be further investigated in future studies.

For future studies, we think it is important to develop similar asymptotic theories for modern convolutional neural networks (CNN) and long-short term memory (LSTM) networks. Most theories nowadays on statistical learning are derived under the framework of Vapnik (1998), which depends on the entropy argument and the Dudley integer. Due to the complex network structures of CNN and LSTM, it may be difficult to obtain upper bounds for the covering numbers of the CNN or LSTM classes. Meanwhile, the rate of convergence of the sieve estimator also depends on the approximation rate of the network to the underlying function, which is difficult to derive. Nevertheless, this is an interesting topic worth further studying in the future.

Supplementary Material

Supplementary Materials

Acknowledgments

This work was supported by the National Institute on Drug Abuse (Award No. R01DA043501) and the National Library of Medicine (Award No. R01LM012848).

Fig. 1. Normal QQ-plot for the test statistic Tn under various sample sizes n = 50, 100, 200, 500, 1500, 3000.

Table 1 Summary of results from the normality tests.

Sample size	50	100	200	500	1500	3000	
Shapiro-Wilks	0.412	0.808	0.065	0.521	0.704	0.498	
Anderson-Darling	0.980	0.820	0.098	0.492	0.837	0.950	

Table 2 Empirical type I error rates under different sample sizes.

Sample size	50	100	200	500	1500	3000	
Empirical type I error	0.044	0.056	0.052	0.054	0.046	0.046	

Table 3 Top 10 SNPs in the APOE gene associated with the whole brain volume by using the goodness of fit test. Significant SNPs (P &lt; 0.05) are highlighted in bold font.

SNP name	P-value	
rs_x94	5.046E–3	
rs_x21	5.918E–3	
rs_x42	8.754E–3	
rs72654471	1.595E–2	
rs112757453	2.781E–2	
rs_x127	2.822E–2	
rs59325138	5.029E–2	
rs_x131	5.057E–2	
rs_x132	5.188E–2	
rs_x52	5.398E–2	

CRediT authorship contribution statement

Xiaoxi Shen: Conceptualization, Formal analysis, Writing - original draft. Chang Jiang: Software, Formal analysis, Writing - original draft. Lyudmila Sakhanenko: Validation, Writing - review &amp; editing. Qing Lu: Supervision, Funding acquisition, Writing - review &amp; editing.

Appendix A. Supplementary data

Supplementary material related to this article can be found online at https://doi.org/10.1016/j.spl.2021.109100.


References

Bartlett PL , Boucheron S , Lugosi G , 2002. Model selection and error estimation. Mach. Learn 48 (1–3 ), 85–113.
Boyd S , Mutapcic A , 2008. Subgradient Methods (notes for EE364B Winter 2006–07. Stanford University).
Chen X , Shen X , 1998. Sieve extremum estimates for weakly dependent data. Econometrica 289–314.
Devroye L , Györfi L , Lugosi G , 2013. A Probabilistic Theory of Pattern Recognition, Vol. 31 . Springer Science &amp; Business Media.
Dudley RM , 1984. A course on empirical processes. In: Ecole D’ÉTÉ de ProbabilitÉS de Saint-Flour XII-1982. Springer, pp. 1–142.
Fukumizu K , 1996. A regularity condition of the information matrix of a multilayer perceptron network. Neural Netw. 9 (5 ), 871–879.12662569
Fukumizu K , 2003. Likelihood ratio of unidentifiable models and multilayer neural networks. Ann. Statist 31 (3 ), 833–851.
Goulaouic C , 1971. Approximation polynômiale de fonctions C∞ et analytiques. Ann. Inst. Fourier Grenoble 21 , 149–173.
Györfi L , Kohler M , Krzyzak A , Walk H , 2006. A Distribution-Free Theory of Nonparametric Regression. Springer Science &amp; Business Media.
Horel E , Giesecke K , 2020. Significance tests for neural networks. Journal of Machine Learning Research 21 (227 ), 1–29.34305477
Hornik K , Stinchcombe M , White H , 1989. Multilayer feedforward networks are universal approximators. Neural Netw. 2 (5 ), 359–366.
Mhaskar HN , 1996. Neural networks for optimal approximation of smooth and analytic functions. Neural Comput. 8 (1 ), 164–177.
Ossiander M , 1987. A central limit theorem under metric entropy with L2 bracketing. Ann. Probab 897–919.
Shen X , Jiang C , Sakhanenko L , Lu Q , 2019. Asymptotic properties of neural network sieve estimators. arXiv preprint arXiv:1906.00875.
Strittmatter WJ , Saunders AM , Schmechel D , Pericak-Vance M , Enghild J , Salvesen GS , Roses AD , 1993. Apolipoprotein E: high-avidity binding to beta-amyloid and increased frequency of type 4 allele in late-onset familial Alzheimer disease. Proc. Natl. Acad. Sci 90 (5 ), 1977–1981.8446617
Thambisetty M , Simmons A , Hye A , Campbell J , Westman E , Zhang Y , Wahlund L-O , Kinsey A , Causevic M , Killick R , , 2011. Plasma biomarkers of brain atrophy in Alzheimer’s disease. PLoS One 6 (12 ), e28527.22205954
van der Vaart AW , Wellner JA , 1996. Weak Convergence and Empirical Processes. Springer.
Vapnik V , 1998. Statistical Learning Theory. 1998, Vol. 3 . Wiley, New York.
Wainwright MJ , 2019. High-Dimensional Statistics: a Non-Asymptotic Viewpoint, Vol. 48 . Cambridge University Press.
Wu C-F , 1981. Asymptotic theory of nonlinear least squares estimation. Ann. Statist 501–513.
Yatchew AJ , 1992. Nonparametric regression tests based on least squares. Econometric Theory 8 (4 ), 435–451.
