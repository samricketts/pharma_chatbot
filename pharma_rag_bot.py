# Databricks notebook source
# MAGIC %sql -- Switch to a cluster that the user has access to
# MAGIC --drop table main.db_demo.full_txt;
# MAGIC create table if not exists main.db_demo.full_txt(id BIGINT generated by default as identity, text string) tblproperties(delta.enableChangeDatafeed=true);

# COMMAND ----------

# MAGIC %sql
# MAGIC --drop table main.db_demo.track_articles;
# MAGIC create table if not exists main.db_demo.track_articles (file_name string) tblproperties(delta.enableChangeDatafeed=true);

# COMMAND ----------

# MAGIC %pip install langchain mlflow==2.10.1 langchain==0.1.5 databricks-vectorsearch==0.22 databricks-sdk==0.18.0 mlflow[databricks]==2.10.1 langchain.community pillow==9.4.0 
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

# MAGIC %pip list

# COMMAND ----------

import os
from pyspark.sql.functions import substring_index

dir_path = "/Volumes/main/db_demo/article_text/pmc_publications"
# dir_path = 'Workspace/Users/samricketts@kubrickgroup.com/Eisai_poc/pmc_publications'

file_paths = [file.path for file in dbutils.fs.ls(dir_path)]

df = spark.createDataFrame(file_paths, "string").select(substring_index("value", "/", -1).alias("file_name"))

df.show()

# COMMAND ----------

df.count()

# COMMAND ----------

import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
import glob
from typing import List
import pandas as pd

volume_path = ""
volume_path = "/Volumes/main/db_demo/article_text/pmc_publications"
# create and direct to a different volume if need be 

def read_files(directory: str) -> List[str]:
    documents = []
    processed_files = spark.sql(f"select distinct file_name from main.db_demo.track_articles").collect()
    processed_files = set(row["file_name"] for row in processed_files)
    new_files = [file for file in os.listdir(volume_path) if file not in processed_files]
    for file_name in new_files:
        file_path = os.path.join(directory, file_name)
        with open(file_path, 'r', encoding='utf-8') as file:
            documents.append(file.read())
    return documents

def chunk_text(documents: List[str], chunk_size = 1000, chunk_overlap: int=200) -> List[str]:
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = []
    for doc in documents:
        chunks.extend(text_splitter.split_text(doc))
    return chunks

def process_articles(volume_path):
    print('Reading all new articles...')
    documents = read_files(volume_path)

    print(f"Total articles read: {len(documents)}")
    print("Chunking articles..")
    chunks = chunk_text(documents)

    print(f"Total chunks created: {len(chunks)}")
    return chunks

chunks = process_articles(volume_path)

chunks_df = pd.DataFrame(chunks, columns=["chunk"])

# chunks_df.to_csv("/Workspace/Users/samricketts@kubrickgroup.com/Eisai_poc/chunks.csv", index=False)



# COMMAND ----------

print(chunks_df)

# COMMAND ----------

print(chunks)

# COMMAND ----------

from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import ArrayType, StringType
import pandas as pd

@pandas_udf("array<string>")
def get_chunks(dummy):
    return pd.Series([chunks])

spark.udf.register("get_chunks_udf", get_chunks)

# COMMAND ----------

# MAGIC %sql
# MAGIC insert into main.db_demo.full_txt(text)
# MAGIC select explode(get_chunks_udf('dummy')) as text;
# MAGIC
# MAGIC --see how many rows were inserted

# COMMAND ----------

#update track_articles so we dont have duplicates
df.createOrReplaceTempView("temp_table")

spark.sql("""
    insert into main.db_demo.track_articles
    select * from temp_table
    where not exists (
        select 1 from main.db_demo.track_articles
        where temp_table.file_name = main.db_demo.track_articles.file_name
    )
""")


# end of job

# COMMAND ----------

# MAGIC %sql 
# MAGIC select * from main.db_demo.track_articles

# COMMAND ----------

# creete vector search endpoint
# create vector search index 
# create retriever

# COMMAND ----------

# DBTITLE 1,create a vector search endpoint
# this cell create a vector search endpoint programatically (endpoint must be created before index)

# this can also be done in the UI with the following steps
# click on compute > vector search tab > create button > name endpoint > create
 
from databricks.vector_search.client import VectorSearchClient

# The following line automatically generates a PAT Token for authentication
client = VectorSearchClient()

# The following line uses the service principal token for authentication, uncomment if needed
#client = VectorSearch(service_principal_client_id=<CLIENT_ID>,service_principal_client_secret=<CLIENT_SECRET>)

client.create_endpoint(
    name="vector_search_endpoint_test",
    endpoint_type="STANDARD"
)


# COMMAND ----------

# DBTITLE 1,create a vector search index
# create the vector search index programatically

# this can be done with the UI as well 
# navigate to the delta table you created (should have id and text columns) > blue create button > vector search index >
#   name the index, primary key should be id, select endpoint created above, columns to sync leave blank, embedding     #   source column is text, select embedding model, sync mode should be triggered, create

from databricks.vector_search.client import VectorSearchClient

client = VectorSearchClient()

index = client.create_delta_sync_index(
  endpoint_name="vector_search_endpoint_test",  #your search endpoint that you previously made
  source_table_name="main.db_demo.pdf_docs", #the delta table with all of your data to make the index from
  index_name="main.db_demo.index_test", #change index name to whatever you want
  pipeline_type="TRIGGERED",
  primary_key="id",
  embedding_source_column="text",
  embedding_model_endpoint_name="databricks-gte-large-en" #make sure to use the same embedding model for your retriever
)


# COMMAND ----------

# create vector search endpoint first, then create vector search index
import os
host = "https://" + spark.conf.get("spark.databricks.workspaceUrl")

os.environ['DATABRICKS_TOKEN'] = os.getenv("db_rag_pat")
# os.environ['DATABRICKS_TOKEN'] = dbutils.secrets.get(scope='repeatable_solutions', key='ws-pat')
# os.environ['DATABRICKS_TOKEN'] = {{secrets/repeatable_solutions/ws-pat}}
# may need to change to use the secrets API 2.0

index_name = "main.db_demo.pmc_index"
host = "https://" + spark.conf.get("spark.databricks.workspaceUrl")

vector_search_endpoint_name = "pubmed_endpoint"

# COMMAND ----------

#creating RAG retriever 
from databricks.vector_search.client import VectorSearchClient
from langchain_community.vectorstores import DatabricksVectorSearch
from langchain_community.embeddings import DatabricksEmbeddings 

embedding_model = DatabricksEmbeddings(endpoint='databricks-gte-large-en')
# model needs to be the same as the vector index model

def get_retriever(persist_dir: str=None):
    os.environ['DATABRICKS_HOST'] = host
    vsc = VectorSearchClient(workspace_url=host, personal_access_token=os.getenv("db_rag_pat"))
    # vsc = VectorSearchClient(workspace_url=host, personal_access_token=os.environ['DATABRICKS_TOKEN'])
    vs_index = vsc.get_index(
        endpoint_name=vector_search_endpoint_name,
        index_name=index_name
    )

    vectorstore = DatabricksVectorSearch(
        vs_index, text_column='text', embedding = embedding_model
    )
    return vectorstore.as_retriever()

# COMMAND ----------

from langchain.chains import RetrievalQA 
from langchain.prompts import PromptTemplate
from langchain_community.chat_models import ChatDatabricks

chat_model = ChatDatabricks(endpoint='databricks-meta-llama-3-1-70b-instruct', max_tokens = 200)

TEMPLATE = """You are a helpful assistant in distilling information from scientific publications that are in .txt files. You are answering questions regarding the information and context that you have data on. If the questions is not related to data you have, say that you don't have that information or decline to answer. If you don't know the answer to a question, say that you don't know the answer. Keep the answer consise and in English.
Use the follwing pieces of context to answer the question at the end:
{context}
Question: {question}
Answer:
"""

prompt = PromptTemplate(template=TEMPLATE, input_variables = ["context", "question"])

chain = RetrievalQA.from_chain_type(
    llm=chat_model,
    chain_type="stuff",
    retriever=get_retriever(),
    chain_type_kwargs={"prompt": prompt}
)

# COMMAND ----------

question = {"query": "what are some challenges with using a biomarker-only diagnosis of Alzheimerâ€™s"}
answer = chain.run(question)
print(answer)

# COMMAND ----------

#register as UC model.. should only need to do this once until you make changes, creates another version 

from mlflow.models import infer_signature
import mlflow
import langchain

# mlflow.pyfunc.get_model_dependencies(model_uri)
mlflow.set_registry_uri("databricks-uc")
model_name = "main.db_demo.pharma_chatbot_model"

with mlflow.start_run(run_name = "pharma_chatbot_run") as run:
    signature=infer_signature(question, answer)
    model_info=mlflow.langchain.log_model(
        chain,
        loader_fn=get_retriever,
        artifact_path='chain',
        registered_model_name=model_name,
        pip_requirements=[
            "mlflow==" + mlflow.__version__,
            "langchain==" + langchain.__version__,
            "databricks-vectorsearch",
        ],
        input_example=question,
        signature=signature
    )

# COMMAND ----------

import mlflow
mlflow.set_experiment("/Workspace/Users/samricketts@kubrickgroup.com/rag_solution_experiment")


# COMMAND ----------

mlflow.log_param("retriever_type", "Dense Retriever")
mlflow.log_param("num_retrieved_docs", 10)
mlflow.log_param("embedding_type", "BERT")
mlflow.log_param("learning_rate", 0.001)
mlflow.log_param("epochs", 3)


# COMMAND ----------


mlflow.log_metric("recall@10", recall_at_10)
mlflow.log_metric("BLEU_score", bleu_score)
mlflow.log_metric("ROUGE_score", rouge_score)
mlflow.log_metric("inference_latency", latency)


# COMMAND ----------

# Assuming you have two models: retriever and generator
mlflow.pytorch.log_model(retriever_model, "retriever_model")
mlflow.pytorch.log_model(generator_model, "generator_model")


# COMMAND ----------

import mlflow
import mlflow.pyfunc

class RAGModel(mlflow.pyfunc.PythonModel):
    def __init__(self, retriever, generator):
        self.retriever = retriever
        self.generator = generator

    def predict(self, context, model_input):
        retrieved_docs = self.retriever.retrieve(model_input)
        generated_answer = self.generator.generate(retrieved_docs)
        return generated_answer

rag_model = RAGModel(retriever_model, generator_model)
mlflow.pyfunc.log_model("rag_pipeline", python_model=rag_model)


# COMMAND ----------

import langchain
import mlflow

chain = mlflow.langchain.load_model(logged_chain_info.model_uri)
chain.invoke(question)

# COMMAND ----------

# MAGIC %pip install databricks-agents

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

from databricks import agents
import time
import mlflow
from databricks.sdk.service.serving import EndpointStateReady, EndpointStateConfigUpdate
from databricks.sdk import WorkspaceClient
w = WorkspaceClient()

# UC_MODEL_NAME = "main.db_demo.ppt_chatbot_model"

# Use Unity Catalog to log the chain
mlflow.set_registry_uri('databricks-uc')

# Register the chain to UC
uc_registered_model_info = mlflow.register_model(model_uri=logged_chain_info.model_uri, name=UC_MODEL_NAME)

# Deploy to enable the Review APP and create an API endpoint
deployment_info = agents.deploy(model_name=UC_MODEL_NAME, model_version=uc_registered_model_info.version)

# Wait for the Review App to be ready
print("\nWaiting for endpoint to deploy.  This can take 10 - 20 minutes.", end="")
while w.serving_endpoints.get(deployment_info.endpoint_name).state.ready == EndpointStateReady.NOT_READY or w.serving_endpoints.get(deployment_info.endpoint_name).state.config_update == EndpointStateConfigUpdate.IN_PROGRESS:
    print(".", end="")
    time.sleep(30)

# COMMAND ----------

#chatbot gui

%pip install dbtunnel[gradio] aiohttp
dbutils.library.restartPython()

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

import os

os.environ['DATABRICKS_TOKEN'] = os.getenv("db_rag_pat")
#update with secrets
os.environ['API_ENDPOINT'] = "https://dbc-26e08dd2-68bb.cloud.databricks.com/serving-endpoints/pharma_endpoint2/invocations"


# COMMAND ----------

import os
import gradio as gr
import requests
import json
from gradio.themes.utils import sizes

def respond(message, history):
    if len(message.strip()) == 0:
        return "ERROR: The question should not be empty"
    
    print("#### Message ####")
    print(message)

    local_token = os.getenv('DATABRICKS_TOKEN')
    local_endpoint = os.getenv('API_ENDPOINT')

    if local_token is None or local_endpoint is None:
        return "ERROR: Missing environment variables"

    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {local_token}'
    }

    payload = {
        "inputs": [message]
    }

    try:
        response = requests.post(
            local_endpoint, json=payload, headers=headers, timeout=100
        )
        response.raise_for_status()  # This will raise an HTTPError for bad responses (4xx or 5xx)
        response_json = response.json()
        print("#### Response JSON ####")
        print(response_json)

        response_data = response_json.get("predictions", [None])[0]
        if response_data is None:
            return "ERROR: No predictions found in response"
        print(response_data)
        return response_data

    except requests.RequestException as e:
        return f"ERROR: Request failed with status code {e.response.status_code} and message {str(e)}"
    except json.JSONDecodeError:
        return "ERROR: Failed to decode JSON from response"
    except KeyError as e:
        return f"ERROR: Missing expected key in response JSON: {str(e)}"
    except Exception as e:
        return f"ERROR: An unexpected error occurred: {str(e)}"

theme = gr.themes.Soft(
    text_size=sizes.text_sm, radius_size=sizes.radius_sm, spacing_size=sizes.spacing_sm,
)

demo = gr.ChatInterface(
    respond,
    chatbot=gr.Chatbot(show_label=False, container=False, show_copy_button=True, bubble_full_width=True),
    textbox=gr.Textbox(placeholder="Ask a question here...", container=False, scale=4),
    title="Pharma Alzheimer's Disease Chatbot",
    description="This chatbot answers Alzheimer's related questions based on information within selected Pubmed Central publications",
    examples=[["What are some symptoms of Alzheimer's?"], ["What is a typical age for an Alzheimer's diagnosis?"],
              ["Who are some prominent published authors in Alzheimer's research?"]],
    cache_examples=False,
    theme=theme,
    retry_btn=None,
    undo_btn=None,
    clear_btn="Clear",
)



# COMMAND ----------

from dbtunnel import dbtunnel
dbtunnel.gradio(demo).run()
